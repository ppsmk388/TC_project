{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Talent Search System - 模块化演示\n",
    "\n",
    "这个notebook展示了如何使用模块化的Talent Search System来搜索和分析人才。\n",
    "\n",
    "## 系统架构\n",
    "\n",
    "系统被拆分为以下模块：\n",
    "- `config.py` - 配置和常量\n",
    "- `utils.py` - 通用工具函数\n",
    "- `schemas.py` - Pydantic数据模型\n",
    "- `llm.py` - LLM配置和封装\n",
    "- `search.py` - 搜索功能（SearXNG）\n",
    "- `extraction.py` - 抽取和解析功能\n",
    "- `graph.py` - LangGraph工作流\n",
    "- `main.py` - 主入口点\n",
    "\n",
    "## 工作流程\n",
    "\n",
    "1. **查询解析** - 将自然语言查询转换为结构化参数\n",
    "2. **搜索规划** - 基于查询参数生成搜索词\n",
    "3. **内容搜索** - 使用SearXNG搜索相关内容\n",
    "4. **URL选择** - 选择最相关的URL进行抓取\n",
    "5. **内容抓取** - 抓取和提取网页内容\n",
    "6. **作者提取** - 从论文中提取作者信息\n",
    "7. **候选人合成** - 生成候选人卡片\n",
    "\n",
    "现在让我们开始使用这些模块！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 设置和导入模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 所有模块导入成功！\n"
     ]
    }
   ],
   "source": [
    "# 导入所有模块\n",
    "import sys\n",
    "import os\n",
    "# 导入模块\n",
    "import config\n",
    "import utils\n",
    "import schemas\n",
    "import llm\n",
    "import search\n",
    "import extraction\n",
    "import graph\n",
    "import main\n",
    "\n",
    "print(\"✅ 所有模块导入成功！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 查看配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "默认会议库：\n",
      "  NeurIPS: ['NeurIPS']\n",
      "  AAAI: ['AAAI']\n",
      "  IJCAI: ['IJCAI']\n",
      "  ICML: ['ICML']\n",
      "  ICLR: ['ICLR']\n",
      "  KDD: ['KDD']\n",
      "  ACL: ['ACL']\n",
      "  EMNLP: ['EMNLP']\n",
      "  NAACL: ['NAACL']\n",
      "  AE: ['AE']\n",
      "  IJCAR: ['IJCAR']\n",
      "  CVPR: ['CVPR']\n",
      "  ICCV: ['ICCV']\n",
      "  ECCV: ['ECCV']\n",
      "  COLING: ['COLING']\n",
      "  SIGGRAPH: ['SIGGRAPH']\n",
      "  ACM-MM: ['ACM MM']\n",
      "  VIS: ['VIS']\n",
      "  SIGMOD: ['SIGMOD']\n",
      "  VLDB: ['VLDB']\n",
      "  SIGIR: ['SIGIR']\n",
      "  PODC: ['PODC']\n",
      "  SIGCOMM: ['SIGCOMM']\n",
      "  INFOCOM: ['INFOCOM']\n",
      "  NSDI: ['NSDI']\n",
      "  CHI: ['CHI']\n",
      "\n",
      "默认年份: [2025, 2024, 2026]\n",
      "SearXNG基础URL: http://127.0.0.1:8888\n",
      "默认搜索结果数: 8\n",
      "默认选择URL数: 16\n"
     ]
    }
   ],
   "source": [
    "# 查看默认会议配置\n",
    "print(\"默认会议库：\")\n",
    "for conf, aliases in config.DEFAULT_CONFERENCES.items():\n",
    "\tprint(f\"  {conf}: {aliases}\")\n",
    "\n",
    "print(f\"\\n默认年份: {config.DEFAULT_YEARS}\")\n",
    "print(f\"SearXNG基础URL: {config.SEARXNG_BASE_URL}\")\n",
    "print(f\"默认搜索结果数: {config.SEARCH_K}\")\n",
    "print(f\"默认选择URL数: {config.SELECT_K}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 测试查询解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = \"Find 5 current PhD/MSc candidates working on graph foundation model.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试查询: Find 5 current PhD/MSc candidates working on graph foundation model.\n",
      "\n",
      "==================================================\n",
      "\n",
      "解析后的查询规格:\n",
      "  目标数量: 5\n",
      "  年份: [2025, 2024]\n",
      "  会议: ['ICLR', 'ICML', 'NeurIPS', 'ACL', 'EMNLP', 'NAACL', 'KDD', 'WWW', 'AAAI', 'IJCAI', 'CVPR', 'ECCV', 'ICCV', 'SIGIR']\n",
      "  关键词: ['graph foundation model', 'foundation model', 'graph neural networks', 'GNN']\n",
      "  必须是学生: True\n",
      "  学位级别: ['PhD', 'MSc']\n",
      "  作者优先级: ['first', 'last']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"测试查询: {test_query}\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# 使用LLM解析查询（如果LLM可用）\n",
    "try:\n",
    "\tllm_instance = llm.get_llm(\"parse\", temperature=0.3)\n",
    "\t\n",
    "\tconf_list = \", \".join(config.DEFAULT_CONFERENCES.keys())\n",
    "\tprompt = (\n",
    "\t\t\"You are a professional talent recruitment analysis assistant responsible for parsing recruitment queries and extracting structured information.\\n\\n\"\n",
    "\t\t\"=== PARSING TASK INSTRUCTIONS ===\\n\"\n",
    "\t\t\"Please carefully analyze the user's recruitment query and extract the following key information:\\n\\n\"\n",
    "\t\t\"1. **top_n** (int): Number of candidates needed. Look for numbers in the query like '10 candidates', '20 people', etc.\\n\\n\"\n",
    "\t\t\"2. **years** (int[]): Years to focus on for papers. Prioritize recent years like [2024,2025]. Default to [2025,2024] if not specified.\\n\\n\"\n",
    "\t\t\"3. **venues** (string[]): Target conferences/journals. Users will explicitly mention venues like 'ACL', 'NeurIPS', etc.\\n\"\n",
    "\t\tf\"   Known venues include (not exhaustive): {conf_list}\\n\"\n",
    "\t\t\"   Recognition rules:\\n\"\n",
    "\t\t\"   - Direct conference names: ACL, EMNLP, NAACL, NeurIPS, ICLR, ICML\\n\"\n",
    "\t\t\"   - Conference variants: NIPS→NeurIPS, The Web Conference→WWW\\n\"\n",
    "\t\t\"   - Platforms: OpenReview (counts as a venue)\\n\\n\"\n",
    "\t\t\"4. **keywords** (string[]): Research areas and technical keywords. Focus on identifying:\\n\"\n",
    "\t\t\"   Technical keywords: LLM, large language models, transformer, attention, multi-agent, multi-agent systems, reinforcement learning, RL, graph neural networks, GNN, foundation model, social simulation, computer vision, CV, natural language processing, NLP\\n\"\n",
    "\t\t\"   Research areas: machine learning, deep learning, AI alignment, robotics, computer vision, NLP, speech recognition, recommendation systems\\n\"\n",
    "\t\t\"   Application areas: autonomous driving, medical AI, finance, social simulation, game theory, human-AI interaction\\n\\n\"\n",
    "\t\t\"5. **must_be_current_student** (bool): Whether candidates must be current students. Look for:\\n\"\n",
    "\t\t\"   - Explicit requirements: current student, currently enrolled, active student\\n\"\n",
    "\t\t\"   - Degree phases: PhD student, Master's student, graduate student\\n\"\n",
    "\t\t\"   - Default: true (unless explicitly stated otherwise)\\n\\n\"\n",
    "\t\t\"6. **degree_levels** (string[]): Acceptable degree levels.\\n\"\n",
    "\t\t\"   Recognition: PhD, MSc, Master, Graduate, Undergraduate, Bachelor, Postdoc\\n\"\n",
    "\t\t\"   Default: ['PhD', 'MSc', 'Master', 'Graduate']\\n\\n\"\n",
    "\t\t\"7. **author_priority** (string[]): Author position preferences.\\n\"\n",
    "\t\t\"   Recognition: first author, last author, corresponding author\\n\"\n",
    "\t\t\"   Default: ['first', 'last']\\n\\n\"\n",
    "\t\t\"8. **extra_constraints** (string[]): Other constraints.\\n\"\n",
    "\t\t\"   Recognition: geographic requirements (e.g., 'Asia', 'North America')\\n\"\n",
    "\t\t\"   institutional requirements (e.g., 'top universities', 'Ivy League')\\n\"\n",
    "\t\t\"   language requirements, experience requirements, etc.\\n\\n\"\n",
    "\t\t\"=== PARSING STRATEGY TIPS ===\\n\"\n",
    "\t\t\"• Prioritize explicitly mentioned information, then make reasonable inferences\\n\"\n",
    "\t\t\"• For technical keywords, identify specific models, methods, and research areas\\n\"\n",
    "\t\t\"• Distinguish between different recruitment goals: interns vs researchers vs postdocs\\n\"\n",
    "\t\t\"• Pay attention to time-sensitive information: recent publications, accepted papers, upcoming deadlines\\n\\n\"\n",
    "\t\t\"Return STRICT JSON format only, no additional text.\\n\\n\"\n",
    "\t\t\"User Query:\\n\"\n",
    "\t\tf\"{test_query}\\n\"\n",
    "\t)\n",
    "\tmock_spec = llm.safe_structured(llm_instance, prompt, schemas.QuerySpec)\n",
    " \n",
    "\tif mock_spec.venues == []:\n",
    "\t\tmock_spec.venues = [\"ICLR\", \"ICML\", \"NeurIPS\", \"ACL\", \"EMNLP\", \"NAACL\", \"KDD\", \"WWW\", \"AAAI\", \"IJCAI\", \"CVPR\", \"ECCV\", \"ICCV\", \"SIGIR\"]\n",
    " \n",
    "\tprint(\"解析后的查询规格:\")\n",
    "\tprint(f\"  目标数量: {mock_spec.top_n}\")\n",
    "\tprint(f\"  年份: {mock_spec.years}\")\n",
    "\tprint(f\"  会议: {mock_spec.venues}\")\n",
    "\tprint(f\"  关键词: {mock_spec.keywords}\")\n",
    "\tprint(f\"  必须是学生: {mock_spec.must_be_current_student}\")\n",
    "\tprint(f\"  学位级别: {mock_spec.degree_levels}\")\n",
    "\tprint(f\"  作者优先级: {mock_spec.author_priority}\")\n",
    "\n",
    "except Exception as e:\n",
    "\tprint(f\"LLM解析失败，使用模拟数据: {e}\")\n",
    "\n",
    "\tmock_spec = schemas.QuerySpec(\n",
    "\t\ttop_n=10,\n",
    "\t\tyears=[2025, 2024],\n",
    "\t\tvenues=[\"ICLR\", \"ICML\", \"NeurIPS\"],\n",
    "\t\tkeywords=[\"social simulation\", \"multi-agent systems\"],\n",
    "\t\tmust_be_current_student=True,\n",
    "\t\tdegree_levels=[\"PhD\", \"Master\"],\n",
    "\t\tauthor_priority=[\"first\"],\n",
    "\t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 测试搜索查询构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "为查询构建了 10 个搜索词:\n",
      "\n",
      " 1. ICLR 2025 \"graph foundation model\"\n",
      " 2. ICLR 2025 \"graph foundation model\" accepted papers\n",
      " 3. ICLR 2025 \"graph foundation model\" accept\n",
      " 4. ICLR 2025 \"graph foundation model\" acceptance\n",
      " 5. ICLR 2025 \"graph foundation model\" program\n",
      " 6. ICLR 2025 \"graph foundation model\" proceedings\n",
      " 7. ICLR 2025 \"graph foundation model\" schedule\n",
      " 8. ICLR 2025 \"graph foundation model\" paper list\n",
      " 9. ICLR 2025 \"graph foundation model\" main conference\n",
      "10. ICLR 2025 \"graph foundation model\" research track\n"
     ]
    }
   ],
   "source": [
    "# 构建搜索查询\n",
    "search_terms = search.build_conference_queries(mock_spec, config.DEFAULT_CONFERENCES, cap=10)\n",
    "\n",
    "print(f\"为查询构建了 {len(search_terms)} 个搜索词:\\n\")\n",
    "for i, term in enumerate(search_terms[:10], 1):  # 只显示前10个\n",
    "\tprint(f\"{i:2d}. {term}\")\n",
    "\t\n",
    "if len(search_terms) > 10:\n",
    "\tprint(f\"    ... 还有 {len(search_terms) - 10} 个搜索词\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用SearXNG搜索引擎测试搜索词...\n",
      "将使用前 3 个搜索词进行演示搜索\n",
      "\n",
      "🔍 搜索 1: ICLR 2025 \"graph foundation model\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   📊 找到 17 个结果\n",
      "   1. AnyGraph: Graph Foundation Model in the Wild...\n",
      "      URL: https://openreview.net/forum?id=Kdcqzfypry\n",
      "      引擎: google\n",
      "   2. GOFA: A Generative One-For-All Model for Joint Graph ......\n",
      "      URL: https://openreview.net/forum?id=mIjblC9hfm\n",
      "      引擎: google\n",
      "\n",
      "🔍 搜索 2: ICLR 2025 \"graph foundation model\" accepted papers\n",
      "   📊 找到 18 个结果\n",
      "   1. GOFA: A Generative One-For-All Model for Joint Graph ......\n",
      "      URL: https://proceedings.iclr.cc/paper_files/paper/2025/hash/652c104b5b0652a03684efeaf805463b-Abstract-Conference.html\n",
      "      引擎: google\n",
      "   2. AnyGraph: Graph Foundation Model in the Wild...\n",
      "      URL: https://openreview.net/forum?id=Kdcqzfypry\n",
      "      引擎: google\n",
      "\n",
      "🔍 搜索 3: ICLR 2025 \"graph foundation model\" accept\n",
      "   📊 找到 19 个结果\n",
      "   1. AnyGraph: Graph Foundation Model in the Wild...\n",
      "      URL: https://openreview.net/forum?id=Kdcqzfypry\n",
      "      引擎: google\n",
      "   2. GOFA: A Generative One-For-All Model for Joint Graph ......\n",
      "      URL: https://proceedings.iclr.cc/paper_files/paper/2025/hash/652c104b5b0652a03684efeaf805463b-Abstract-Conference.html\n",
      "      引擎: google\n",
      "\n",
      "🎯 总共找到 54 个搜索结果\n",
      "📈 使用的搜索引擎: google\n",
      "🌐 SearXNG服务: http://127.0.0.1:8888\n",
      "\n",
      "📊 引擎结果分布:\n",
      "   google: 54 个结果\n",
      "\n",
      "🔧 扩展测试：不同搜索参数对比\n",
      "==================================================\n",
      "\n",
      "✅ 搜索词与引擎测试完成！\n",
      "💡 提示：如需测试更多功能，可调整参数或添加新的测试用例\n"
     ]
    }
   ],
   "source": [
    "## 4.1 使用搜索词进行实际搜索\n",
    "\n",
    "# 使用构建的搜索词进行实际搜索\n",
    "print(\"使用SearXNG搜索引擎测试搜索词...\")\n",
    "print(f\"将使用前 {min(3, len(search_terms))} 个搜索词进行演示搜索\")\n",
    "print()\n",
    "\n",
    "# 演示搜索（使用前3个搜索词）\n",
    "demo_results = []\n",
    "for i, search_term in enumerate(search_terms[:3], 1):\n",
    "\tprint(f\"🔍 搜索 {i}: {search_term}\")\n",
    "\ttry:\n",
    "\t\t\n",
    "\t\tresults = search.searxng_search(\n",
    "\t\t\tquery=search_term,\n",
    "\t\t\tengines=[\"google\"],  # 使用配置的搜索引擎\n",
    "\t\t\tpages=2,  # 只搜索前两页页用于演示\n",
    "\t\t\tk_per_query=10  # 每个查询返回10个结果\n",
    "\t\t)\n",
    "\t\tdemo_results.extend(results)\n",
    "\n",
    "\t\tprint(f\"   📊 找到 {len(results)} 个结果\")\n",
    "\t\tfor j, result in enumerate(results[:2], 1):  # 只显示前2个结果\n",
    "\t\t\tprint(f\"   {j}. {result['title'][:60]}...\")\n",
    "\t\t\tprint(f\"      URL: {result['url']}\")\n",
    "\t\t\tprint(f\"      引擎: {result.get('engine', 'unknown')}\")\n",
    "\t\tprint()\n",
    "\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"   ❌ 搜索失败: {e}\")\n",
    "\t\tprint()\n",
    "\n",
    "print(f\"🎯 总共找到 {len(demo_results)} 个搜索结果\")\n",
    "print(f\"📈 使用的搜索引擎: {config.SEARXNG_ENGINES}\")\n",
    "print(f\"🌐 SearXNG服务: {config.SEARXNG_BASE_URL}\")\n",
    "\n",
    "# 显示搜索结果的分布\n",
    "if demo_results:\n",
    "    engine_counts = {}\n",
    "    for result in demo_results:\n",
    "        engine = result.get('engine', 'unknown')\n",
    "        engine_counts[engine] = engine_counts.get(engine, 0) + 1\n",
    "\n",
    "    print(\"\\n📊 引擎结果分布:\")\n",
    "    for engine, count in engine_counts.items():\n",
    "        print(f\"   {engine}: {count} 个结果\")\n",
    "\n",
    "# 扩展测试：测试不同的搜索参数\n",
    "print(\"\\n🔧 扩展测试：不同搜索参数对比\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\n✅ 搜索词与引擎测试完成！\")\n",
    "print(\"💡 提示：如需测试更多功能，可调整参数或添加新的测试用例\")\n",
    "\n",
    "\n",
    "# simuluate the search remove the duplicate\n",
    "seen = set()\n",
    "uniq_results = []\n",
    "for r in demo_results:\n",
    "\tu = r.get(\"url\", \"\")\n",
    "\tif u and u not in seen:\n",
    "\t\tseen.add(u)\n",
    "\t\tuniq_results.append(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save unique result to jsonl\n",
    "import json\n",
    "with open(\"mock_serp.jsonl\", \"w\") as f:\n",
    "    for r in uniq_results:\n",
    "        f.write(json.dumps(r) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 测试URL选择逻辑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[select] URL 1 selected: https://iclr.cc/virtual/2025/poster/28473...\n",
      "[select] URL 2 selected: https://dl.acm.org/doi/10.1145/3711896.3736568...\n",
      "[select] URL 3 selected: https://proceedings.iclr.cc/paper_files/paper/2025/file/652c104b5b0652a03684efeaf805463b-Paper-Conference.pdf...\n",
      "[select] URL 4 selected: https://arxiv.org/html/2508.20906v1...\n",
      "[select] URL 5 selected: https://dl.acm.org/doi/10.1145/3696410.3714952...\n",
      "[select] URL 6 selected: https://icml.cc/virtual/2025/poster/44147...\n",
      "[select] URL 7 selected: https://www.computer.org/csdl/journal/tp/2025/06/10915556/24RViEuz34Q...\n",
      "[select] URL 8 selected: https://arxiv.org/pdf/2403.16137...\n",
      "[select] URL 9 rejected: https://towardsdatascience.com/foundation-models-in-graph-geometric-deep-learning-f363e2576f58/...\n",
      "[select] URL 10 selected: https://neurips.cc/virtual/2024/poster/94900...\n",
      "[select] URL 11 selected: https://aclanthology.org/2024.findings-emnlp.132.pdf...\n",
      "[select] URL 12 selected: https://www.researchgate.net/publication/383267044_AnyGraph_Graph_Foundation_Model_in_the_Wild...\n",
      "[select] URL 13 selected: https://proceedings.iclr.cc/paper_files/paper/2025/hash/652c104b5b0652a03684efeaf805463b-Abstract-Conference.html...\n",
      "[select] URL 14 selected: https://icml.cc/virtual/2025/poster/46113...\n",
      "[select] URL 15 selected: https://arxiv.org/html/2310.11829v4...\n",
      "[select] URL 16 selected: https://www.yfang.site/publications...\n",
      "[select] URL 17 selected: http://www.shichuan.org/doc/196.pdf...\n",
      "[select] URL 18 selected: https://www.arxiv.org/pdf/2407.09709...\n",
      "[select] URL 19 selected: https://yaoma24.github.io/...\n",
      "[select] URL 20 selected: https://www.researchgate.net/publication/382654651_Boosting_Graph_Foundation_Model_from_Structural_Perspective...\n",
      "[select] URL 21 selected: https://link.springer.com/article/10.1007/s11227-025-07029-9...\n",
      "[select] URL 22 selected: https://yixinliu233.github.io/...\n",
      "[select] URL 23 selected: https://link.springer.com/content/pdf/10.1007/978-981-96-8197-6_13.pdf?pdf=inline%20link...\n",
      "[select] URL 24 selected: https://lechengkong.github.io/...\n"
     ]
    }
   ],
   "source": [
    "# load the jsonl file\n",
    "import json\n",
    "with open(\"mock_serp.jsonl\", \"r\") as f:\n",
    "    mock_serp = [json.loads(line) for line in f]\n",
    "    \n",
    "\n",
    "llm_instance = llm.get_llm(\"select\", temperature=0.3)\n",
    "\n",
    "\n",
    "selected_urls = []\n",
    "selected_serp = []\n",
    "\n",
    "# More strict URL filtering - remove low-quality domains\n",
    "not_allowed_domains = [\n",
    "\t\"x.com\", \"twitter.com\", \"github.com\", \"linkedin.com\", \"facebook.com\",\n",
    "\t\"youtube.com\", \"reddit.com\", \"medium.com\", \"substack.com\"\n",
    "]\n",
    "\n",
    "# Also filter out news sites and general forums\n",
    "low_quality_domains = [\n",
    "\t\"news\", \"blog\", \"forum\", \"discussion\", \"comment\", \"review\"\n",
    "]\n",
    "\n",
    "filtered_items = []\n",
    "for item in mock_serp:\n",
    "\turl = item.get(\"url\", \"\").lower()\n",
    "\t\n",
    "\t# Check if URL contains any blocked domains\n",
    "\tif any(domain in url for domain in not_allowed_domains):\n",
    "\t\tcontinue\n",
    "\t\t\n",
    "\t# Check if URL contains low-quality indicators\n",
    "\tif any(indicator in url for indicator in low_quality_domains):\n",
    "\t\tcontinue\n",
    "\t\t\n",
    "\tfiltered_items.append(item)\n",
    "\n",
    "# Judge each filtered URL individually\n",
    "for i, item in enumerate(filtered_items):\n",
    "\ttitle = item.get(\"title\", \"\")[:150]\n",
    "\tsnippet = item.get(\"snippet\", \"\")[:200]\n",
    "\turl = item.get(\"url\", \"\")\n",
    "\t\n",
    "\tquery_context = \"\"\n",
    "\tif mock_spec.venues:\n",
    "\t\tquery_context += f\"Target conferences: {', '.join(mock_spec.venues)}\\n\"\n",
    "\tif mock_spec.years:\n",
    "\t\tquery_context += f\"Target years: {', '.join(map(str, mock_spec.years))}\\n\"\n",
    "\tif mock_spec.keywords:\n",
    "\t\tquery_context += f\"Research keywords: {', '.join(mock_spec.keywords)}\\n\"\n",
    "\t\n",
    "\tprompt = (\n",
    "\t\tf\"Look at this search result and decide if it's worth fetching for academic talent search.\\n\\n\"\n",
    "\t\tf\"Query Context:\\n{query_context}\\n\"\n",
    "\t\tf\"Title: {title}\\n\"\n",
    "\t\tf\"URL: {url}\\n\"\n",
    "\t\tf\"Snippet: {snippet}\\n\\n\"\n",
    "\t\tf\"SELECT this URL if it contains ANY of these valuable information:\\n\"\n",
    "\t\tf\"1. **Paper details**: Paper titles, author names, abstracts, or paper content\\n\"\n",
    "\t\tf\"2. **Academic lists**: Accepted papers lists, conference programs, workshop papers\\n\"\n",
    "\t\tf\"3. **Researcher info**: Academic profiles, university pages, research institutions\\n\"\n",
    "\t\tf\"4. **Conference content**: Proceedings, accepted papers, research tracks\\n\\n\"\n",
    "\t\tf\"PRIORITY: URLs that match the query context (conferences, years, keywords) are preferred.\\n\\n\"\n",
    "\t\tf\"REJECT only if it's clearly:\\n\"\n",
    "\t\tf\"- Pure social media posts (x.com, facebook, linkedin)\\n\"\n",
    "\t\tf\"- General news without academic content\\n\"\n",
    "\t\tf\"- Personal blogs with no research information\\n\"\n",
    "\t\tf\"- Spam or irrelevant content\\n\\n\"\n",
    "\t\tf\"Be reasonable - if it looks like it might contain academic/research info, select it.\\n\"\n",
    "\t\tf\"Should I fetch this URL? Return JSON: {{ \\\"should_fetch\\\": true/false }}\"\n",
    "\t)\n",
    "\t\n",
    "\ttry:\n",
    "\t\t# Get LLM decision for this URL\n",
    "\t\tresult = llm.safe_structured(llm_instance, prompt, schemas.LLMSelectSpec)\n",
    "\t\t\n",
    "\t\tif result and result.should_fetch:\n",
    "\t\t\turl = item.get(\"url\", \"\")\n",
    "\t\t\tselected_urls.append(url)\n",
    "\t\t\tselected_serp.append(item)\n",
    "\t\t\tprint(f\"[select] URL {i+1} selected: {url}...\")\n",
    "\t\telse:\n",
    "\t\t\tprint(f\"[select] URL {i+1} rejected: {url}...\")\n",
    "\t\t\t\t\n",
    "\texcept Exception as e:\n",
    "\t\tif config.VERBOSE:\n",
    "\t\t\tprint(f\"[select] URL {i+1} error: {e}\")\n",
    "\t\tcontinue\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'GOFA: A Generative One-For-All Model for Joint Graph ...',\n",
       "  'url': 'https://iclr.cc/virtual/2025/poster/28473',\n",
       "  'snippet': '2025 ... However, unlike text and image data, graph data do not have a definitive structure, posing great challenges to developing a Graph Foundation Model (GFM).',\n",
       "  'engine': 'google',\n",
       "  'authors': []},\n",
       " {'title': 'Graph Foundation Models: Challenges, Methods, and ...',\n",
       "  'url': 'https://dl.acm.org/doi/10.1145/3711896.3736568',\n",
       "  'snippet': 'by Z Wang · 2025 — Boosting Graph Foundation Model from Structural Perspective. arXiv (2024). Google Scholar. [18]. Yuanning Cui, Zequn Sun, and Wei Hu. 2025. A ...',\n",
       "  'engine': 'google',\n",
       "  'authors': []},\n",
       " {'title': 'GOFA: A GENERATIVE ONE-FOR-ALL MODEL FOR ...',\n",
       "  'url': 'https://proceedings.iclr.cc/paper_files/paper/2025/file/652c104b5b0652a03684efeaf805463b-Paper-Conference.pdf',\n",
       "  'snippet': 'by L Kong · Cited by 24 — We introduce GOFA, a generative One-for-All graph foundation model. GOFA is pre-trained under a graph completion framework to enable large-scale self-supervised ...32 pages',\n",
       "  'engine': 'google',\n",
       "  'authors': []},\n",
       " {'title': 'Turning Tabular Foundation Models into Graph ...',\n",
       "  'url': 'https://arxiv.org/html/2508.20906v1',\n",
       "  'snippet': '3 days ago — (2025) , a graph foundation model should satisfy three natural inductive biases: (i) feature permutation invariance; (ii) label permutation ...',\n",
       "  'engine': 'google',\n",
       "  'authors': []},\n",
       " {'title': 'Learning a Graph Foundation Model from Riemannian ...',\n",
       "  'url': 'https://dl.acm.org/doi/10.1145/3696410.3714952',\n",
       "  'snippet': '22 Apr 2025 — ... graph foundation model with structural vocabulary. The key innovation is the discovery of a simple yet effective structural vocabulary of ...',\n",
       "  'engine': 'google',\n",
       "  'authors': []},\n",
       " {'title': 'How Expressive are Knowledge Graph Foundation Models?',\n",
       "  'url': 'https://icml.cc/virtual/2025/poster/44147',\n",
       "  'snippet': 'In ICLR, 2022. image. Cui, Y., Sun, Z., and Hu, W. A prompt-based knowledge graph foundation model for universal in-context reasoning. In NeurIPS, 2024 ...',\n",
       "  'engine': 'google',\n",
       "  'authors': []},\n",
       " {'title': 'Graph Foundation Models: Concepts, Opportunities and ...',\n",
       "  'url': 'https://www.computer.org/csdl/journal/tp/2025/06/10915556/24RViEuz34Q',\n",
       "  'snippet': 'by J Liu · 2025 · Cited by 17 — Definition: A graph foundation model (GFM) is a model that is expected to benefit from the pre-training of broad graph data, and can be adapted to a wide range ...',\n",
       "  'engine': 'google',\n",
       "  'authors': []},\n",
       " {'title': 'A Survey on Self-Supervised Graph Foundation Models',\n",
       "  'url': 'https://arxiv.org/pdf/2403.16137',\n",
       "  'snippet': 'by Z Zhao · 2024 — Graph model and graph foundation model (GFM). A graph model is an encoding function Z = f(G; Θ) that can be parameterized by GNNs [4]–[6] ...',\n",
       "  'engine': 'google',\n",
       "  'authors': []},\n",
       " {'title': 'A Prompt-Based Knowledge Graph Foundation Model for ...',\n",
       "  'url': 'https://neurips.cc/virtual/2024/poster/94900',\n",
       "  'snippet': 'A prompt-based knowledge graph foundation model for universal in-context reasoning. Yuanning Cui · Zequn Sun · Wei Hu',\n",
       "  'engine': 'google',\n",
       "  'authors': []},\n",
       " {'title': 'OpenGraph: Towards Open Graph Foundation Models',\n",
       "  'url': 'https://aclanthology.org/2024.findings-emnlp.132.pdf',\n",
       "  'snippet': 'by L Xia · 2024 · Cited by 52 — In this work, we pro- pose a novel graph foundation model, called. OpenGraph, to address this challenge. Our approach tackles several ...',\n",
       "  'engine': 'google',\n",
       "  'authors': []},\n",
       " {'title': 'AnyGraph: Graph Foundation Model in the Wild',\n",
       "  'url': 'https://www.researchgate.net/publication/383267044_AnyGraph_Graph_Foundation_Model_in_the_Wild',\n",
       "  'snippet': 'Given the ubiquity of graph data, it is intriguing to ask: Is it possible to train a graph foundation model on a broad range of graph data across diverse ...',\n",
       "  'engine': 'google',\n",
       "  'authors': []},\n",
       " {'title': 'GOFA: A Generative One-For-All Model for Joint Graph ...',\n",
       "  'url': 'https://proceedings.iclr.cc/paper_files/paper/2025/hash/652c104b5b0652a03684efeaf805463b-Abstract-Conference.html',\n",
       "  'snippet': 'However, unlike text and image data, graph data do not have a definitive structure, posing great challenges to developing a Graph Foundation Model (GFM).',\n",
       "  'engine': 'google',\n",
       "  'authors': []},\n",
       " {'title': 'Learning Generalities Across Graphs via Task-Trees',\n",
       "  'url': 'https://icml.cc/virtual/2025/poster/46113',\n",
       "  'snippet': 'To validate our framework, we introduce Graph Generality Identifier on Task-Trees (GIT), a graph foundation model that demonstrates strong performance on ...',\n",
       "  'engine': 'google',\n",
       "  'authors': []},\n",
       " {'title': 'Graph Foundation Models: Concepts, Opportunities and ...',\n",
       "  'url': 'https://arxiv.org/html/2310.11829v4',\n",
       "  'snippet': 'Definition A graph foundation model (GFM) is a model that is expected to benefit from the pre-training of broad graph data, and can be adapted to a wide range ...',\n",
       "  'engine': 'google',\n",
       "  'authors': []},\n",
       " {'title': 'Yuan FANG @ SMU - Publications',\n",
       "  'url': 'https://www.yfang.site/publications',\n",
       "  'snippet': 'Accepted by ICLR 2025. [Paper] [Code]. Exploring the Potential of Large ... SAMGPT: Text-free Graph Foundation Model for Multi-domain Pre-training and ...',\n",
       "  'engine': 'google',\n",
       "  'authors': []},\n",
       " {'title': 'Graph Foundation Models: Concepts, Opportunities and ...',\n",
       "  'url': 'http://www.shichuan.org/doc/196.pdf',\n",
       "  'snippet': 'by J Liu · Cited by 17 — Definition A graph foundation model (GFM) is a model that is expected to benefit from the pre-training of broad graph data, and can be adapted to a wide range ...',\n",
       "  'engine': 'google',\n",
       "  'authors': []},\n",
       " {'title': 'arXiv:2407.09709v2 [cs.LG] 24 Apr 2025',\n",
       "  'url': 'https://www.arxiv.org/pdf/2407.09709',\n",
       "  'snippet': 'by L Kong · 2024 · Cited by 23 — In this paper, we first identify three desirable properties of a graph foundation model (GFM), namely large-scale self-supervised pre-training, ...',\n",
       "  'engine': 'google',\n",
       "  'authors': []},\n",
       " {'title': 'Yao Ma',\n",
       "  'url': 'https://yaoma24.github.io/',\n",
       "  'snippet': '01/2025 Two papers on graph data vaulation accepted by ICLR 2025. 01 ... 02/2024 Check out our preprints on Graph Data Valuation, Graph Foundation Model ...',\n",
       "  'engine': 'google',\n",
       "  'authors': []},\n",
       " {'title': 'Boosting Graph Foundation Model from Structural ...',\n",
       "  'url': 'https://www.researchgate.net/publication/382654651_Boosting_Graph_Foundation_Model_from_Structural_Perspective',\n",
       "  'snippet': 'To address the problem, in this paper, we boost graph foundation model from structural perspective and propose BooG. The model constructs virtual super ...',\n",
       "  'engine': 'google',\n",
       "  'authors': []},\n",
       " {'title': 'Scalable training of trustworthy and energy-efficient ...',\n",
       "  'url': 'https://link.springer.com/article/10.1007/s11227-025-07029-9',\n",
       "  'snippet': 'by M Lupo Pasini · 2025 · Cited by 5 — ... graph foundation model, along with their curation and preprocessing. ... Accepted: 01 February 2025. Published: 14 March 2025. DOI : https ...',\n",
       "  'engine': 'google',\n",
       "  'authors': []},\n",
       " {'title': 'Yixin Liu: About Me',\n",
       "  'url': 'https://yixinliu233.github.io/',\n",
       "  'snippet': '2025/02: Our paper on graph foundation model has been accepted by PAKDD 2025. 2025/01: Our benchmark on graph-level anomaly/OOD detection has been accepted by ...',\n",
       "  'engine': 'google',\n",
       "  'authors': []},\n",
       " {'title': 'Investigating the Limits of Graph Foundation Model in Real- ...',\n",
       "  'url': 'https://link.springer.com/content/pdf/10.1007/978-981-96-8197-6_13.pdf?pdf=inline%20link',\n",
       "  'snippet': '3.1 Graph Foundation Model. The Graph Foundation Model (GFM) approach adapts GraphAny to our travel recommendation task, focusing on the bipartite structure ...',\n",
       "  'engine': 'google',\n",
       "  'authors': []},\n",
       " {'title': 'Lecheng Kong (孔乐成) - Homepage',\n",
       "  'url': 'https://lechengkong.github.io/',\n",
       "  'snippet': 'Very recent interests on developing universal graph foundation model! Our work accepted as ICLR-24 Spotlight is here. I am excited to talk about related ...',\n",
       "  'engine': 'google',\n",
       "  'authors': []}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_serp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 内容抓取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意：这个需要实际的网络连接和SearXNG服务\n",
    "fetch_sources = {}\n",
    "\n",
    "for serp_item in selected_serp:\n",
    "\turl = serp_item[\"url\"]\n",
    "\tsnippet = serp_item[\"snippet\"]\n",
    "\ttry:\n",
    "\t\ttext = search.fetch_text(url, max_chars=config.FETCH_MAX_CHARS, snippet=snippet)\n",
    "\t\t\n",
    "\t\tif len(text) < config.MIN_TEXT_LENGTH:\n",
    "\t\t\tcontinue\n",
    "\t\tfetch_sources[url] = text\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"[fetch-error] {url} -> {e}\")\n",
    "\t\tcontinue\n",
    "\tutils.safe_sleep(0.08)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'https://iclr.cc/virtual/2025/poster/28473': 'SNIPPET: 2025 ... However, unlike text and image data, graph data do not have a definitive structure, posing great challenges to developing a Graph Foundation Model (GFM).\\n\\nTITLE: Main Navigation\\n\\nBODY:\\nPoster\\nGOFA: A Generative One-For-All Model for Joint Graph Language Modeling\\nLecheng Kong · Jiarui Feng · Hao Liu · Chengsong Huang · Jiaxin Huang · Yixin Chen · Muhan Zhang\\nFoundation models, such as Large Language Models (LLMs) or Large Vision Models (LVMs), have emerged as one of the most powerful tools in the respective fields. However, unlike text and image data, graph data do not have a definitive structure, posing great challenges to developing a Graph Foundation Model (GFM). For example, current attempts at designing general graph models either transform graph data into a language format for LLM-based prediction or still train a GNN model with LLM as an assistant. The former can handle unlimited tasks, while the latter captures graph structure much better---yet, no existing work can achieve both simultaneously. In this paper, we first identify three key desirable properties of a GFM: self-supervised pretraining, fluidity in tasks, and graph awareness. To account for these properties, we extend the conventional language modeling to the graph domain and propose a novel generative graph language model GOFA. The model interleaves randomly initialized GNN layers into a frozen pre-trained LLM so that the semantic and structural modeling abilities are organically combined. GOFA is pre-trained on newly proposed graph-level next-word prediction, question-answering, structural understanding, and information retrieval tasks to obtain the above GFM properties. The pre-trained model is further instruction fine-tuned to obtain the task-solving ability. Our GOFA model is evaluated on various downstream datasets unseen during the pre-training and fine-tuning phases, demonstrating a strong ability to solve structural and contextual problems in zero-shot scenarios. The code is available at https://github.com/JiaruiFeng/GOFA.\\n\\nSOURCE: https://iclr.cc/virtual/2025/poster/28473',\n",
       " 'https://dl.acm.org/doi/10.1145/3711896.3736568': 'SNIPPET: by Z Wang · 2025 — Boosting Graph Foundation Model from Structural Perspective. arXiv (2024). Google Scholar. [18]. Yuanning Cui, Zequn Sun, and Wei Hu. 2025. A ...\\n\\n[FetchError] HTTP 403 for https://dl.acm.org/doi/10.1145/3711896.3736568\\n\\nSOURCE: https://dl.acm.org/doi/10.1145/3711896.3736568',\n",
       " 'https://proceedings.iclr.cc/paper_files/paper/2025/file/652c104b5b0652a03684efeaf805463b-Paper-Conference.pdf': 'SNIPPET: by L Kong · Cited by 24 — We introduce GOFA, a generative One-for-All graph foundation model. GOFA is pre-trained under a graph completion framework to enable large-scale self-supervised ...32 pages\\n\\nTITLE: (from PDF)\\n\\nBODY:\\nPublished as a conference paper at ICLR 2025\\n\\nGOFA: A GENERATIVE ONE-FOR-ALL MODEL FOR\\nJOINT GRAPH LANGUAGE MODELING\\n\\nLecheng Kong1∗ Jiarui Feng1∗ Hao Liu1∗ Chengsong Huang1 Jiaxin Huang1\\nYixin Chen1 Muhan Zhang2†\\n1Washington University in St. Louis 2Peking University\\n{jerry.kong, feng.jiarui, liuhao, chengsong, jiaxinh}@wustl.edu\\nychen25@wustl.edu, muhan@pku.edu.cn\\n\\nABSTRACT\\n\\nFoundation models, such as Large Language Models (LLMs) or Large Vision\\nModels (LVMs), have emerged as one of the most powerful tools in the respective\\nfields. However, unlike text and image data, graph data do not have a definitive\\nstructure, posing great challenges to developing a Graph Foundation Model (GFM).\\nFor example, current attempts at designing general graph models either transform\\ngraph data into a language format for LLM-based prediction or still train a GNN\\nmodel with LLM as an assistant. The former can handle unlimited tasks, while\\nthe latter captures graph structure much better—yet, no existing work can achieve\\nboth simultaneously. In this paper, we first identify three key desirable properties\\nof a GFM: self-supervised pretraining, fluidity in tasks, and graph awareness. To\\naccount for these properties, we extend the conventional language modeling to the\\ngraph domain and propose a novel generative graph language model GOFA. The\\nmodel interleaves randomly initialized GNN layers into a frozen pre-trained LLM\\nso that the semantic and structural modeling abilities are organically combined.\\nGOFA is pre-trained on newly proposed graph-level next-word prediction, question-\\nanswering, structural understanding, and information retrieval tasks to obtain the\\nabove GFM properties. The pre-trained model is further instruction fine-tuned to ob-\\ntain the task-solving ability. Our GOFA model is evaluated on various downstream\\ndatasets unseen during the pre-training and fine-tuning phases, demonstrating a\\nstrong ability to solve structural and contextual problems in zero-shot scenarios.\\nThe code is available at https://github.com/JiaruiFeng/GOFA.\\n\\n1\\n\\nINTRODUCTION\\n\\nWith the emergence of Large Language Models (LLMs), the field of artificial intelligence is undergo-\\ning a profound transformation, shifting from specialized, fragmented models to universal foundation\\nmodels. A foundation model is pre-trained on large-scale datasets and can be further adapted to\\ndiverse downstream tasks using fine-tuning (Hu et al., 2022) or in-context learning (Bommasani et al.,\\n2021; Touvron et al., 2023). Foundation models have been developed in different domains to handle\\ntext (Brown et al., 2020; Touvron et al., 2023), image (Kirillov et al., 2023; Bai et al., 2023), and\\neven multi-modal data (Zhang et al., 2023c; Li et al., 2023; Alayrac et al., 2022). Because of their\\nversatility and generalizability, foundation models have become prevalent in these domains.\\n\\nHowever, despite preliminary efforts, a foundation model in the graph domain has arguably yet to be\\nproposed. In the graph domain, data are highly flexible and dynamic. For example, social networks\\nreceive millions of new connections daily (Hardiman & Katzir, 2013), and novel molecules and\\nprotein structures are frequently discovered (Abramson et al., 2024; Gilmer et al., 2017). While\\npast researchers have proposed specialized models to learn graph data (Ying et al., 2021; Kipf &\\nWelling, 2017), the models require retraining to accommodate new graphs (Dai et al., 2022; Mo et al.,\\n2022). Moreover, trained models are usually tied to specific applications and cannot be generalized\\nto new domains and tasks. It becomes increasingly difficult for models to adjust to the ever-evolving\\n\\n∗Contributed equally. Listing order is random.\\n†Corresponding author\\n\\n1\\n\\n\\x0cPublished as a conference paper at ICLR 2025\\n\\nFigure 1: Examples of our pre-training tasks.\\n\\nnature of graph data. Hence, a graph foundation model (GFM) applicable to new domains/tasks\\nwith minimal or no adaptation costs is urgently needed, spurring recent endeavors to study general\\ngraph models. In particular, a strong zero-shot ability is both challenging and fascinating for GFM\\nresearchers.\\n\\nThe success of LLMs inspired a series of preliminary attempts which use LLMs to develop general\\ngraph models. They can be roughly divided into two categories: LLM as a predictor and LLM\\nas an enhancer (Chen et al., 2023). The LLM as a predictor approach transforms graph data\\ninto representations that LLMs can understand and use LLMs to generate predictions (Tang et al.,\\n2023). However, as suggested by a recent study (Wang et al., 2023), such an approach falls short of\\nunderstanding graph structures. This inspired the LLM as an enhancer approach, which adopts\\nLLM to process and unify diverse graph data and feeds them to a GNN to train general graph\\nmodels (Liu et al., 2023a; Huang et al., 2023a). Nevertheless, because GNN outputs fixed-sized\\nrepresentations/predictions, they can only handle specific tasks such as classification, and cannot\\ngeneralize to arbitrary, new tasks due to the lack of generation ability. In summary, the current two\\napproaches cannot fully utilize structural information and be generative simultaneously. We discuss\\nthe pros and cons of existing approaches in detail in Section 2.\\n\\nIn this paper, we first identify three desirable properties of a graph foundation model (GFM), namely\\nlarge-scale self-supervised pre-training, fluidity in tasks, and graph understanding. To achieve the\\nfirst property, we propose a generic graph self-supervised learning problem similar to the next-\\ntoken prediction problem in LLMs, allowing label-agnostic and continual training on highly diverse\\ngraph data. We then propose a generative model termed Generative One-For-All (GOFA) that\\ninterleaves GNN layers into an LLM to achieve the second and third properties. Such a novel design\\nsystematically integrates GNN into an LLM, granting the LLM graph structural learning ability\\nwhile keeping LLM’s original free-form text generation ability. Meanwhile, this design allows the\\npipeline of the original LLM to remain intact, giving GOFA a close-to-LLM level of task fluidity. We\\npre-train the model with large-scale real-world graph data, Question-Answer (QA) chain data adopted\\nfrom the NLP domain, and graph structural data to empower the model with the aforementioned\\nfoundational abilities in the graph domain (Examples in Figure 1). After pre-training, we further\\ninstruction fine-tune the model on a small amount of data (relative to the pre-training data) to make it\\nunderstand task formats. The fine-tuned model is finally evaluated on various downstream datasets\\nunseen during pre-training and fine-tuning. GOFA achieved impressive results on the zero-shot\\nscenario, which demonstrates the strong potential of GOFA to serve as a graph foundation model.\\n\\n2 A DESIRED FOUNDATION MODEL FOR GRAPH\\n\\nIn this section, we elaborate on three crucial properties a true graph foundation model should possess\\nto motivate our GOFA model design. We note that many contemporary works (partly) propose similar\\nideas to ours and thus we do not claim the credit. We kindly refer readers to the latest surveys (Liu\\net al., 2023b; Jin et al., 2023; Zhang et al., 2023d) for more discussions on GFMs.\\n\\n2\\n\\n📦📦📦📦💬💬💬💬🌐🌐🌐🌐📄📄📄Sentence Completion TaskQuestion Answering TaskStructural Understanding TaskInformation Retrieval TaskWhich type of Rock is commonly used for construction and why?Sedimentary rock. It is easy to extract, cut, and shape.Are there any other types of rocks used for construction?Yes. Igneous rocks like granite are used for their durability.QAQAPABCBACDPABCThis is [Node D]. Wikipedia entry: quickdraw. A graphics software …DThis is [Node C]. Wikipedia entry: system_7. Seventh major release of …AThis is [Node A].  Product: Wireless Controller for Switch or OLED… DThis is [Node D]. Product: Amazon Fire TV, 4-series 4K UHD smart TV…QAQAThis is [Node B]. Title: Attention is all you need. Abstract: The dominant sequence transduction models …Abstract: We present graph attention networks (GATs), novel neural network architectures that operate on graph …PDo certain regions or cultures have preference of rocks?Yes, limestone is commonly used in UK because it can withstand high levels of rainfall and humidity.PCompute the shortest path between [Node A] and [Node D] and generate all shortest paths from [Node A] to [Node D].The shortest path distance is 2. Shortest path: [Node A] -> [Node B] -> [Node D] .PATAG Raw TextPromptAnswerTAGTaskBThis is [Node A]. Title: Graph Attention Networks.CThis is [Node C]. Title: Adam: A method for stochastic optimization. Abstract: We introduce Adam, an algorithm for …Please output the content of [Node D] .Wikipedia entry: system_7. Seventh major release of theclassic Mac OSoperating systemforMacintosh …This is [Node A]. Wikipedia entry: unix. Unix is a family of multitasking…This is [Node B]. Product: Nintendo Switch with Blue and Red Joy-Con…APDCB📄DNo prompt for sentence completion task.\\x0cPublished as a conference paper at ICLR 2025\\n\\nLarge-Scale Self-Supervised Pre-training: One fundamental design of LLM is that it unifies all\\nNLP tasks into a single next-token-prediction paradigm, which enables self-supervised pre-training\\non a large corpus collected from different sources. For pre-training graph models, while numerous\\nefforts have been made from both the LLM as a predictor and LLM as an enhancer approaches,\\nthese attempts usually require the learning target to be labeled (Liu et al., 2023a; Chen et al., 2023).\\nHowever, a graph foundation model should have no constraint on the input graph (has labels or not)\\nand can learn cross-domain knowledge from large-scale graph data in a self-supervised fashion.\\n\\nFluidity in Tasks: A graph foundation model should also possess the same level of versatility and\\nfluidity in handling different tasks as an LLM. Specifically, such ability can be broken down into\\nthree levels: (a) The graph foundation model can naturally respond appropriately to different graph\\ntasks based on user instructions without requiring task-specific adjustment (e.g., the same model\\nperforms classification and question-answering tasks without any modification.) (b) With appropriate\\ninstruction-tuning, the model should have in-context learning ability on unseen tasks (e.g., a model\\ntuned on citation network also performs well on knowledge graphs with proper instructions). (c) Users\\nshould be able to define new, previously unseen tasks by modifying the graph structure and features\\nin a way that aligns with the universal input representation of the model. They can continuously train\\nthe model on new data without special adaptation. Existing approaches that use GNN models as the\\npredictors are usually either restricted in the output format (Liu et al., 2023a; Xia et al., 2024; He\\net al., 2024a) or need additional fine-tuning on the task head (Sun et al., 2023; Wang et al., 2022).\\nConsequently, despite having better structural modeling ability, such models cannot accommodate\\ntask changes or deal with novel tasks, e.g., shifting from a classification task to a question-answering\\ntask that requires outputting all shortest paths between two nodes.\\n\\nGraph Understanding: Since the LLM as a predictor approach uses a generative LLM to take\\ntext input and produce text output, it naturally has the fluidity to accept varied prompts to tackle\\ndifferent tasks. However, such an approach processes the structural information poorly (Wang et al.,\\n2023), making the utility of these models limited on many graph tasks. More importantly, even\\nthough some recent variants can use auxiliary graph models (such as GNNs) to incorporate structural\\ninformation (Tang et al., 2023; He & Hooi, 2024; Zhang et al., 2024), the graph models are frozen\\nand not responsive to different prompts, and the output from the graph models may not be the most\\nrelevant to the input prompt. On the contrary, a graph foundation model should account for the\\nunique structural information of graphs such as node degrees, shortest paths, common neighbors,\\netc., and generate graph representations dependent on the input prompt. It should not only have\\nLLM’s prompt learning capability but also learn graph structure and semantic information jointly.\\n\\n3 METHOD\\n\\nIn this section, we first propose a generative modeling framework for graphs, serving as the graph\\ncounterpart of traditional language modeling. Next, we introduce a novel GNN-LLM architecture for\\nthe proposed graph generative modeling problem. Finally, we describe the unified pre-training tasks\\nto train GOFA towards the proposed GFM properties.\\n\\n3.1 GENERATIVE MODELING FOR GRAPH\\n\\nUnifed task formats. A generative model usually takes existing contexts, such as user prompts\\nand passages, as input to generate conditional output related to the contexts, such as answers and\\ncompleted sentences. Defining unified input and output formats for tasks in language applications\\nis easy, as they are purely text-based. Further, because both the pre-training and downstream tasks\\nare constructed in the same format (i.e., next-token-prediction), the downstream tasks conveniently\\nadapt the knowledge from pre-training tasks, resulting in surprising capabilities, such as zero-shot\\nlearning. However, graph data from different domains vary significantly by input feature (e.g., nodes\\nin a citation network have completely different vector representations as nodes in a knowledge graph)\\nand output target, preventing direct knowledge transfer between tasks. Hence, the first challenge is\\nto define a unified format for graph tasks, such that the model can do large-scale self-supervised\\npre-training on arbitrary graphs and transfer to downstream tasks seamlessly.\\n\\nTo unify graph task input, we follow the previous work OFA (Liu et al., 2023a) and extend the\\ndefinition of Text-Attribute Graph (TAG) beyond graphs with text features such as citation and\\n\\n3\\n\\n\\x0cPublished as a conference paper at ICLR 2025\\n\\nproduct networks. In fact, any node and edge features can be represented by texts. For example, in\\nairline networks, airport and flight route details can be converted into textual descriptions for nodes\\nand edges. Non-textural features, like numerical data, can also be transformed into text strings, as in\\nLLMs. Even for graphs without any features, we can still attach sentences like \"The degree of this\\nnode is 3\" to nodes. Formally, a TAG is a graph G = {V, E, XV , XE} where V and E are the sets\\nof nodes and edges. Each node v ∈ V (edge e ∈ E) corresponds to a text description x(v) ∈ XV\\n(x(e) ∈ XE). Such a format encodes almost all existing graph data and serves well as a general input\\nrepresentation.\\n\\nFor self-supervised language modeling, the generated output essentially completes th\\n...[truncated]',\n",
       " 'https://arxiv.org/html/2508.20906v1': 'SNIPPET: 3 days ago — (2025) , a graph foundation model should satisfy three natural inductive biases: (i) feature permutation invariance; (ii) label permutation ...\\n\\nTITLE: Turning Tabular Foundation Models into Graph Foundation Models\\n\\nBODY:\\nTurning Tabular Foundation Models\\ninto Graph Foundation Models\\nAbstract\\nWhile foundation models have revolutionized such fields as natural language processing and computer vision, their application and potential within graph machine learning remain largely unexplored. One of the key challenges in designing graph foundation models (GFMs) is handling diverse node features that can vary across different graph datasets. Although many works on GFMs have been focused exclusively on text-attributed graphs, the problem of handling arbitrary features of other types in GFMs has not been fully addressed. However, this problem is not unique to the graph domain, as it also arises in the field of machine learning for tabular data. In this work, motivated by the recent success of tabular foundation models like TabPFNv2, we propose G2T-FM, a simple graph foundation model that employs TabPFNv2 as a backbone. Specifically, G2T-FM augments the original node features with neighborhood feature aggregation, adds structural embeddings, and then applies TabPFNv2 to the constructed node representations. Even in a fully in-context regime, our model achieves strong results, significantly outperforming publicly available GFMs and performing on par with well-tuned GNNs trained from scratch. Moreover, after finetuning, G2T-FM surpasses well-tuned GNN baselines, highlighting the potential of the proposed approach. More broadly, our paper reveals a previously overlooked direction of utilizing tabular foundation models for graph machine learning tasks.111Our source code is available at https://github.com/yandex-research/G2T-FM.\\n1 Introduction\\nIn recent years, foundation models have become a major breakthrough in deep learning. Foundation models are large machine learning models that are pretrained on diverse and extensive datasets. After this pretraining phase, they can be easily adapted to a variety of specific tasks with minimal additional training. Well-known examples include BERT (Devlin et al., 2019) and GPT (Brown et al., 2020) in natural language processing, as well as CLIP (Radford et al., 2021) in computer vision. The core principle behind foundation models is to learn general representations by leveraging large and varied data. These representations capture important patterns and semantics in the data, making the pretrained models highly transferable to different tasks. As a result, foundation models consistently achieve state-of-the-art results, while also improving efficiency and generalization. Furthermore, these models unify techniques across different fields, driving rapid progress and innovation in deep learning research and applications.\\nDespite their remarkable success in such areas as computer vision and natural language processing, the development of foundation models for graph data has been less advanced. The challenges of developing graph foundation models (GFMs) stem from the fact that graphs are not actually a single domain, but rather a way to represent data from different domains. These domains use graphs to represent very different structures, such as social networks, web networks, road networks, co-purchasing networks, molecules, connectomes, or even abstract objects and their relations. Thus, successful GFMs should be able to work with graphs from different domains representing very different objects with nodes and very different relations with edges, which is a rather formidable task that requires overcoming many serious challenges. Two key challenges faced by GFMs are the ability to transfer to new feature spaces and target spaces. Graphs from different domains often have different node features and different targets, making it difficult to design GFMs that can work across various types of graphs. Some existing GFMs restrict themselves to text-attributed graphs (Wang et al., 2024b; He & Hooi, 2024; Liu et al., 2024), which allows them to use pretrained text encoders. Another approach is to use simple dimensionality reduction methods like SVD and PCA (Xia & Huang, 2024; Zhao et al., 2024a; Wang et al., 2025a; Yu et al., 2025), which allow transforming all feature spaces to a space with a fixed predefined number of features. But these approaches do not allow for fully and effectively leveraging arbitrary node features in graphs from new domains.\\nHowever, we note that the challenges of transferring to new feature and target spaces are not exclusive to graphs. Tabular data — one of the most widespread data modalities in machine learning — is similar to graph-structured data in that it does not constitute a single domain but is rather a way to represent data from different domains, and different tabular datasets come with different feature and target spaces. Thus, tabular foundation models face similar issues to GFMs. While tabular foundation models are not as developed as foundation models for language or vision, they have seen increased interest recently (Van Breugel & Van Der Schaar, 2024), and the first successful approaches have been proposed (Hollmann et al., 2023; 2025; Mueller et al., 2025; Ma et al., 2024; Qu et al., 2025). For instance, TabPFNv2 (Hollmann et al., 2025) demonstrates strong performance in both in-context learning and finetuning scenarios, and it has recently gained significant attention from the community.\\nWe argue that GFM developers should take inspiration from tabular foundation model approaches, as they have to deal with many of the same problems. In this paper, we take a first step in this direction and show that tabular foundation models, such as TabPFNv2 (Hollmann et al., 2025), can be effectively adapted to graph datasets. We introduce a simple framework named Graph-to-Table Foundation Model (G2T-FM), which transforms graph tasks into tabular ones and solves them with a tabular foundation model. More specifically, we augment the original features with neighborhood feature aggregations (Bazhenov et al., 2025), classical structure-based features (node degree, PageRank, and the eigenvectors of the graph Laplacian), and learnable structure-based encodings (Kanatsoulis et al., 2025). Then, we apply TabPFNv2 to the constructed node representations.\\nOur empirical results indicate that this straightforward framework achieves strong results in a fully in-context regime, significantly outperforming existing publicly available GFMs and performing on par with well-tuned GNNs trained from scratch. Moreover, after finetuning, G2T-FM surpasses well-tuned GNN baselines, highlighting the potential of the proposed approach and the positive transfer brought by the usage of foundation models.\\nOur main contributions are as follows:\\n-\\n•\\nWe draw attention to a promising and previously overlooked direction of applying tabular foundation models to graph machine learning.\\n-\\n•\\nAs a proof of concept, we introduce G2T-FM, a simple framework that uses a tabular foundation model (TabPFNv2 in our experiments) as the backbone of a graph foundation model.\\n-\\n•\\nWe show that, despite its simplicity, G2T-FM is a strong baseline for GFMs, substantially outperforming the existing publicly available GFMs.\\n-\\n•\\nWe further demonstrate that finetuned G2T-FM surpasses traditional GNNs trained from scratch.\\nWe hope that our study will stimulate further development of generalizable and robust graph foundation models and encourage further adoption of tabular foundation models for graph-structured data, as well as possibly for other data modalities.\\n2 Background\\n2.1 Graph Foundation Models for Node Classification\\nGraph foundation models (GFMs) have recently gained significant attention in the field of graph machine learning. The main purpose of GFMs is to enable effective transfer of knowledge across different graph datasets. In other words, they aim to learn knowledge from a variety of graph tasks that can be successfully applied to other graphs.\\nIn this work, we primarily focus on node-level tasks such as node classification and node regression. While many GFMs are limited to text-attributed graphs (TAGs) (Wang et al., 2024b; He & Hooi, 2024; Liu et al., 2024), they are not truly general as graphs in many domains come with other feature types that are difficult to describe with text. Because of that, we specifically discuss methods that can be applied to graphs with arbitrary numerical and categorical node features which frequently appear in various real-world industrial applications.\\nIn the following sections, we review the key design choices and considerations in the development of GFMs. In particular, we focus on pretraining objectives and data, as well as how GFMs handle graph structure and node features. For further details, see the survey by Wang et al. (2025b).\\nPretraining objective\\nSome graph foundation models use self-supervised learning (SSL) objectives to guide their pretraining process (Zhao et al., 2024a; Yu et al., 2025; Wang et al., 2025a), whereas others employ supervised learning strategies (Lachi et al., 2024; Finkelshtein et al., 2025). Notably, several works (Xia et al., 2024; Xia & Huang, 2024) reduce the node classification task to link prediction. Specifically, for each label in a downstream task, they create a virtual node that is connected to all train nodes of that class. After that, node classification reduces to prediction of links to those virtual class nodes, and these models are pretrained on link prediction task.\\nPretraining data\\nCollecting a sufficiently diverse collection of datasets for pretraining graph foundation models remains a significant challenge. To address this, some studies (Fey et al., 2025; Lachi et al., 2024; Xia et al., 2024) incorporate synthetic data — either together with real-world data or as an alternative. This includes the use of simple random graph models like stochastic block models (Lachi et al., 2024), as well as synthetic graphs generated by large language models (Xia et al., 2024). However, the majority of graph foundation models rely primarily on real-world datasets for pretraining. The number of datasets used for this purpose varies widely, ranging from as few as one graph (Finkelshtein et al., 2025), to more moderate collections of 2–10 datasets (Zhao et al., 2024a; Wang et al., 2025a), and up to several dozens in some studies (Xia & Huang, 2024; Lachi et al., 2024).\\nHandling features\\nOne of the key challenges for graph foundation models is handling heterogeneous features that can vary significantly across different datasets. Some approaches address this by focusing exclusively on text-attributed graphs (TAGs), sometimes additionally converting all features of other types to text, and then applying a text encoder (Wang et al., 2024b; He & Hooi, 2024; Liu et al., 2024). Methods aiming to deal with arbitrary features often rely on simple dimensionality reduction techniques such as SVD or PCA to obtain feature embeddings (Xia & Huang, 2024; Zhao et al., 2024a; Wang et al., 2025a; Yu et al., 2025). There are alternatives, such as learning dimension encoding modules that produce feature transformations (Zhao et al., 2024b) or replacing node attribute values with their statistical dependencies (Shen et al., 2025), but they seem to be less popular. We also highlight Finkelshtein et al. (2025), which constructs separate embeddings for each (node, feature) pair, enabling a more fine-grained representation of feature information.\\nHandling structure\\nHandling the structure is relatively easier, as graph neural networks are particularly well-suited for this task, and they are inherently capable of processing arbitrary graph structures. Consequently, many GFMs simply adopt GNNs as their backbone to handle graph structure (Zhao et al., 2024a; Finkelshtein et al., 2025; Yu et al., 2025; Wang et al., 2025a). In addition to GNN-based approaches, some methods use matrix decomposition techniques such as SVD applied to graph-derived matrices (for example, the normalized adjacency matrix or the sum of its powers), to encode structural information (Xia et al., 2024). However, while GNNs can in principle operate on any graph, their performance may still be limited due to varying graph structures. To address this, some works implement additional mechanisms specifically designed to handle structural differences (Yu et al., 2025; Wang et al., 2025a).\\n2.2 Limitations of Existing GFMs\\nFocus on text-attributed graphs\\nMany existing graph foundation models are specifically designed for text-attributed graphs, where nodes or edges have associated textual information (Wang et al., 2024b; He & Hooi, 2024; Liu et al., 2024). These models typically leverage large language models or other text encoders to process textual attributes, integrating natural language representations with graph structures. While this approach can be effective for certain domains such as academic networks or knowledge graphs, it limits the application of GFMs to a broader range of graphs where such text attributes are not available. For instance, for graphs representing transportation networks, biological networks, or transaction networks (commonly used for fraud detection tasks), which often come with rich numerical and categorical features, the reliance exclusively on textual information restricts the model’s usability and effectiveness. As a result, many current GFMs may not generalize well to graphs with non-textual attributes, hindering their adoption across diverse real-world scenarios.\\nLimited support for regression tasks\\nMost publicly available GFMs are designed and evaluated on classification tasks, where the goal is to predict categorical labels for nodes, edges, or entire graphs. To date, none of the popular GFMs besides TS-GNN (Finkelshtein et al., 2025) support regression tasks, where the output is a continuous value rather than a class label. This is a substantial limitation because many important graph-based applications require regression instead of classification. The lack of support for regression tasks reduces the practical applicability of current GFMs and highlights an important area for future research.\\nMisleading use of the “zero-shot” term\\nSome recent studies on graph foundation models have described their methods as operating in a “zero-shot” setting (Xia & Huang, 2024; Xia et al., 2024). Typically, these approaches introduce virtual nodes that represent target classes and connect them to the corresponding real nodes with known class labels. Then, the node classification problem reduces to predicting links between the test nodes and the appropriate virtual nodes. This process makes it possible to perform evaluation on unseen graphs without additional finetuning. While inventive and interesting, this technique does not truly realize zero-shot learning. Strictly speaki\\n...[truncated]',\n",
       " 'https://dl.acm.org/doi/10.1145/3696410.3714952': 'SNIPPET: 22 Apr 2025 — ... graph foundation model with structural vocabulary. The key innovation is the discovery of a simple yet effective structural vocabulary of ...\\n\\n[FetchError] HTTP 403 for https://dl.acm.org/doi/10.1145/3696410.3714952\\n\\nSOURCE: https://dl.acm.org/doi/10.1145/3696410.3714952',\n",
       " 'https://icml.cc/virtual/2025/poster/44147': 'SNIPPET: In ICLR, 2022. image. Cui, Y., Sun, Z., and Hu, W. A prompt-based knowledge graph foundation model for universal in-context reasoning. In NeurIPS, 2024 ...\\n\\nTITLE: Main Navigation\\n\\nBODY:\\nPoster\\nHow Expressive are Knowledge Graph Foundation Models?\\nXingyue Huang · Pablo Barcelo · Michael Bronstein · Ismail Ceylan · Mikhail Galkin · Juan Reutter · Miguel Romero Orth\\nEast Exhibition Hall A-B #E-3011\\nKnowledge Graph Foundation Models (KGFMs) are at the frontier for deep learning on knowledge graphs (KGs), as they can generalize to completely novel knowledge graphs with different relational vocabularies. Despite their empirical success, our theoretical understanding of KGFMs remains very limited. In this paper, we conduct a rigorous study of the expressive power of KGFMs. Specifically, we show that the expressive power of KGFMs directly depends on the motifs that are used to learn the relation representations. We then observe that the most typical motifs used in the existing literature are binary, as the representations are learned based on how pairs of relations interact, which limits the model\\'s expressiveness. As part of our study, we design more expressive KGFMs using richer motifs, which necessitate learning relation representations based on, e.g., how triples of relations interact with each other. Finally, we empirically validate our theoretical findings, showing that the use of richer motifs results in better performance on a wide range of datasets drawn from different domains.\\nKnowledge graphs foundation model (KGFM) can understand and conduct predictions on entirely new knowledge graphs, a structured way to represent information with entities (like people or places) and their relationships (like \"lives in\" or \"is part of\"). However, we still don’t fully understand why KGFMs work so well. In this paper, we dive into the theory behind them. We find that their success depends on the building blocks they use to understand relationships, what we call “motifs.” Current models mostly look at how two relationships interact, but we show that this is often not enough. By designing models that consider more complex patterns, like interactions between three or more relations, we can build a more expressive model that can distinguish more links in the knowledge graphs that previously could not be distinguished. We test these ideas and find that these new models do better across a variety of real-world datasets.\\n\\nSOURCE: https://icml.cc/virtual/2025/poster/44147',\n",
       " 'https://www.computer.org/csdl/journal/tp/2025/06/10915556/24RViEuz34Q': 'SNIPPET: by J Liu · 2025 · Cited by 17 — Definition: A graph foundation model (GFM) is a model that is expected to benefit from the pre-training of broad graph data, and can be adapted to a wide range ...\\n\\nTITLE: CSDL\\n\\nBODY:\\nCSDL | IEEE Computer Society\\n\\nSOURCE: https://www.computer.org/csdl/journal/tp/2025/06/10915556/24RViEuz34Q',\n",
       " 'https://arxiv.org/pdf/2403.16137': 'SNIPPET: by Z Zhao · 2024 — Graph model and graph foundation model (GFM). A graph model is an encoding function Z = f(G; Θ) that can be parameterized by GNNs [4]–[6] ...\\n\\nTITLE: (from PDF)\\n\\nBODY:\\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\\n\\n1\\n\\nA Survey on Self-Supervised Graph Foundation\\nModels: Knowledge-Based Perspective\\n\\nZiwen Zhao†, Yixin Su†, Yuhua Li(cid:0), Yixiong Zou, Ruixuan Li, and Rui Zhang(cid:0)\\n\\n5\\n2\\n0\\n2\\n\\ny\\na\\nM\\n6\\n\\n]\\n\\nG\\nL\\n.\\ns\\nc\\n[\\n\\n3\\nv\\n7\\n3\\n1\\n6\\n1\\n.\\n3\\n0\\n4\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\n✦\\n\\nAbstract—The field of graph foundation models (GFMs) has seen a\\ndramatic rise in interest in recent years. Their powerful generalization\\nability is believed to be endowed by self-supervised pre-training and\\ndownstream tuning techniques. There is a wide variety of knowledge\\npatterns embedded in the graph data, such as node properties and\\nclusters, which are crucial for learning generalized representations for\\nGFMs. We present a comprehensive survey of self-supervised GFMs\\nfrom a novel knowledge-based perspective. Our main contribution is\\na knowledge-based taxonomy that categorizes self-supervised graph\\nmodels by the specific graph knowledge utilized: microscopic (nodes,\\nlinks, etc.), mesoscopic (context, clusters, etc.), and macroscopic (global\\nstructure, manifolds, etc.). It covers a total of 9 knowledge categories\\nand 300 references for self-supervised pre-training as well as various\\ndownstream tuning strategies. Such a knowledge-based taxonomy al-\\nlows us to more clearly re-examine potential GFM architectures, includ-\\ning large language models (LLMs), as well as provide deeper insights\\nfor constructing future GFMs.\\n\\nIndex Terms—Graph foundation models, self-supervised learning, pre-\\ntraining, graph neural networks, large language models\\n\\nFig. 1. How self-supervised GFMs are believed to work: pre-training and\\ndownstream tuning. Updating the pre-trained model during downstream\\ntuning is optional depending on the tuning strategy.\\n\\n1 INTRODUCTION\\n\\nG RAPHS are prevalent in various real-world applica-\\n\\ntions. They exhibit diverse knowledge patterns due\\nto the inherent topology [1]–[3]. Moreover, the availability\\nof features and properties associated with nodes and links,\\nsuch as textual attributes and centrality measures, further\\nenriches the knowledge present in graphs. Over time, deep\\ngraph mining techniques have evolved from graph neural\\nnetworks (GNNs) [4]–[6] to graph Transformers (GTs) [7], [8]\\nand more recent large language model (LLM)-based graph\\nlanguage models [9]–[11]. They are motivated by capturing\\nmore comprehensive knowledge patterns within the graph\\ndata, from local relationships to the global structure.\\n\\n• Z. Zhao, Y. Su, Y. Li, Y. Zou, R. Li, and R. Zhang are with School of\\nComputer Science and Technology, Huazhong University of Science and\\nTechnology. E-mail: {zwzhao, idcliyuhua, yixiongz, rxli}@hust.edu.cn,\\nyixin.su@outlook.com, rayteam@yeah.net (www.ruizhang.info).\\n\\n† Z. Zhao and Y. Su are co-first authors.\\n(cid:0) Y. Li and R. Zhang are corresponding authors.\\nThis work is supported by the National Key Research and Development\\nProgram of China under grant 2024YFC3307900; the National Natural Sci-\\nence Foundation of China under grants 62436003, 62376103, 62206102 and\\n62302184; the Science and Technology Support Program of Hubei Province\\nunder grant 2022BAA046; Hubei science and technology talent service project\\nunder grant 2024DJC078; Ant Group through CCF-Ant Research Fund; and\\nthe HPC Platform of Huazhong University of Science and Technology.\\n\\nHowever, when confronted with various downstream\\ntask requirements, researchers often encounter graph data\\nthat lacks available labels, such as the field of an article\\nin citation networks. Fortunately, self-supervised learning on\\ngraphs has emerged as a powerful approach to uncovering\\nunderlying patterns in enormous unannotated data [12],\\n[13]. SSL methods design unsupervised tasks – pretext tasks –\\nto pre-train a graph model, and adapt the pre-trained model\\nto the specific application scenarios by downstream tuning\\napproaches, as depicted in Fig. 1. Researchers have observed\\npowerful generalization ability within graph models pre-\\ntrained with self-supervision [14], as they aim to mine the\\nunderlying knowledge patterns of graph data instead of\\nsolely relying on manual labels that are limited to spe-\\ncific task spaces. Therefore, self-supervised pre-training and\\ndownstream tuning are believed to be the most promising\\ntechniques to achieve a graph foundation model (GFM) – a\\nhighly generalized model that can handle a wide range of\\napplication tasks [15].\\n\\nPrevious efforts. The popularity of self-supervised\\nlearning and LLMs on graphs in recent years has given\\nrise to a flood of surveys. Early efforts [3], [16], [17] fo-\\ncus on summarizing general self-supervised graph models.\\n[18]–[21] systematically summarize the trending direction\\nof graphs meet LLMs, shortly after the sensational de-\\nbut of ChatGPT. The success of LLMs has also activated\\n\\n(unlabeled)pre-training datadownstream data (w/ or w/o labels)update(II)downstream tuningmodel to be pre-trainedpre-trained modeldownstreambranchesupdate......Graph foundation model(I)pre-training(i)GNN(ii)GraphTransformer(ii)GraphTransformer(iii)Graphlanguage model(iii)Graphlanguage model...pretext tasksdownstream task 1downstream task 2 \\n \\n \\n \\n \\n \\n\\x0cJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\\n\\nheated discussions towards GFMs [14], [22], summarizing\\nkey techniques and principles of learning generalized graph\\nmodels and providing outlooks towards the realization of\\nGFMs. Despite the promising work, we reveal three major\\nshortcomings of the existing surveys:\\n\\n(1) Lack of comprehensiveness: existing surveys in the field\\nof self-supervised graph learning [3], [16], [17] do not cover\\nthe latest progress in this fast-developing field. For example,\\nnone of these surveys have discussed the new achievements\\nof masked graph autoencoders [23] and learning graph\\nmanifolds [24]. A recent survey [25] includes cutting-edge\\ndevelopments in graph contrastive learning, yet it focuses\\non real-world applications rather than realizing GFMs.\\n\\n(2) Unclear categorization: existing surveys [3], [14], [17]\\nbroadly categorize graph pre-training methods as “genera-\\ntive – contrastive (predictive)”. This rough categorization is\\ninsufficient to capture the unique characteristics of graphs,\\nwhich have diverse knowledge patterns embedded in their\\nstructure and properties. For instance, predicting links re-\\nquires local relationships between nodes, whereas predict-\\ning clusters requires the node distribution on the entire\\ngraph. However, both generative and contrastive (predic-\\ntive) frameworks can utilize the knowledge of links [8], [12]\\nand clusters [26], [27], which the aforementioned taxonomy\\nfails to distinguish. On the other hand, recent surveys of\\nGFMs only give a brief summary of existing pre-training\\nand downstream tuning methods: [14] puts the emphasis\\non the architecture design of graph models, while [18], [22]\\nare closer to outlooks towards future directions of GFMs.\\n\\n(3) Limited to specific architectures: the aforementioned\\ngraph self-supervised learning surveys are limited to\\nGNNs/GTs only. On the other hand, LLM-based sur-\\nveys [19]–[21] overemphasize the language model archi-\\ntectures and textual attributes of graphs while overlooking\\nother structural patterns. A recent GFM survey [14] catego-\\nrizes existing studies into three groups of GNN, LLM, and\\nGNN+LLM, still limited by specific backbone architectures\\ninstead of an in-depth perspective towards the ultimate\\ngoal – mining generalized graph knowledge. As language\\nmodels are not designed for mining various types of graph\\nknowledge, it still remains an unanswered question if LLMs\\nare ideal architectures for GFMs. If other promising gener-\\nalized architectures showed up in the near future (which\\nis happening right now), their architecture-based taxonomy\\nmight no longer apply.\\n\\nOur contributions. Considering the aforementioned is-\\nsues, it is necessary to provide a comprehensive survey of\\nself-supervised graph models with a clearer categorization\\nand taxonomy, which will offer a better understanding and\\ngreater insight into how future GFMs work. We first propose\\na knowledge-based taxonomy that categorizes self-supervised\\ngraph pre-training based on the types of knowledge uti-\\nlized: microscopic pre-training (Section 3) that focuses on\\nindividual nodes and links; mesoscopic pre-training (Sec-\\ntion 4) that focuses on local relationships in the graph,\\nsuch as context and clusters; and macroscopic pre-training\\n(Section 5) that focuses on the structure and the manifold\\nunderlying the entire graph. Such a knowledge-based tax-\\nonomy provides a unified perspective to analyze the pre-\\ntraining and downstream tuning strategies (Section 6) of\\nboth GNNs/GTs and recent graph language models (Section\\n\\n2\\n\\n7), providing valuable insights for the future directions of\\nGFMs (Section 8). Our knowledge-based perspective is also\\narchitecture-agnostic, compared to existing surveys which\\nare applicable to only certain types of architectures. There-\\nfore, we provide a more systematic view covering a much\\nwider range of graph models. As illustrated in Fig. 2, we\\nanalyze 9 knowledge categories and 300 references ranging\\nfrom the 2010s to 2025, which are to our knowledge the most\\ndetailed categorization of self-supervised GFMs. All papers\\nincluded are summarized as tables in Appendix A for\\nbetter comparison. We also collate more than 500 relevant\\npapers and list them on GitHub1. We hope this survey will\\nhelp researchers exploit more powerful GFMs by exploiting\\ngraph-specific knowledge.\\n\\n2 PRELIMINARY\\nThis section provides basic concepts related to our topic.\\n\\nGraph. Graph is a data structure consisting of a node\\n(vertex) set and an edge (link) set G = (V, E). The ad-\\njacency matrix A ∈ {0, 1}n×n indicates if two nodes are\\nconnected by a link. For an attributed graph, each node\\nis associated with a row of the feature matrix X ∈ Rn×d.\\nFor every node i ∈ V, its (undirected) neighborhood is\\nNi = {j ∈ V|Ai,j = 1}. A graph dataset can contain one\\ngraph only or multiple relatively small graphs.\\n\\nGraph model and graph foundation model (GFM). A graph\\nmodel is an encoding function Z = f (G; Θ) that can be\\nparameterized by GNNs [4]–[6], GTs [7], [8], graph language\\nmodels [9]–[11], etc. A GFM is an (ideal) graph model pre-\\ntrained on various kinds of unsupervised graph data to\\nhandle various types of graph-related tasks [14].\\n\\nPre-training task (pretext). A pretext L ∈ T is a self-\\nsupervised task performed during the pre-training phase\\nof a graph model, where T represents the pretext task set.\\nA pretext should meet two conditions: (1) during the self-\\nsupervised pre-training, no manual-labeled data is used;\\n(2) its goal is to achieve improved performance on one or\\nmultiple downstream tasks ˇL:\\n\\n(cid:88)\\n\\nˇL∈ ˇT\\n\\nmin\\nΦ,Θ∗\\n\\nˇL( ˇf · f ∗, ˇG, ˇY), s.t. f ∗ =\\n\\n(cid:88)\\n\\nL∈T\\n\\narg min\\n\\nΘ\\n\\nL(f, G) (1)\\n\\nˇf ( ˇG; Φ) denotes\\n\\nsome optional downstream\\nwhere\\nbranches. “Θ∗” is optional depending on whether the pre-\\ntrained model parameters are tuned for downstream tasks.\\n“ ˇY” is also optional depending on whether task-specific\\nlabels are used, also known as supervised fine-tuning (SFT).\\n\\n3 MICROSCOPIC PRE-TRAINING TASKS\\nMicroscopic pre-training tasks treat nodes or edges as indi-\\nvidual instances. They extract features, properties, and local\\nrelationships between these instances.\\n\\n3.1 Node Features\\n\\nNode features are a rich source of semantic information\\nin attributed graphs, encoding domain-specific knowledge\\nsuch as textual content in citation networks or chemical\\nproperties in molecular graphs. The expressiveness and\\n\\n1. https://github.com/Newiz430/Pretext\\n\\n\\x0cJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\\n\\n3\\n\\nFeature prediction\\n\\nMGAE [28], GALA [29], Graph-Bert [30], GMI [31], AttrMask [32], GraphComp [33],\\nAttributeMask [34], LaGraph [35], SLAPS [36], GPT-GNN [37], GraphMAE [23],\\nGraphMAE2 [38], HGMAE [39], Mole-BERT [40], DiscoGNN [41]\\n\\nNode features\\n(Section 3.1)\\n\\nNode instance discrimination\\n\\nGRACE [42], GCA [43], ProGCL [44], MERIT [45], COSTA [46], CGI [47],\\nBGRL [48], SGRL [49], SUGRL [50], SimGCL [51], LightGCL [52],\\nImGCL [53], GRADE [54], SP-GCL [55], HASH-CODE [56]\\n\\nMicroscopic tasks\\n(Section 3)\\n\\nNode properties\\n(Section 3.2)\\n\\nDimension discrimination\\n\\nG-BT [57], CCA-SSG [58], VICReg [59], LogDet [60]\\n\\nScoreRank [61], NodeProperty [34], NWR-GAE [62], MaskGAE [63], PIGAE [64], CenPre [65]\\n\\nMesoscopic tasks\\n(Section 4)\\n\\ng\\nn\\ni\\nn\\ni\\na\\nr\\nt\\n-\\ne\\nr\\np\\nh\\np\\na\\nr\\ng\\nd\\ne\\ns\\ni\\nv\\nr\\ne\\np\\nu\\ns\\n-\\nf\\nl\\ne\\nS\\n\\nLinks\\n(Section 3.3)\\n\\nContext\\n(Section 4.1)\\n\\nLong-range\\nsimilarities\\n(Section 4.2)\\n\\nMotifs\\n(Section 4.3)\\n\\nClusters\\n(Section 4.4)\\n\\nMacroscopic tasks\\n(Section 5)\\n\\nGlobal structure\\n(Section 5.1)\\n\\nManifolds\\n(Section 5.2)\\n\\nGAE [12], VGAE [12], ARGA [66], ARVGA [66], SIG-VAE [67], EdgeMask [34], GPPT [68], S2GAE [69],\\nMaskGAE [63], Bandana [70], CG3 [71], SuperGAT [72], PIGAE [64], ASD-VAE [73], D-VGAE [74]\\n\\nGraphSAGE [6], EGI [75], DGSI [76], COLES [77], GLEN [78], Self-Pro [79], ContextPred [32], GCC [80],\\nS3-CL [81], Graph-MLP [82], N2N [83], Subg-Con [84], AFGRL [85], HGRL [86], BSG [87]\\n\\nSimilarity prediction\\n\\nS2GRL [88], Graph-Bert [30], PairwiseDistance [34], PairwiseAttrSim [34], AGE [89]\\n\\nSimilarity graph alignment\\n\\nAM-GCN [90], DLR-GAE [91], ASP [92], MVMI-FT [93], AEGCL [94]\\n\\nGROVER [7], MGSSL [95], GraphFP [96], MoAMa [97], DGPM [98], MotifRGC [99], MICRO-Graph [100],\\nCTAug [101]\\n\\nNode clustering\\n\\nM3S [26], NodeCluster [33], GraphLoG [102], HomoGCL [103], MGSE [104],\\nCARL-G [105], CommDGI [27], S3-CL [81], DCGL [106]\\n\\nGraph partitioning\\n\\nClusterDetect [61], GraphPar [33], Distance2Clusters [34], DGVAE [107],\\nMask-GVAE [108], gCooL [109], CSGCL [110], StructComp [111]\\n\\nGraph instance discrimination\\n\\nGraphCL [112], JOAO [113], AD-GCL [114], SimGRACE [115],\\nDGI [13], InfoGraph [116], MVGRL [117], GRV [118], GGD [119],\\nD-SLA [120], CGC [121], SPAN [122]\\n\\nGraph similarity prediction\\n\\nKernelPred [123], D-SLA [120], HTML [124]\\n\\nHGCL [125], DSGC [24], SelfMGNN [126], Graph-JEPA [127], HDM-GAE [128], RiemannGFM [129]\\n\\nFig. 2. Our knowledge-based taxonomy of self-supervised graph pre-training with representative literature.\\n\\nutility of these features heavily depend on their origin and\\nthe encoding methods employed.\\n\\nFeature prediction. Feature prediction serves as a funda-\\nmental pretext task in graph autoencoding methods like\\nMGAE [28], GALA [29], and Graph-Bert [30]. These meth-\\nods reconstruct low-dimensional node representations and\\nmatch them with the original feature size, minimizing the\\nreconstruction error such as MSE: L = Ei∈V [∥X i − ˆX i∥2],\\nwhile GMI [31] maximizes the mutual information between\\nthe original graph and the output representations by a\\ndiscriminator network.\\n\\nAnother kind of prediction task focuses on feature de-\\nnoising, where noise is first added to the original features\\n˜X = X + ϵ, and then the \\n...[truncated]',\n",
       " 'https://neurips.cc/virtual/2024/poster/94900': 'SNIPPET: A prompt-based knowledge graph foundation model for universal in-context reasoning. Yuanning Cui · Zequn Sun · Wei Hu\\n\\nTITLE: Main Navigation\\n\\nBODY:\\nPoster\\nA Prompt-Based Knowledge Graph Foundation Model for Universal In-Context Reasoning\\nYuanning Cui · Zequn Sun · Wei Hu\\nExtensive knowledge graphs (KGs) have been constructed to facilitate knowledge-driven tasks across various scenarios. However, existing work usually develops separate reasoning models for different KGs, lacking the ability to generalize and transfer knowledge across diverse KGs and reasoning settings. In this paper, we propose a prompt-based KG foundation model via in-context learning, namely KG-ICL, to achieve a universal reasoning ability. Specifically, we introduce a prompt graph centered with a query-related example fact as context to understand the query relation. To encode prompt graphs with the generalization ability to unseen entities and relations in queries, we first propose a unified tokenizer that maps entities and relations in prompt graphs to predefined tokens. Then, we propose two message passing neural networks to perform prompt encoding and KG reasoning, respectively. We conduct evaluation on 43 different KGs in both transductive and inductive settings. Results indicate that the proposed KG-ICL outperforms baselines on most datasets, showcasing its outstanding generalization and universal reasoning capabilities. The source code is accessible on GitHub: https://github.com/nju-websoft/KG-ICL.\\n\\nSOURCE: https://neurips.cc/virtual/2024/poster/94900',\n",
       " 'https://aclanthology.org/2024.findings-emnlp.132.pdf': 'SNIPPET: by L Xia · 2024 · Cited by 52 — In this work, we pro- pose a novel graph foundation model, called. OpenGraph, to address this challenge. Our approach tackles several ...\\n\\nTITLE: (from PDF)\\n\\nBODY:\\n2365\\nFindings of the Association for Computational Linguistics: EMNLP 2024, pages 2365–2379\\nNovember 12-16, 2024 ©2024 Association for Computational Linguistics\\n\\nOpenGraph:TowardsOpenGraphFoundationModelsLianghaoXiaUniversityofHongKongaka_xia@foxmail.comBenKaoUniversityofHongKongkao@cs.hku.hkChaoHuang*UniversityofHongKongchuang7@hku.hkAbstractGraphlearninghasbecomeessentialinvari-ousdomains,includingrecommendationsys-temsandsocialnetworkanalysis.GraphNeu-ralNetworks(GNNs)haveemergedaspromis-ingtechniquesforencodingstructuralinfor-mationandimprovingperformanceintaskslikelinkpredictionandnodeclassification.However,akeychallengeremains:thedif-ficultyofgeneralizingtounseengraphdatawithdifferentproperties.Inthiswork,wepro-poseanovelgraphfoundationmodel,calledOpenGraph,toaddressthischallenge.Ourapproachtacklesseveraltechnicalobstacles.Firstly,weenhancedataaugmentationusingalargelanguagemodel(LLM)toovercomedatascarcityinreal-worldscenarios.Sec-ondly,weintroduceaunifiedgraphtokenizerthatenablesthemodeltogeneralizeeffectivelytodiversegraphdata,evenwhenencounter-ingunseenpropertiesduringtraining.Thirdly,ourdevelopedscalablegraphtransformercap-turesnode-wisedependencieswithintheglobaltopologicalcontext.Extensiveexperimentsvalidatetheeffectivenessofourframework.ByadaptingOpenGraphtonewgraphchar-acteristicsandcomprehendingdiversegraphs,ourapproachachievesremarkablezero-shotgraphlearningperformanceacrossvariousset-tings.Wereleasethemodelimplementationathttps://github.com/HKUDS/OpenGraph.1IntroductionGraphlearningisacrucialmethodologyinvari-ousfields,suchasrecommendersystems(Heetal.,2020),socialnetworkanalysis(Sankaretal.,2021),citationnetworks(Lvetal.,2021),andtransporta-tionnetworks(Wangetal.,2020).ByutilizingGraphNeuralNetworks(GNNs)andrecursivemessagepassing,wecapturethecomplexstruc-turesofgraphseffectively.GNNsleverageinter-dependenciesamongnodestoincorporatehigh-*ChaoHuangisthecorrespondingauthor.orderconnectivitiesintolearnedgraphrepresenta-tions(Yingetal.,2018;Jinetal.,2020).Aprimarychallengeincurrentend-to-endgraphneuralnetworksistheirheavyrelianceonscarceandlow-qualitylabeleddata(Liuetal.,2022;Jinetal.,2022).Toovercomethis,self-supervisedlearning(SSL)hasemergedasasolutionbylever-agingaugmentedself-supervisionsignals.Con-trastiveSSL,exemplifiedbyDGI(Veliˇckovi´cetal.,2018b)andGraphCL(Youetal.,2020),incorpo-ratescontrastiveobjectivesasself-supervisedalign-mentloss.RecentadvancementslikeJOAO(Youetal.,2021)andGCA(Zhuetal.,2021)automatecontrastivelearningthroughadaptiveaugmentation.ByintegratingSSLtechniques,weenhancegraphneuralnetworkswithlimitedlabeleddata.Graphpre-trainingexcelsatcapturingintrinsicgraphpropertiesbutstruggletoeffectivelygener-alizetodiversedownstreamdomains,particularlywhenfacedwithdistributionshifts(Sunetal.,2023;Wuetal.,2021c;Guietal.,2022).Forexample,inrecommendersystems,handlingpreviouslyunseenuserinteractiongraphsincold-startrecommenda-tionscenariosiscrucial(Chenetal.,2022a).Trans-ferringknowledgefrompre-trainedgraphdomainstootherdownstreamdomainsisdesirable(Zhangetal.,2022a).However,applyingthesemodelstounseengraphsresultsinsignificantperformancedeteriorationduetovariationsinnodesetsandre-lationsemanticsacrossdifferentscenarios.Recentresearchexploresprompt-tuningasatask-specificalternativetofine-tuning,bridgingthegapbetweenpre-traininganddownstreamobjec-tives(Sunetal.,2022;Liuetal.,2023;Fangetal.,2023).Theseapproachesalignthepre-trainedmodel’sunderstandingwithspecifictaskrequire-ments.However,practicalscenariosinvolvevari-ationsinnodesetsandfeaturesemanticsacrossdiversedownstreamgraphdomains.Furtherexplo-rationisneededtoenhancegraphmodels’general-izationandadaptabilitytoreal-worldgraphs.\\x0c2366\\n\\nThisworkaimstodevelopascalablegraphmodelthatenableszero-shotlearning,effectivelymakingaccuratepredictionsonunseengraphs.Buildingsuchamodelposessignificantchallenges.•C1:Domain-SpecificDataScarcity.Datascarcityisacommonchallengeacrossdown-streamdomaintasks,drivenbyfactorslikepri-vacyconcerns.Limitedavailabilityofdomain-specificuserbehaviorgraphsrestrictsdatacollec-tion.Therefore,developinglabel-lesslearningframeworkswithingraphmodelsiscrucialtoef-fectivelyunderstandthecontextofdownstreamtasksinthefaceofdatascarcity.•C2:NodeTokenSetShift.Akeychallengeinzero-shotgraphlearningistheshiftinnodetokensetsacrossgraphs.Thisrequiresthemodeltoreconcilevariationsinnodecharacteristics.Generatinguniversalgraphtokensiscrucialtoeffectivelyrepresentandcomprehenddiverseun-seengraphswithdifferenttopologicalcontexts.•C3:EfficientGraphDependencyModeling.Nodesinlarge-scalegraphshavecomplexdepen-dencies.Understandinglocalandglobalinter-dependenciesamongallnodesiscrucialforaccu-rateprediction.Efficientnode-wisedependencyencodingisvitaltoenhancetheperformanceandscalabilityofgraphmodels.PresentWork.Toovercomethechallenges,weintroduceagraphmodelforzero-shotlearningthatcapturesuniversalandtransferablestructuralpat-ternsacrossmultipledomains.ToaddressC1,weproposecombininglargelanguagemodels(LLMs)withdataaugmentationtechniquesforsyntheticgraphgeneration.Bygeneratingaugmentedgraphsresemblingreal-worldinstances,weenhancethepre-trainingprocessofOpenGraphandgainadeeperunderstandingofdownstreamtaskcontexts.Thisisachievedthroughtheintegrationoftree-of-promptregularizationwithGibbssampling.ToaddressC2,weproposeatopology-awaregraphtokenizerthatgeneratesuniversalgraphto-kensfromarbitrarygraphs.ForC3,wedevelopascalablegraphtransformerwithefficientself-attentionusinganchorsampling.Ourapproachensurescomputationalefficiencythroughatwo-stageself-attentionprocessandoptimizestrainingbyleveragingtokensequencesampling,reducingsequencelengthwhilepreservingcrucialcontext.Weconductedextensiveexperimentsondiversedatasets,demonstratingtheremarkablegeneraliza-tionabilitiesofourmodelacrossvarioussettings.2PreliminariesGraphLearning.AgraphG=(V,E,F)consistsofanodesetV=vi,anedgesetE=(vs,vt),andnodeattributesF∈R|V|×f.Graphlearningaimstoproducenoderepresentationsthatencodebothstructuralandattributeinformation.Theseem-beddingsareusedfortaskssuchaslinkpredictionandnodeclassification,involvingthepredictionofnodeconnectionsandcategories,respectively.Thecorrespondinglossestobeminimizedare:Llink=Xvs,vt(1−es,t)f(vs,vt)−es,tf(vs,vt),Lnode=−Xvs,ys(cid:16)f(vs,ys)/Xy′s̸=ysf(vs,y′s)(cid:17)(1)wherees,t∈{0,1}denotesthelinklabelfornodesvsandvt,andys∈Cindicatethegroundtruthcate-goryfornodevs.FunctionfdenotesthepredictionmodelwithlearnableparametersΘf.Zero-shotGraphLearning.Currentgraphmod-elsexcelinstandardtasksbutstruggletogeneralizeacrossdiversedomains.Theirperformancedete-riorateswhenappliedtonewgraphswithvaryingcharacteristics,suchasnodesetsandfeatures.Toaddresstheselimitations,wefocusonzero-shotgraphlearning,whereamodelistrainedonasetofgraphsandevaluatedondifferenttestgraphswithoutsharedgraphtokens.Itaimstoassessthemodel’sabilitytolearngeneralizedtopolog-icalstructuresandnode-wisedependencies.For-mally,weseektominimizetheerrormeasurementϵ(Gt,f),withargminfdenotingtheoptimization.Θf=argminΘfL({Gs},f),Vt∩Vs=∅,Et∩Es=∅,Rft̸=Rfs(2)Thisobjectiveistodevelopauniversalgraphmod-elingarchitecturef(·).ItsparametersΘfarelearnedbyminimiznggraphlearninglossesLonthetraininggraphs{Gs}.Notably,thetraininggraphsandtestgraphsGthavenocommonnodes,edges,ornodefeatures.Thispresentsauniquechallengeforthegraphmodeltohandlethesignifi-cantdistributionshiftthatoccursacrossdifferentgraphdomainswithentirelydistinctdatasets.3MethodologyThissectionpresentsthedesigndetailsofthepro-posedOpenGraphframework.Figure1givesanoverallillustrationforthemodel.Inappendix,we\\x0c2367\\n\\nelaborateonhowOpenGraphhandlesnodeclassi-fiction(A.1.1)andgraphfeatures(A.1.2),detailedconfigurationsofourscalablegraphtransformer(A.1.3),aswellasthegenerationalgorithms(A.2).3.1UnifiedGraphTokenizerTohandlediversegraphswithvaryingnodesandfeatures,ourgoalistodevelopagraphtokenizerthattransformsinputgraphsintounifiedtokense-quences:G→{ei}.Eachtokenrepresentsanodeaccompaniedbyasemanticvectorei.Byutiliz-ingasharedrepresentationspaceandaflexiblesequencestructure,weaimtostandardizedistribu-tionsacrossgraphs.Specifically,ourtokenizerusesthesmoothedadjacencymatrix˜A,andatopology-awareprojectionfunctionϕ:R|V|→Rd.3.1.1SmoothedHigh-OrderAdjacencyWestartwiththeoriginaladjacencymatrixA∈R|V|×|V|builtfromedgesE.Thesmoothingproce-durefortheadjacencymatrixisasfollows:˜A=¯A1+¯A2+···¯AL,¯A=D−12AD−12(3)Fornumericalstability,weuseLaplaciannormal-ization¯AwiththediagonaldegreematrixDofadjacencyA.Tocapturehigh-orderconnectivityandsparsenode-wiserelations,OpenGraphcom-bines¯Aatdifferentorders.Thisprovidesuswithtopologyinformationforfurtherprocessing,withLrepresentingthemaximumpowerorderconsidered.3.1.2Topology-awareProjectionwithArbitaryGraphsTohandlethevaryingdimensionality|V|×|V|ofadjacency˜A,OpenGraphappliesaprojectionfunc-tionϕ:R|V|→Rdtotransformtheadjacencyintosequencedata.Alargehiddendimensionalitydisusedtominimizeinformationloss.Previousresearchhasshownthatevenrandomprojectionswithlargedimensionscanachievesatisfactoryper-formance(Zhengetal.,2022).Topreservetopol-ogyinformation,weemployfastsingularvaluedecomposition(SVD)astheprojectionϕ.SVDisknownforitsefficiencyandeffectivenessinadja-cencycompression(JamaliandEster,2010).OurempiricalanalysisdemonstratesthattwoiterationsoffastSVDeffectivelypreservetopologyinforma-tionwithminimalcomputationaloverhead.Thegraphtokenizerperformsthefollowingoperationstocalculatetheresultingtokensequence:ev=ϕ(˜Av,:)=˜Av,:·LN((U√Λ∥V√Λ))(4)whereU,V∈R|V|×dandΛ∈Rd×dareobtainedfromSVD.Theconcatenationoperator∥combinestheminthehiddendimension.LayernormalizationfunctionLN(·)reducesnumericalvarianceacrossdatasets.Theresultingev∈Rdincorporatestopol-ogyinformationfrom˜Aandthetopology-awareprojectionϕ.Thisinformationstrengthenssubse-quentlearnableneuralnetworks.3.2ScalableGraphTransformerWiththeuniversaltopology-awaregraphtokens,thesubsequenttaskistoempowerourgraphmodeltograspthecomplexnode-wisedependencieswithintheglobalcontext.Inspiredbythesuccessoftransformerarchitecturesinmodelingcomplexre-lationshipsbetweeninstances,OpenGraphutilizesagraphtransformerasthebackbone.Toensurescalabilityandeffectivenessforlarge-scalegraphs,weintroducethefollowingtechniques.TokenSequenceSampling.Forefficiency,wetrainthegraphtransformerusingsampledtokensequencesfromthecurrenttrainingbatch,whichcontainscentricnodesvcb,positivenodesvpb,andnegativenodesvnb.Theinputisasfollows:(ec1···ecB)∥(ep1···epB)∥(en1···enB)(5)Thisapproachsignificantlyreducesthesequencelengthfrom|V|to3×B,enablingefficienttrain-ingforlarge-scalegraphs.Despiteusingasub-sequence,thetopology-awareembeddingscontainlocalstructuralinformationforeachnodeandre-flecttheoverallgraphstructure.Additionally,thissamplingtechniqueemphasizesthecurrenttrainingbatch,leadingtofurthertrainingimprovements.EfficientSelf-AttentionwithAnchors.Toac-celeratetheself-attentionpartofOpenGraphwithquadraticcomplexity,weintroduceastepofsam-plinganchornodesvasfors∈S,whereS<3B.Thissplitstheself-attentionprocessintotwostages:propagatingmessagesfromallnodestotheanchornodesandthenpropagatingtheanchorembeddingstoallnodes.Thisdecompositionreducesthecom-plexityfromO(B2×d)toO(B×S),ensuringscalabilityforlarge-scalegraphs.3.3KnowledgeDistillationfromLLMObtainingdiversegraphdatasetsfordifferentdo-mainscanbechallengingduetofactorslikeprivacyissuesthatrestrictaccesstoessentialdata(Zhel-evaandGetoor,2007).Inspiredbytheremark-ableknowledgeandunderstandingdemonstrated\\x0c2368\\n\\nSVDℝ!ℝ\"Unified Graph TokenizerGraph TokensGraphsScalable Graph Transformer×××PosNegToken SamplingAnchor Sampling𝑞𝑘𝑣𝐻𝑑𝜎AnchorsMulti-head AttKnowledge Distillation from Large Language ModelsNode GenerationReal-world Instances----------------------------------TextGeneralNodeDivideDivideEdge Generation------------Gibbs Sampling𝑝Text Sim𝑛#𝑛$Locality𝑝NormT’Graph Pattern InjectionRegenerate withGCN EmbeddingAdjL𝑁Train BatchFigure1:OverallmodelarchitectureoftheOpenGraphframework.bylargelanguagemodels(LLMs),weleveragetheirpowertoenhancethegenerationofdiversegraph-structureddata.ToimprovetheefficacyofourLLM-augmentedgraphdataforpre-trainingourmodel,wehavedevelopedanaugmentationmechanism.ThismechanismenablestheLLM-augmentedgraphdatatocloselyapproximatereal-worldgraphcharacteristics,enhancingtherele-vanceandusefulnessoftheaugmenteddata.3.3.1LLM-basedNodeGenerationOurfirststepistocreateanodesettailoredtotheapplicationscenario,characterizedbytext-basedprofilesthatgeneratesubsequentedges.However,dealingwithreal-worldscenariosposeschallengesduetothelargescaleofthenodeset.Forexam-ple,e-commerceplatformsmayhavebillionsofproducts,makingitchallengingfortheLLMtoefficientlygeneratealargenumberofnodes.Toaddressthischallenge,weadoptanitera-tivestrategyofdividinggeneralnodesintosub-categorieswithfinersemanticgranularity.Forin-stance,inthecaseofgeneratingproductnodes,weprompttheLLMwithaquerylike\"Listsub-categoriesofproductsonplatformslikeAma-zon.\"TheLLMprovidesalistofsub-categoriessuchas\"clothing\"and\"electronics.\"Werepeatthisiterativedivisionprocess,refiningeachsub-categoryfurther,untilweobtainnodesthatresem-blereal-worldinstances,suchas\"women’scloth-ing,\"\"sweaters,\"\"hoodedsweaters,\"and\"whitehoodedsweaters.\"AppendixA.2.1presentsdetailsonourprompttemplateandgenerationexamples.Tree-of-PromptAlgorithm.Theprocessofdivid-ingnodesintosub-categoriesandgeneratingfine-grainedentitiesfollowsatreestructure.Theinitialgeneralnode(e.g.,\"products,\"\"deeplearningpa-pers\")servesastheroot,andfine-grainedentitiesactasleafnodes.Weemployatree-of-promptstrategytotraverseandgeneratethesenodes.Forfurtherdetails,pleaseseeAppendixA.2.2.3.3.2EdgeSamplingusingNodeProfilesTogenerateedges,weusetheGibbssamplingal-gorithm(Gelfand,2000)withthegeneratednodesetV.Thealgorithmstartswitharandomsam-ple.Forinstance,inapaper-wisecitationnetwork,theinitialsampleisanodepair(vs0,vt0).Inaperson-entityrelationscenariolikeanauthor-papernetwork,theinitialsampleisabinaryvectora0∈{0,1}|V|.Eachelementai∈a0indicateswhetherthereisaninteractionbetweenthesampledpersonandthei-thnodevi.Inthecaseofperson-entityrelations,theGibbsalgorithmforedgesam-plingisdescribedinAppendixA.2.3.Thekeyisestimatingtheprobabilityp(at⊕vt′|at),with⊕representingsettingthet′-thdimensionofatto1.Node-wiseConnectionProbabilityEstimation.Toestimatetheprobabilityp(at⊕vt′|at)ofcon-nectingtwonodesinourgeneratedgraph,welever-agethereasoningcapabilitiesoftheLLM.How-ever,directlypromptingtheLLMforpredictionsoneachedgecanbecomputationallyexpensive,withO(|V|×|V|)promptsrequired.Toensureefficiency,weadoptanalternativeapproach.WeprompttheLLMtogeneratehiddenrepresenta-tionshiforeachnodevi.Then,wecalculatetheprobabilityforeachedgewithdot-productas:p(at⊕vt′|at)=Xviati(hi/∥at∥0)⊤·ht′(6)Byutilizingthetextembeddingshiandht′pro-videdbytheLLM,wecaneffectivelycapturethesemanticrelationsbetweentherespectivenodes.DynamicProbabilityNormalization.Toensurethatthecalculatedprobabilityscoresfallwithinareasonabl\\n...[truncated]',\n",
       " 'https://www.researchgate.net/publication/383267044_AnyGraph_Graph_Foundation_Model_in_the_Wild': 'SNIPPET: Given the ubiquity of graph data, it is intriguing to ask: Is it possible to train a graph foundation model on a broad range of graph data across diverse ...\\n\\nTITLE: 383267044 AnyGraph Graph Foundation Model in the Wild\\n\\nSOURCE: https://www.researchgate.net/publication/383267044_AnyGraph_Graph_Foundation_Model_in_the_Wild',\n",
       " 'https://proceedings.iclr.cc/paper_files/paper/2025/hash/652c104b5b0652a03684efeaf805463b-Abstract-Conference.html': 'SNIPPET: However, unlike text and image data, graph data do not have a definitive structure, posing great challenges to developing a Graph Foundation Model (GFM).\\n\\nTITLE: GOFA: A Generative One-For-All Model for Joint Graph Language Modeling\\n\\nBODY:\\nPart of International Conference on Representation Learning 2025 (ICLR 2025) Conference\\nLecheng Kong, Jiarui Feng, Hao Liu, Chengsong Huang, Jiaxin Huang, Yixin Chen, Muhan Zhang\\nFoundation models, such as Large Language Models (LLMs) or Large Vision Models (LVMs), have emerged as one of the most powerful tools in the respective fields. However, unlike text and image data, graph data do not have a definitive structure, posing great challenges to developing a Graph Foundation Model (GFM). For example, current attempts at designing general graph models either transform graph data into a language format for LLM-based prediction or still train a GNN model with LLM as an assistant. The former can handle unlimited tasks, while the latter captures graph structure much better---yet, no existing work can achieve both simultaneously. In this paper, we first identify three key desirable properties of a GFM: self-supervised pretraining, fluidity in tasks, and graph awareness. To account for these properties, we extend the conventional language modeling to the graph domain and propose a novel generative graph language model GOFA. The model interleaves randomly initialized GNN layers into a frozen pre-trained LLM so that the semantic and structural modeling abilities are organically combined. GOFA is pre-trained on newly proposed graph-level next-word prediction, question-answering, structural understanding, and information retrieval tasks to obtain the above GFM properties. The pre-trained model is further instruction fine-tuned to obtain the task-solving ability. Our GOFA model is evaluated on various downstream datasets unseen during the pre-training and fine-tuning phases, demonstrating a strong ability to solve structural and contextual problems in zero-shot scenarios. The code is available at https://github.com/JiaruiFeng/GOFA.\\n\\nSOURCE: https://proceedings.iclr.cc/paper_files/paper/2025/hash/652c104b5b0652a03684efeaf805463b-Abstract-Conference.html',\n",
       " 'https://icml.cc/virtual/2025/poster/46113': 'SNIPPET: To validate our framework, we introduce Graph Generality Identifier on Task-Trees (GIT), a graph foundation model that demonstrates strong performance on ...\\n\\nTITLE: Main Navigation\\n\\nBODY:\\nPoster\\nTowards Graph Foundation Models: Learning Generalities Across Graphs via Task-Trees\\nZehong Wang · Zheyuan Zhang · Tianyi MA · Nitesh Chawla · Chuxu Zhang · Yanfang Ye\\nEast Exhibition Hall A-B #E-3211\\nFoundation models are pretrained on large-scale corpora to learn generalizable patterns across domains and tasks---such as contours, textures, and edges in images, or tokens and sentences in text. In contrast, discovering such generalities in graph-structured data, especially across heterogeneous graph tasks, remains an open challenge. To address this, we propose a novel approach to cross-task generalization in graphs via task-trees, which serve as unified learning instances aligning node-, edge-, and graph-level tasks. We theoretically analyze the stability, transferability, and generalization properties of task-trees, showing that pretraining a graph neural network (GNN) on diverse task-trees with a reconstruction objective induces transferable knowledge. This enables efficient adaptation to downstream tasks with minimal fine-tuning. To validate our framework, we introduce Graph Generality Identifier on Task-Trees (GIT), a graph foundation model that demonstrates strong performance on over 30 graphs across five domains via fine-tuning, in-context learning, and zero-shot generalization. Code and data are available at https://github.com/Zehong-Wang/GIT.\\nGraph-structured data is everywhere---from social networks to molecular structures---but building general-purpose models for graphs has been difficult due to the wide variety of graph types and tasks. Inspired by the success of foundation models in text and vision, this work introduces a new approach to generalize across different graph tasks using a concept called “task-trees.” A task-tree is a structure that captures the essential parts of a graph relevant to a specific task (e.g., classifying a node or predicting a link), and unifies different types of graph tasks into a common format. The paper further proposes a model called GIT (Graph Generality Identifier on Task-Trees), which is pretrained on task-trees from diverse graphs. GIT demonstrates strong performance in fine-tuning, few-shot learning, and even zero-shot generalization across 30+ datasets in five domains. Theoretical analysis supports the effectiveness of task-trees for learning transferable patterns. Overall, this work provides a scalable and principled foundation for training general-purpose graph models, advancing the field toward graph foundation models similar to GPTs for text or CLIP for vision.\\n\\nSOURCE: https://icml.cc/virtual/2025/poster/46113',\n",
       " 'https://arxiv.org/html/2310.11829v4': 'SNIPPET: Definition A graph foundation model (GFM) is a model that is expected to benefit from the pre-training of broad graph data, and can be adapted to a wide range ...\\n\\nTITLE: Graph Foundation Models:\\nConcepts, Opportunities and Challenges\\n\\nBODY:\\nGraph Foundation Models: Concepts, Opportunities and Challenges\\nAbstract\\nFoundation models have emerged as critical components in a variety of artificial intelligence applications, and showcase significant success in natural language processing and several other domains. Meanwhile, the field of graph machine learning is witnessing a paradigm transition from shallow methods to more sophisticated deep learning approaches. The capabilities of foundation models in generalization and adaptation motivate graph machine learning researchers to discuss the potential of developing a new graph learning paradigm. This paradigm envisions models that are pre-trained on extensive graph data and can be adapted for various graph tasks. Despite this burgeoning interest, there is a noticeable lack of clear definitions and systematic analyses pertaining to this new domain. To this end, this article introduces the concept of Graph Foundation Models (GFMs), and offers an exhaustive explanation of their key characteristics and underlying technologies. We proceed to classify the existing work related to GFMs into three distinct categories, based on their dependence on graph neural networks and large language models. In addition to providing a thorough review of the current state of GFMs, this article also outlooks potential avenues for future research in this rapidly evolving domain.\\nIndex Terms:\\nGraph Foundation Models, Large Language Models1 Introduction\\nWith the rise in computational power and breakthroughs in deep learning techniques, the artificial intelligence (AI) community has introduced the notion of “foundation models”: A foundation model is any model that is trained on broad data and can be adapted to a wide range of downstream tasks [1]. Foundation models enjoy unique attributes like emergence and homogenization, empowering them to serve as the primary building blocks for a myriad of downstream AI applications [1]. Emergence suggests that as a foundation model scales up, it may spontaneously manifest novel capabilities [2]. Meanwhile, homogenization alludes to the model’s versatility, enabling its deployment across diverse applications [1]. Thanks to the development of large language models (LLMs), the concept of foundation models first became a reality in natural language processing (NLP). Since then, foundation models have demonstrated impressive versatility, processing not just text but also image data, video data, audio data and multi-modal inputs. This versatility empowers them to excel in tasks ranging from computer vision [3] and audio signal processing [4] to recommender systems [5].\\nMuch like the evolution witnessed in NLP, graph machine learning is also undergoing a paradigm transition. In its early stages, graph tasks predominantly employed shallow methods, such as random walk [6, 7] and matrix factorization [8, 9, 10, 11, 12]. These methods, however, were typically limited to transductive learning [13]. The more recent shift towards deep learning methods has catalyzed the rise of graph neural networks (GNNs). GNNs have revolutionized the landscape by introducing the message-passing mechanism, where nodes iteratively aggregate information from their neighbors. By harnessing GNNs in fully supervised, semi-supervised, or unsupervised settings, researchers have pioneered a variety of customized graph models. These advancements have yielded substantial improvements in tasks like node classification [14], link prediction [15], graph classification [16], and graph clustering [17]. However, certain challenges of GNN models still persist. For example, GNNs are restricted with issues related to expressive power [18] and generalizability [19], especially given the ever-expanding datasets and the widening spectrum of tasks.\\nThe remarkable success of foundation models in varied domains is increasingly garnering the interest of graph machine learning researchers. This naturally evokes the question: Could graph foundation models represent the next frontier in graph machine learning? Such models, if realized, would boast enhanced expressive power, improved transferability, and applicability to more intricate graph data and tasks. As illustrated in Figure 1, a graph foundation model (GFM) is envisioned as a model pre-trained on extensive graph data, primed for adaptation across diverse downstream graph tasks. Drawing parallels with traditional foundation models, a GFM is also anticipated to embody two principal characteristics: emergence and homogenization. Specifically, emergence refers to novel capabilities shown exclusively in large-scale graph models, while homogenization denotes the model’s adaptability across different types of graph tasks. Existing deep graph learning methods struggle to encompass these features: their inherent architectures and learning paradigms focus on specific tasks, which restrict the utilization of extensive unlabeled data, subsequently limiting their expressive and generalization abilities.\\nInspired by the success of LLMs as foundation models in NLP, researchers have explored the possibilities of graph foundation models towards the emergence and homogenization capabilities. These explorations primarily focus on the design of backbone architectures for GFMs, and different learning paradigms including pre-training and adaptation, as they are the key strategies of LLMs to acheive the aforementioned capabilities. First and foremost, the emergent abilities of foundation models typically exist only in backbones with a large number of parameters, whereas the parameter count of GNNs is significantly smaller than that of the language backbones. This implies that the backbone of GFMs may need to be redesigned to achieve more substantial knowledge storage towards emergence. As graph data is typically associated with rich text information, an alternative approach is to use LLMs as GFMs. Nonetheless, it remains uncertain whether LLMs can effectively handle graph data and associated tasks, and it is crucial to determine how to model graph structures in LLMs. Additionally, the homogenization of foundation models necessitates the handling of diverse tasks in a uniform manner. Devising effective pre-training tasks (also called pretext tasks) and downstream task adaptation methods are challenging for graph data, due to the complexity in interconnected nodes and various forms of attributes, as well as the diversity in tasks across node-, edge- and graph-levels. Therefore, there is also a need to design suitable pre-training tasks and adaptation mechanisms.\\nWhile there is no definitive solution for designing and implementing GFMs, this paper surveys some related researches and categorizes them into three distinct approaches based on their reliance on GNNs and LLMs: (1) GNN-based Models: They aim to enhance existing graph learning paradigms through innovations in the backbone, pre-training, and adaptation aspects; (2) LLM-based Models: They explore the feasibility of using an LLM as a GFM by converting graphs into text or tokens; (3) GNN+LLM-based Models: They explore various forms of synergy between GNNs and LLMs to empower them with enhanced capabilities.\\nTo the best of our knowledge, this is the first survey towards graph foundation models. Existing surveys of foundation models typically explore modalities such as language and vision [1, 20], rather than graphs. Additionally, there are two surveys [21, 22] dedicated to knowledge graphs and large language models, but knowledge graphs, due to their distinct nature in construction and application, fall outside the scope of this article. We have also noticed a very recent article that mentions the concept of large graph models [23], but it emphasizes opinion statements and lacks a systematic taxonomy. Therefore, the contributions of this article can be summarized as follows:\\nThis article defines the concept of graph foundation models for the first time, and examines the core issues and characteristics of their capabilities.\\nThis article introduces a novel taxonomy and discusses the strengths and limitations of each approach towards graph foundation models.\\nThis article provides promising future directions towards graph foundation models.\\nThe subsequent sections are organized as follows. In Section 2, we introduce the background related to GFMs. Section 3 defines GFMs and highlights their similarities and differences with language foundation models. Sections 4 - 6 delve into the relevant works that consider GNN-based models, LLM-based models and GNN+LLM-based models as GFMs, separately. Section 7 engages in a discussion on the future directions of GFMs. In Section 8, we summarize the key points of this paper.\\n2 Background\\nBefore introducing GFMs, we review background knowledge on deep graph learning and language foundation models.\\n2.1 Deep Graph Learning\\nThis section provides a concise overview from three key aspects: graph data, backbone architectures, and learning paradigms.\\n2.1.1 Graph Data\\nGraphs capture intricate relationships among entities and possess several key characteristics that make them challenging for machine learning tasks. The primary challenge stems from their (1) Non-Euclidean Nature: Graph data is inherently non-Euclidean [24], lacking the rigid geometric structure of traditional data formats such as 1D text, 2D images or tabular data. This means that graph data cannot be adequately described within a simple flat space because its intrinsic structure does not conform to the principles of Euclidean geometry [25]. Unlike Euclidean data, which often comes in predefined shapes (e.g., images of a specific resolution), non-Euclidean data can vary greatly in size and shape. This variability complicates the design of algorithms that often navigate complex topologies, significantly increasing computational cost compared to operations on simpler Euclidean spaces.\\nBeyond this fundamental structural complexity, two additional challenges are posed by the nature of graph data. (2) Various Domains: Graph data appears in domains such as social networks [26], biology [27], and transportation [28]. It is also used in tasks like 3D human skeleton recognition [29], semantic segmentation [30], and video classification [31]. Domain-specific variability with different node types and edge semantics makes creating a universal model challenging [32]. (3) Various Types: graph data includes homogeneous, heterogeneous [33], hyper- [34], and dynamic ones [35]. Such diversity also brings challenges to deep graph learning.\\n2.1.2 Backbone Architectures\\nGNNs are the current mainstream backbone architecture for deep graph learning. Most GNNs follow the message-passing framework [36], enabling nodes to exchange information with neighbors. For example, GCN [14] introduces graph convolution layers, GraphSAGE [37] generates node embeddings using inductive learning, and GAT [38] adds an attention mechanism to weigh neighbor importance, enhancing expressive power. These contributions make GNNs versatile tools for deep graph learning.\\nHowever, deepening GNNs is challenging. Increasing layers leads to over-smoothing, where node representations become similar [39], and over-squashing, where information is overly compressed [18]. Efforts like DropEdge [40], which randomly removes edges, improve GCN performance and scalability. Graph transformers [41, 42, 43], with their fully connected attention and long-range relationship modeling, help alleviate over-smoothing and over-squashing [44].\\n2.1.3 Learning Paradigms\\nThe learning paradigms for deep graph learning encompass three primary categories:\\nSupervised learning. In supervised learning, algorithms use a training dataset with input data and output labels. This is used in tasks like graph classification [45] and graph regression [46]. For instance, in molecular property prediction [47], GNNs predict chemical properties using labeled data, aiding drug development and materials research.\\nSemi-supervised learning. Semi-supervised learning, as discussed in [48], is a primary focus in deep graph learning. It uses both labeled and unlabeled data to improve model performance, with node classification [14] being a key application. The message-passing mechanism [36] allows GNNs to exchange information among nodes, incorporating both data types for predictions. GNNs can also combine with methods like label propagation for better performance [49].\\nUnsupervised learning. Unsupervised learning discovers patterns and structures without manual labels. Graph clustering identifies structures based on node relationships, while link prediction estimates missing connections. A subclass, self-supervised learning, generates labels from the data itself [50], allowing GNNs to be trained end-to-end for tasks like graph clustering [17] and link prediction [15].\\n2.2 Language Foundation Models\\nAI is currently undergoing a transformative shift marked by the emergence of some specific language models (such as GPT-3) that are trained on extensive and diverse datasets using self-supervised learning. These models, known as foundation models, are able to produce a wide array of outputs, enabling them to tackle a broad spectrum of downstream tasks. In contrast to the deep graph learning pipeline, the foundation model’s approach embraces a pre-training and adaptation framework, enabling the model to achieve several significant advancements, including the emergence [2] and homogenization [1]. Foundation models have primarily established themselves in the field of NLP [1], so our discussion will focus on language foundation models in this section.\\n2.2.1 Language Data\\nLanguage data refers to text or spoken content in a human language, encompassing the grammar rules of the natural language and the associated semantics of the words. The quality and quantity of language data play a crucial role in the performance of NLP systems, impacting their accuracy, robustness, and overall effectiveness in various language tasks. In contrast to graph data, language data as Euclidean data is easier to model, and its rich semantic information significantly enhances the knowledge transferability of language models.\\n2.2.2 Backbone Architectures\\nAn early breakthrough in foundation models is pre-trained language models (PLMs), designed to capture context-aware word representations, which proved remarkably effective as versatile semantic features. Furthermore, researchers have observed that increasing the scale of PLMs, whether by augmenting model size or training data, frequently results in increased model capacity for downstream tasks. These larger PLMs, collectively referred to as LLMs, exhibit emergent abilities [2] compared to their smaller counterparts \\n...[truncated]',\n",
       " 'https://www.yfang.site/publications': \"SNIPPET: Accepted by ICLR 2025. [Paper] [Code]. Exploring the Potential of Large ... SAMGPT: Text-free Graph Foundation Model for Multi-domain Pre-training and ...\\n\\nTITLE: Yuan FANG @ SMU - Publications\\n\\nBODY:\\n# Student (incl. visiting students) supervised; ^ Research staff supervised; * Corresponding author (journal articles)\\nRefereed Conference Papers\\nUnified Molecule Pre-training with Flexible 2D and 3D Modalities: Single and Paired Modality Integration.\\nTengwei Song#, Min Wu and Yuan Fang. Accepted by CIKM 2025.Quantizing Text-attributed Graphs for Semantic-Structural Integration.\\nJianyuan Bo#, Hao Wu and Yuan Fang. Accepted by KDD 2025.\\n[Paper] [Code]GCoT: Chain-of-Thought Prompt Learning for Graphs.\\nXingtong Yu^, Chang Zhou, Zhongwei Kuai, Xinming Zhang and Yuan Fang. Accepted by KDD 2025.\\n[Paper] [Code]Advancing Molecular Graph-Text Pre-training via Fine-grained Alignment.\\nYibo Li#, Yuan Fang, Mengmei Zhang and Chuan Shi. Accepted by KDD 2025.\\n[Paper] [Code]Graph Positional Autoencoders as Self-supervised Learners.\\nYang Liu#, Deyu Bo, Wenxuan Cao, Yuan Fang, Yawen Li and Chuan Shi. Accepted by KDD 2025.\\n[Paper] [Code]Node-Time Conditional Prompt Learning in Dynamic Graphs.\\nXingtong Yu^, Zhenghao Liu, Xinming Zhang and Yuan Fang. Accepted by ICLR 2025.\\n[Paper] [Code]Exploring the Potential of Large Language Models for Heterophilic Graphs.\\nYuxia Wu^, Shujie Li, Yuan Fang and Chuan Shi. Accepted by NAACL 2025.\\n[Paper] [Code]SAMGPT: Text-free Graph Foundation Model for Multi-domain Pre-training and Cross-domain Adaptation.\\nXingtong Yu^, Zechuan Gong, Chang Zhou, Yuan Fang and Hui Zhang. Accepted by TheWebConf 2025.\\n[Paper] [Code]Unlocking the Potential of Black-box Pre-trained GNNs for Graph Few-shot Learning.\\nQiannan Zhang#, Shichao Pei, Yuan Fang and Xiangliang Zhang. Accepted by AAAI 2025.\\n[Paper] [Poster] [Code]Non-Homophilic Graph Pre-Training and Prompt Learning.\\nXingtong Yu^, Jie Zhang, Yuan Fang and Renhe Jiang. Accepted by KDD 2025.\\n[Paper] [Code]Learning to Identify Seen, Unseen and Unknown in the Open World: A Practical Setting for Zero-Shot Learning.\\nSethupathy Parameswaran^, Yuan Fang, Chandan Gautam, Savitha Ramasamy and Xiaoli Li. Accepted by WACV 2025.\\n[Paper] [Code]A Contrastive Framework with User, Item and Review Alignment for Recommendation.\\nViet Hoang Dong#, Yuan Fang and Hady W. Lauw. Accepted by WSDM 2025.\\n[Paper] [Poster] [Slides] [Code]An Aspect Performance-Aware Hypergraph Neural Network for Review-based Recommendation.\\nJunrui Liu#, Tong Li, Wu Di, Zifang Tang and Yuan Fang, Zhen Yang. Accepted by WSDM 2025.\\n[Paper] [Code]Context-Aware Adapter Tuning for Few-Shot Relation Learning in Knowledge Graphs.\\nRan Liu#, Zhongzhou Liu#, Xiaoli Li and Yuan Fang. In EMNLP 2024, pp. 17525--17537.\\n[Paper] [Poster] [Slides] [Code]A Survey of Ontology Expansion for Conversational Understanding.\\nJinggui Liang, Yuxia Wu^, Yuan Fang, Hao Fei and Lizi Liao. In EMNLP 2024, pp. 18111--18127.\\n[Paper] [Code]Class Name Guided Out-of-Scope Intent Classification.\\nChandan Gautam, Sethupathy Parameswaran^, Aditya Kane, Yuan Fang, Savitha Ramasamy, Suresh Sundaram, Sunil Kumar Sahu and Xiaoli Li. In EMNLP Findings 2024, pp. 9100--9112.\\n[Paper] [Code]Collaborative Cross-modal Fusion with Large Language Model for Recommendation.\\nZhongzhou Liu#, Hao Zhang, Kuicai Dong and Yuan Fang. In CIKM 2024, pp. 1565--1574.\\n[Paper] [Slides] [Data]A Learned Generalized Geodesic Distance Function-Based Approach for Node Feature Augmentation on Graphs.\\nAmitoz Azad^ and Yuan Fang. In KDD 2024, pp. 49--58.\\n[Paper] [Slides] [Code]SIBO: A Simple Booster for Parameter-Efficient Fine-Tuning.\\nZhihao Wen^, Jie Zhang and Yuan Fang. In ACL Findings 2024, pp. 1241--1257.\\n[Paper] [Code]Contrastive General Graph Matching with Adaptive Augmentation Sampling.\\nJianyuan Bo# and Yuan Fang. In IJCAI 2024, pp. 3724--3732.\\n[Paper] [Supplementary] [Slides] [Poster] [Code]Heterogeneous Graph Transformer with Poly-Tokenization.\\nZhiyuan Lu, Yuan Fang, Cheng Yang and Chuan Shi. In IJCAI 2024, pp. 2234--2242.\\n[Paper] [Slides] [Poster] [Code]MultiGPrompt for Multi-Task Pre-Training and Prompting on Graphs.\\nXingtong Yu#, Chang Zhou, Yuan Fang and Xinming Zhang. In TheWebConf 2024, pp. 515--526.\\n[Paper] [Poster] [Code]On the Feasibility of Simple Transformer for Dynamic Graph Modeling.\\nYuxia Wu^, Yuan Fang and Lizi Liao. In TheWebConf 2024, pp. 870--880.\\n[Paper] [Poster] [Code]Diffusion-based Negative Sampling on Graphs for Link Prediction.\\nTrung-Kien Nguyen^, Yuan Fang. In TheWebConf 2024, pp. 948--958.\\n[Paper] [Slides] [Poster] [Code]HGPrompt: Bridging Homogeneous and Heterogeneous Graphs for Few-shot Prompt Learning.\\nXingtong Yu#, Yuan Fang, Zemin Liu and Xinming Zhang. In AAAI 2024, pp. 16578--16586.\\n[Paper] [Code] [Poster]Estimating Propensity for Causality-based Recommendation without Exposure Data.\\nZhongzhou Liu#, Yuan Fang and Min Wu. In NeurIPS 2023.\\n[Paper] [Code] [Poster] [中文概述]Graph Contrastive Learning with Stable and Scalable Spectral Encoding.\\nDeyu Bo#, Yuan Fang, Yang Liu, Chuan Shi. In NeurIPS 2023.\\n[Paper] [Code] [Poster]Voucher Abuse Detection with Prompt-based Fine-tuning on Graph Neural Networks.\\nZhihao Wen#, Yuan Fang, Yihan Liu, Yang Guo and Shuji Hao. In CIKM 2023 (Applied Research), pp. 4864--4870.\\n[Paper] [Code] [Slides] [中文概述]Augmenting Low-Resource Text Classification with Graph-Grounded Pre-training and Prompting.\\nZhihao Wen# and Yuan Fang. In SIGIR 2023, pp. 506--516.\\n[Paper] [Code] [Slides] [中文概述]Link Prediction on Latent Heterogeneous Graphs.\\nTrung-Kien Nguyen^, Zemin Liu^ and Yuan Fang. In TheWebConf 2023, pp. 263--273.\\n[Paper] [Code] [Slides]GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks.\\nZemin Liu^, Xingtong Yu#, Yuan Fang and Xinming Zhang. In TheWebConf 2023, pp. 417--428.\\n[Paper] [Code] [Slides] [中文概述]On Generalized Degree Fairness in Graph Neural Networks.\\nZemin Liu^, Trung-Kien Nguyen^ and Yuan Fang. In AAAI 2023, pp. 4525--4533.\\n[Paper] [Supplementary] [Code] [Slides] [Poster]Learning to Count Isomorphisms with Graph Neural Networks.\\nXingtong Yu#, Zemin Liu^, Yuan Fang and Xinming Zhang. In AAAI 2023, pp. 4845--4853.\\n[Paper] [Supplementary] [Code] [Slides] [Poster] [中文概述]End-to-End Open-Set Semi-Supervised Node Classification with Out-of-Distribution Detection.\\nTiancheng Huang, Donglin Wang, Yuan Fang and Zhengyu Chen. In IJCAI 2022, pp. 2087--2093.\\n[Paper] [Slides] [Poster]TREND: TempoRal Event and Node Dynamics for Graph Representation Learning.\\nZhihao Wen# and Yuan Fang. In TheWebConf 2022, pp. 1159--1169.\\n[Paper] [Code] [Slides] [中文概述]On Size-Oriented Long-Tailed Graph Classification of Graph Neural Networks.\\nZemin Liu^, Qiheng Mao, Chenghao Liu, Yuan Fang and Jianling Sun. In TheWebConf 2022, 1506--1516.\\n[Paper] [Code]Contrastive Pre-training of GNNs on Heterogeneous Graphs.\\nXunqiang Jiang, Yuanfu Lu#, Yuan Fang and Chuan Shi. In CIKM 2021, pp. 803--812.\\n[Paper] [Code] [Slides]Topic-aware Heterogeneous Graph Neural Network for Link Prediction.\\nSiyong Xu, Cheng Yang, Chuan Shi, Yuan Fang, Yuxin Guo, Tianchi Yang, Luhao Zhang and Maodi Hu. In CIKM 2021, pp. 2261--2270.\\n[Paper] [Code] [Slides]Dynamic Heterogeneous Graph Embedding via Heterogeneous Hawkes Process.\\nYugang Ji#, Tianrui Jia, Yuan Fang and Chuan Shi. In ECML-PKDD 2021 Part I, pp. 388--403.\\n[Paper] [Code] [Slides]Tail-GNN: Tail-Node Graph Neural Networks.\\nZemin Liu^, Trung-Kien Nguyen^ and Yuan Fang. In KDD 2021, pp. 1109--1119.\\n[Paper] [Code] [Slides] [Poster] [中文概述]Pre-training on Large-Scale Heterogeneous Graph.\\nXunqiang Jiang, Tianrui Jia, Yuan Fang, Chuan Shi, Zhe Lin, Hui Wang. In KDD 2021, pp. 756--766 .\\n[Paper] [Code] [Slides] [Poster]Node-wise Localization of Graph Neural Networks.\\nZemin Liu^, Yuan Fang, Chenghao Liu^ and Steven C. H. Hoi. In IJCAI 2021, pp. 1520--1526.\\n[Paper] [Supplementary] [Code] [Slides] [Poster]Meta-Inductive Node Classification across Graphs.\\nZhihao Wen#, Yuan Fang and Zemin Liu^. In SIGIR 2021, pp. 1219--1228.\\n[Paper] [Code] [Slides]Learning to Pre-train Graph Neural Networks.\\nYuanfu Lu#, Xunqiang Jiang, Yuan Fang and Chuan Shi. In AAAI 2021, pp. 4276--4284.\\n[Paper] [Supplementary] [Code] [Slides] [Poster] [中文概述]Relative and Absolute Location Embedding for Few-Shot Node Classification on Graph.\\nZemin Liu^, Yuan Fang, Chenghao Liu^ and Steven C. H. Hoi. In AAAI 2021, pp. 4267--4275.\\n[Paper] [Supplementary] [Code] [Slides] [Poster]Towards Locality-Aware Meta-Learning of Tail Node Embeddings on Networks.\\nZemin Liu^, Wentao Zhang#, Yuan Fang, Xinming Zhang and Steven C. H. Hoi. In CIKM 2020, pp. 975--984.\\n[Paper] [Code] [Slides]TPR: Text-aware Preference Ranking for Recommender Systems.\\nYu-Neng Chuang, Chih-Ming Chen, Chuan-Ju Wang, Ming-Feng Tsai, Yuan Fang and Ee-Peng Lim. In CIKM 2020, pp. 215--224.\\n[Paper] [Code] [Slides]Adaptive Task Sampling for Meta-Learning.\\nChenghao Liu^, Zhihao Wang, Doyen Sahoo, Yuan Fang, Kun Zhang and Steven C. H. Hoi. In ECCV 2020 Part XVIII, pp 752--769.\\n[Paper] [Supplementary] [Code] [Slides]Temporal Heterogeneous Interaction Graph Embedding For Next-Item Recommendation.\\nYugang Ji#, Mingyang Yin, Yuan Fang, Hongxia Yang, Xiangwei Wang, Tianrui Jia and Chuan Shi. In ECML-PKDD 2020 Part III, pp. 314--329.\\n[Paper] [Code] [Slides]Social Influence Attentive Neural Network for Friend-Enhanced Recommendation.\\nYuanfu Lu#, Ruobing Xie, Chuan Shi, Yuan Fang, Wei Wang, Xu Zhang and Leyu Lin. In ECML-PKDD 2020 Part IV (Applied Data Science), pp. 3--18.\\n[Paper] [Code] [Slides] [中文概述]Meta-learning on Heterogeneous Information Networks for Cold-start Recommendation.\\nYuanfu Lu#, Yuan Fang and Chuan Shi. In KDD 2020, pp. 1563--1573.\\n[Paper] [Code] [Slides] [Video] [Poster] [中文概述]BiANE: Bipartite Attributed Network Embedding.\\nWentao Huang, Yuchen Li, Yuan Fang, Ju Fan and Hongxia Yang. In SIGIR 2020, pp. 149--158.\\n[Paper] [Code] [Slides] [中文概述]Multiplex Memory Network for Collaborative Filtering.\\nXunqiang Jiang, Binin Hu#, Yuan Fang and Chuan Shi. In SDM 2020, pp. 91--99.\\n[Paper] [Supplementary] [Code] [中文概述]Correlation-Sensitive Next-Basket Recommendation.\\nDuc-Trong Le, Hady W. Lauw and Yuan Fang. In IJCAI 2019, pp. 2808--2814.\\n[Paper] [Code] [Slides] [Poster]Adversarial Learning on Heterogeneous Information Networks.\\nBinbin Hu#, Yuan Fang and Chuan Shi. In KDD 2019, pp. 120--129.\\n[Paper] [Code] [Poster] [中文概述]Heterogeneous Embedding Propagation for Large-scale E-Commerce User Alignment.\\nVincent W. Zheng, Mo Sha, Yuchen Li, Hongxia Yang, Yuan Fang, Zhenjie Zhang, Kian-Lee Tan and Kevin C.-C. Chang. In ICDM 2018 (Short), pp. 1434--1439.\\n[Paper] [Slides] [Poster]Modeling Contemporaneous Basket Sequences with Twin Networks for Next-Item Recommendation.\\nDuc-Trong Le, Hady W. Lauw and Yuan Fang. In IJCAI 2018, pp. 3414--3420.\\n[Paper] [Code] [Slides] [Poster]Region Average Pooling for Context-Aware Object Detection.\\nKingsley Kuan, Gaurav Manek, Jie Lin, Yuan Fang and Vijay Chandrasekhar. In ICIP 2017, pp. 1347--1351.\\n[Paper]Object Detection Meets Knowledge Graphs.\\nYuan Fang, Kingsley Kuan, Jie Lin, Cheston Tan and Vijay Chandrasekhar. In IJCAI 2017, pp. 1661--1667.\\n[Paper] [Code] [Slides] [Poster]Basket-Sensitive Personalized Item Recommendation.\\nDuc-Trong Le, Hady W. Lauw and Yuan Fang. In IJCAI 2017, pp. 2060--2066.\\n[Paper] [Slides] [Poster]Modeling Sequential Preferences with Dynamic User and Context Factors.\\nDuc-Trong Le, Yuan Fang and Hady W. Lauw. In ECML-PKDD 2016, pp. 145--161.\\n[Paper] [Supplementary] [Slides] [Poster]Learning to Query: Focused Web Page Harvesting for Entity Aspects.\\nYuan Fang, Vincent W. Zheng and Kevin C.-C. Chang. In ICDE 2016, pp. 1002--1013.\\n[Paper] [Slides] [Poster]Semantic Proximity Search on Graphs with Metagraph-based Learning.\\nYuan Fang, Wenqing Lin, Vincent W. Zheng, Min Wu, Kevin C.-C. Chang and Xiao-Li Li. In ICDE 2016, pp. 277--288.\\n[Paper] [Code] [Slides] [Poster]Graph-based Semi-supervised Learning: Realizing Pointwise Smoothness Probabilistically.\\nYuan Fang, Kevin C.-C. Chang and Hady W. Lauw. In ICML 2014 (2), pp. 406--414.\\n[Paper] [Supplementary] [Data] [Slides]Incremental and Accuracy-Aware Personalized PageRank through Scheduled Approximation.\\nFanwei Zhu, Yuan Fang, Kevin C.-C. Chang and Jing Ying. In VLDB 2013, 6(6), pp. 481--492.\\nExtended version invited to the collection of Best Papers of VLDB'13.\\n[Paper] [Data] [Slides]RoundTripRank: Graph-based Proximity with Importance and Specificity.\\nYuan Fang, Kevin C.-C. Chang and Hady W. Lauw. In ICDE 2013, pp. 613--624.\\n[Paper] [Data] [Slides] [Poster]Conﬁdence-Aware Graph Regularization with Heterogeneous Pairwise Features.\\nYuan Fang, Bo-June P. Hsu and Kevin C.-C. Chang. In SIGIR 2012, pp. 951--960.\\n[Paper] [Slides]Searching Patterns for Relation Extraction over the Web: Rediscovering the Pattern-Relation Duality.\\nYuan Fang and Kevin C.-C. Chang. In WSDM 2011, pp. 825--834.\\n[Paper] [Poster] [Notes]Privacy beyond Single Sensitive Attribute.\\nYuan Fang, Mafruz Zaman Ashrafi and See-Kiong Ng. In DEXA 2011, pp. 187--201.\\n[PDF]Efficient Skyline Maintenance for Streaming Data with Partially-Ordered Domains.\\nYuan Fang and Chee-Yong Chan. In DASFAA 2010, pp. 322--336.\\n[Paper] [Slides]\\nRefereed Journal Papers\\nTemporal and Heterogeneous Graph Neural Network for Remaining Useful Life Prediction.\\nZhihao Wen^, Yuan Fang*, Pengcheng Wei, Fayao Liu, Zhenghua Chen and Min Wu.\\n[PDF]Graph Foundation Models: Concepts, Opportunities and Challenges.\\nJiawei Liu, Cheng Yang, Zhiyuan Lu, Junze Chen, Yibo Li, Mengmei Zhang, Ting Bai, Yuan Fang, Lichao Sun, Philip S. Yu and Chuan Shi. To appear in IEEE TPAMI.\\n[PDF] [Resource]GNNSynergy: A Multi-view Graph Neural Network for Predicting Anti-cancer Drug Synergy.\\nZhifeng Hao, Jianming Zhan, Yuan Fang, Min Wu and Ruichu Cai. To appear in IEEE/ACM TCBB.\\n[Paper] [Code]On the Probability of Necessity and Sufficiency of Explaining Graph Neural Networks: A Lower Bound Optimization Approach.\\nRuichu Cai, Yuxuan Zhu, Xuexin Chen, Yuan Fang, Min Wu, Jie Qiao and Zhifeng Hao. To appear in Neural Networks.\\n[PDF]Temporal Relational Graph Convolutional Network Approach to Financial Performance Prediction.\\nBrindha Priyadarshini Jeyaraman#, Bing Tian Dai and Yuan Fang. In MAKE 6(4), pp. 2303--2320.\\n[PDF]An end-to-end bi-objective approach to deep graph partitioning.\\nPengcheng Wei^, Yuan Fang, Zhihao Wen, Zheng Xiao, Binbin Chen. In Neural Networks 181, 2024, Article No. 106823.\\n[PDF]Prompt Tuning on Graph-augmented Low-resource Text Classification.\\nZhihao Wen# and Yuan Fang. In IEEE TKDE 36(12), 2024, pp. 9080--9095.\\n[PDF] [Code]Generalized Graph Prompt: Toward a Unification of Pre-Training and Downstream Tasks on Graphs.\\nXingtong Yu#, Zhenghao Liu, Yuan Fang*, Zemin Liu, Sihong Chen and Xinming Zhang*. In IEEE TKDE 36(11), 2024, pp. 6237--6250.\\n[PDF] [Code]Dynamic Meta-path Guided Temporal Heterogeneous Graph Neural Networks.\\nYugang Ji, Chuan Shi* and Yuan Fang. In World Scientific Annual Review of Artificial Intelligence 1, 2024, Article No. 2350002.\\n[Paper]Locality-Aware Tail Node E\\n...[truncated]\",\n",
       " 'http://www.shichuan.org/doc/196.pdf': 'SNIPPET: by J Liu · Cited by 17 — Definition A graph foundation model (GFM) is a model that is expected to benefit from the pre-training of broad graph data, and can be adapted to a wide range ...\\n\\nTITLE: (from PDF)\\n\\nBODY:\\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\\n\\n1\\n\\nGraph Foundation Models:\\nConcepts, Opportunities and Challenges\\n\\nJiawei Liu*, Cheng Yang*, Zhiyuan Lu, Junze Chen, Yibo Li,\\nMengmei Zhang, Ting Bai, Yuan Fang, Lichao Sun, Philip S. Yu, and Chuan Shi\\n\\nAbstract—Foundation models have emerged as critical components in a variety of artificial intelligence applications, and showcase\\nsignificant success in natural language processing and several other domains. Meanwhile, the field of graph machine learning is\\nwitnessing a paradigm transition from shallow methods to more sophisticated deep learning approaches. The capabilities of foundation\\nmodels in generalization and adaptation motivate graph machine learning researchers to discuss the potential of developing a new\\ngraph learning paradigm. This paradigm envisions models that are pre-trained on extensive graph data and can be adapted for various\\ngraph tasks. Despite this burgeoning interest, there is a noticeable lack of clear definitions and systematic analyses pertaining to this\\nnew domain. To this end, this article introduces the concept of Graph Foundation Models (GFMs), and offers an exhaustive explanation\\nof their key characteristics and underlying technologies. We proceed to classify the existing work related to GFMs into three distinct\\ncategories, based on their dependence on graph neural networks and large language models. In addition to providing a thorough\\nreview of the current state of GFMs, this article also outlooks potential avenues for future research in this rapidly evolving domain.\\n\\nIndex Terms—Graph Foundation Models, Large Language Models\\n✦\\n\\n1 INTRODUCTION\\n\\nW ITH the rise in computational power and breakthroughs\\n\\nin deep learning techniques, the artificial intelligence (AI)\\ncommunity has introduced the notion of “foundation models”:\\nA foundation model is any model that is trained on broad data\\nand can be adapted to a wide range of downstream tasks [1].\\nFoundation models enjoy unique attributes like emergence and ho-\\nmogenization, empowering them to serve as the primary building\\nblocks for a myriad of downstream AI applications [1]. Emergence\\nsuggests that as a foundation model scales up, it may sponta-\\nneously manifest novel capabilities [2]. Meanwhile, homogeniza-\\ntion alludes to the model’s versatility, enabling its deployment\\nacross diverse applications [1]. Thanks to the development of\\nlarge language models (LLMs), the concept of foundation models\\nfirst became a reality in natural language processing (NLP). Since\\nthen, foundation models have demonstrated impressive versatility,\\nprocessing not just text but also image data, video data, audio\\ndata and multi-modal inputs. This versatility empowers them to\\nexcel in tasks ranging from computer vision [3] and audio signal\\nprocessing [4] to recommender systems [5].\\n\\nMuch like the evolution witnessed in NLP, graph machine\\nlearning is also undergoing a paradigm transition. In its early\\n\\n•\\n\\nJiawei Liu, Cheng Yang, Zhiyuan Lu, Junze Chen, Yibo Li, Ting Bai\\nand Chuan Shi are with School of Computer Science, Beijing University\\nof Posts and Telecommunications, Beijing, China. E-mail: {liu jiawei,\\nyangcheng, luzy, junze, yiboL, baiting, shichuan}@bupt.edu.cn\\n\\n• Mengmei Zhang is with China Telecom Bestpay, Beijing, China. E-mail:\\n\\n•\\n\\n•\\n\\nzhangmengmei@bestpay.com.cn\\nYuan Fang is with School of Computing and Information Systems, Singa-\\npore Management University, Singapore. E-mail: yfang@smu.edu.sg\\nLichao Sun is with Lehigh University, Bethlehem, Pennsylvania, USA. E-\\nmail: lis221@lehigh.edu\\n\\n• Philip S. Yu is with University of Illinois Chicago, Chicago, USA. E-mail:\\n\\npsyu@uic.edu\\nJiawei Liu and Cheng Yang contributed equally to this research.\\n\\n•\\n• Corresponding author: Chuan Shi\\n\\nstages, graph tasks predominantly employed shallow methods,\\nsuch as random walk [6, 7] and matrix factorization [8, 9,\\n10, 11, 12]. These methods, however, were typically limited to\\ntransductive learning [13]. The more recent shift towards deep\\nlearning methods has catalyzed the rise of graph neural networks\\n(GNNs). GNNs have revolutionized the landscape by introducing\\nthe message-passing mechanism, where nodes iteratively aggre-\\ngate information from their neighbors. By harnessing GNNs in\\nfully supervised, semi-supervised, or unsupervised settings, re-\\nsearchers have pioneered a variety of customized graph models.\\nThese advancements have yielded substantial improvements in\\ntasks like node classification [14], link prediction [15], graph\\nclassification [16], and graph clustering [17]. However, certain\\nchallenges of GNN models still persist. For example, GNNs\\nare restricted with issues related to expressive power [18] and\\ngeneralizability [19], especially given the ever-expanding datasets\\nand the widening spectrum of tasks.\\n\\nThe remarkable success of foundation models in varied do-\\nmains is increasingly garnering the interest of graph machine\\nlearning researchers. This naturally evokes the question: Could\\ngraph foundation models represent the next frontier in graph\\nmachine learning? Such models, if realized, would boast enhanced\\nexpressive power, improved transferability, and applicability to\\nmore intricate graph data and tasks. As illustrated in Figure 1,\\na graph foundation model (GFM) is envisioned as a model pre-\\ntrained on extensive graph data, primed for adaptation across\\ndiverse downstream graph tasks. Drawing parallels with traditional\\nfoundation models, a GFM is also anticipated to embody two prin-\\ncipal characteristics: emergence and homogenization. Specifically,\\nemergence refers to novel capabilities shown exclusively in large-\\nscale graph models, while homogenization denotes the model’s\\nadaptability across different types of graph tasks. Existing deep\\ngraph learning methods struggle to encompass these features: their\\ninherent architectures and learning paradigms focus on specific\\ntasks, which restrict the utilization of extensive unlabeled data,\\n\\n\\x0cIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\\n\\n2\\n\\nto enhance existing graph learning paradigms through innovations\\nin the backbone, pre-training, and adaptation aspects; (2) LLM-\\nbased Models: They explore the feasibility of using an LLM as a\\nGFM by converting graphs into text or tokens; (3) GNN+LLM-\\nbased Models: They explore various forms of synergy between\\nGNNs and LLMs to empower them with enhanced capabilities.\\n\\nTo the best of our knowledge, this is the first survey towards\\ngraph foundation models. Existing surveys of foundation models\\ntypically explore modalities such as language and vision [1, 20],\\nrather than graphs. Additionally, there are two surveys [21, 22]\\ndedicated to knowledge graphs and large language models, but\\nknowledge graphs, due to their distinct nature in construction and\\napplication, fall outside the scope of this article. We have also\\nnoticed a very recent article that mentions the concept of large\\ngraph models [23], but it emphasizes opinion statements and lacks\\na systematic taxonomy. Therefore, the contributions of this article\\ncan be summarized as follows:\\n\\n• This article defines the concept of graph foundation models\\nfor the first time, and examines the core issues and characteristics\\nof their capabilities.\\n\\n• This article introduces a novel taxonomy and discusses\\nthe strengths and limitations of each approach towards graph\\nfoundation models.\\n\\n• This article provides promising future directions towards\\n\\ngraph foundation models.\\n\\nThe subsequent sections are organized as follows. In Section 2,\\nwe introduce the background related to GFMs. Section 3 defines\\nGFMs and highlights their similarities and differences with lan-\\nguage foundation models. Sections 4 - 6 delve into the relevant\\nworks that consider GNN-based models, LLM-based models and\\nGNN+LLM-based models as GFMs, separately. Section 7 engages\\nin a discussion on the future directions of GFMs. In Section 8, we\\nsummarize the key points of this paper.\\n\\n2 BACKGROUND\\nBefore introducing GFMs, we review background knowledge on\\ndeep graph learning and language foundation models.\\n\\n2.1 Deep Graph Learning\\n\\nThis section provides a concise overview from three key aspects:\\ngraph data, backbone architectures, and learning paradigms.\\n\\n2.1.1 Graph Data\\nGraphs capture intricate relationships among entities and possess\\nseveral key characteristics that make them challenging for machine\\nlearning tasks. The primary challenge stems from their (1) Non-\\nEuclidean Nature: Graph data is inherently non-Euclidean [24],\\nlacking the rigid geometric structure of traditional data formats\\nsuch as 1D text, 2D images or tabular data. This means that graph\\ndata cannot be adequately described within a simple flat space\\nbecause its intrinsic structure does not conform to the principles\\nof Euclidean geometry [25]. Unlike Euclidean data, which often\\ncomes in predefined shapes (e.g., images of a specific resolution),\\nnon-Euclidean data can vary greatly in size and shape. This\\nvariability complicates the design of algorithms that often navigate\\ncomplex topologies, significantly increasing computational cost\\ncompared to operations on simpler Euclidean spaces.\\n\\nBeyond this fundamental structural complexity, two additional\\nchallenges are posed by the nature of graph data. (2) Vari-\\nous Domains: Graph data appears in domains such as social\\n\\nFig. 1: The distinction between deep graph learning and graph foun-\\ndation models. Deep graph learning tackles specific tasks on specific\\ndatasets through end-to-end training. In contrast, graph foundation\\nmodels (GFMs) are pre-trained on broad graph data and can be\\nadapted to a wide range of downstream graph tasks, expected to\\ndemonstrate emergence and homogenization capabilities.\\n\\nsubsequently limiting their expressive and generalization abilities.\\nInspired by the success of LLMs as foundation models in NLP,\\nresearchers have explored the possibilities of graph foundation\\nmodels towards the emergence and homogenization capabilities.\\nThese explorations primarily focus on the design of backbone\\narchitectures for GFMs, and different learning paradigms includ-\\ning pre-training and adaptation, as they are the key strategies\\nof LLMs to acheive the aforementioned capabilities. First and\\nforemost, the emergent abilities of foundation models typically\\nexist only in backbones with a large number of parameters,\\nwhereas the parameter count of GNNs is significantly smaller than\\nthat of the language backbones. This implies that the backbone of\\nGFMs may need to be redesigned to achieve more substantial\\nknowledge storage towards emergence. As graph data is typically\\nassociated with rich text information, an alternative approach is to\\nuse LLMs as GFMs. Nonetheless, it remains uncertain whether\\nLLMs can effectively handle graph data and associated tasks,\\nand it is crucial to determine how to model graph structures in\\nLLMs. Additionally, the homogenization of foundation models\\nnecessitates the handling of diverse tasks in a uniform manner.\\nDevising effective pre-training tasks (also called pretext tasks) and\\ndownstream task adaptation methods are challenging for graph\\ndata, due to the complexity in interconnected nodes and various\\nforms of attributes, as well as the diversity in tasks across node-,\\nedge- and graph-levels. Therefore, there is also a need to design\\nsuitable pre-training tasks and adaptation mechanisms.\\n\\nWhile there is no definitive solution for designing and imple-\\nmenting GFMs, this paper surveys some related researches and\\ncategorizes them into three distinct approaches based on their\\nreliance on GNNs and LLMs: (1) GNN-based Models: They aim\\n\\nGraph Foundation ModelDeep Graph LearningDownstream TasksPre-trainingAdaptationEmergencePerformance#ParametersPerformance#Parameters...?...??Pretext Task (e.g., link prediction)End-to-End•In-context Learning•Graph Reasoning•Zero-shot Generation•...(e.g., node classification)(Node-,Edge-, Graph-level Tasks)HomogenizationDownstream Task????\\x0cIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\\n\\n3\\n\\nnetworks [26], biology [27], and transportation [28]. It is also\\nused in tasks like 3D human skeleton recognition [29], semantic\\nsegmentation [30], and video classification [31]. Domain-specific\\nvariability with different node types and edge semantics makes\\ncreating a universal model challenging [32]. (3) Various Types:\\ngraph data includes homogeneous, heterogeneous [33], hyper-\\n[34], and dynamic ones [35]. Such diversity also brings challenges\\nto deep graph learning.\\n\\n2.1.2 Backbone Architectures\\n\\nGNNs are the current mainstream backbone architecture for\\ndeep graph learning. Most GNNs follow the message-passing\\nframework [36], enabling nodes to exchange information with\\nneighbors. For example, GCN [14] introduces graph convolution\\nlayers, GraphSAGE [37] generates node embeddings using in-\\nductive learning, and GAT [38] adds an attention mechanism to\\nweigh neighbor importance, enhancing expressive power. These\\ncontributions make GNNs versatile tools for deep graph learning.\\nHowever, deepening GNNs is challenging. Increasing layers\\nleads to over-smoothing, where node representations become\\nsimilar [39], and over-squashing, where information is overly\\ncompressed [18]. Efforts like DropEdge [40], which randomly\\nremoves edges, improve GCN performance and scalability. Graph\\ntransformers [41, 42, 43], with their fully connected attention and\\nlong-range relationship modeling, help alleviate over-smoothing\\nand over-squashing [44].\\n\\n2.1.3 Learning Paradigms\\n\\nThe learning paradigms for deep graph learning encompass three\\nprimary categories:\\n\\nSupervised learning. In supervised learning, algorithms use\\na training dataset with input data and output labels. This is used\\nin tasks like graph classification [45] and graph regression [46].\\nFor instance, in molecular property prediction [47], GNNs predict\\nchemical properties using labeled data, aiding drug development\\nand materials research.\\n\\nSemi-supervised learning. Semi-supervised learning, as dis-\\ncussed in [48], is a primary focus in deep graph learning. It\\nuses both labeled and unlabeled data to improve model perfor-\\nmance, with node classification [14] being a key application. The\\nmessage-passing mechanism [36] allows GNNs to exchange in-\\nformation among nodes, incorporating both data types for predic-\\ntions. GNNs can also combine with methods like label propagation\\nfor better performance [49].\\n\\nUnsupervised learning. Unsupervised learning discovers pat-\\nterns and structures without manual labels. Graph clustering iden-\\ntifies structures based on node relationships, while link prediction\\nestimates missing connections. A subclass, self-supervised learn-\\ning, generates labels from the data itself [50], allowing GNNs to\\nbe trained end-to-end for tasks like graph clustering [17] and li\\n...[truncated]',\n",
       " 'https://www.arxiv.org/pdf/2407.09709': 'SNIPPET: by L Kong · 2024 · Cited by 23 — In this paper, we first identify three desirable properties of a graph foundation model (GFM), namely large-scale self-supervised pre-training, ...\\n\\nTITLE: (from PDF)\\n\\nBODY:\\n5\\n2\\n0\\n2\\n\\nr\\np\\nA\\n4\\n2\\n\\n]\\n\\nG\\nL\\n.\\ns\\nc\\n[\\n\\n2\\nv\\n9\\n0\\n7\\n9\\n0\\n.\\n7\\n0\\n4\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\nPublished as a conference paper at ICLR 2025\\n\\nGOFA: A GENERATIVE ONE-FOR-ALL MODEL FOR\\nJOINT GRAPH LANGUAGE MODELING\\n\\nLecheng Kong1∗ Jiarui Feng1∗ Hao Liu1∗ Chengsong Huang1 Jiaxin Huang1\\nYixin Chen1 Muhan Zhang2†\\n1Washington University in St. Louis 2Peking University\\n{jerry.kong, feng.jiarui, liuhao, chengsong, jiaxinh}@wustl.edu\\nychen25@wustl.edu, muhan@pku.edu.cn\\n\\nABSTRACT\\n\\nFoundation models, such as Large Language Models (LLMs) or Large Vision\\nModels (LVMs), have emerged as one of the most powerful tools in the respective\\nfields. However, unlike text and image data, graph data do not have a definitive\\nstructure, posing great challenges to developing a Graph Foundation Model (GFM).\\nFor example, current attempts at designing general graph models either transform\\ngraph data into a language format for LLM-based prediction or still train a GNN\\nmodel with LLM as an assistant. The former can handle unlimited tasks, while\\nthe latter captures graph structure much better—yet, no existing work can achieve\\nboth simultaneously. In this paper, we first identify three key desirable properties\\nof a GFM: self-supervised pretraining, fluidity in tasks, and graph awareness. To\\naccount for these properties, we extend the conventional language modeling to the\\ngraph domain and propose a novel generative graph language model GOFA. The\\nmodel interleaves randomly initialized GNN layers into a frozen pre-trained LLM\\nso that the semantic and structural modeling abilities are organically combined.\\nGOFA is pre-trained on newly proposed graph-level next-word prediction, question-\\nanswering, structural understanding, and information retrieval tasks to obtain the\\nabove GFM properties. The pre-trained model is further instruction fine-tuned to ob-\\ntain the task-solving ability. Our GOFA model is evaluated on various downstream\\ndatasets unseen during the pre-training and fine-tuning phases, demonstrating a\\nstrong ability to solve structural and contextual problems in zero-shot scenarios.\\nThe code is available at https://github.com/JiaruiFeng/GOFA.\\n\\n1\\n\\nINTRODUCTION\\n\\nWith the emergence of Large Language Models (LLMs), the field of artificial intelligence is undergo-\\ning a profound transformation, shifting from specialized, fragmented models to universal foundation\\nmodels. A foundation model is pre-trained on large-scale datasets and can be further adapted to\\ndiverse downstream tasks using fine-tuning (Hu et al., 2022) or in-context learning (Bommasani et al.,\\n2021; Touvron et al., 2023). Foundation models have been developed in different domains to handle\\ntext (Brown et al., 2020; Touvron et al., 2023), image (Kirillov et al., 2023; Bai et al., 2023), and\\neven multi-modal data (Zhang et al., 2023c; Li et al., 2023; Alayrac et al., 2022). Because of their\\nversatility and generalizability, foundation models have become prevalent in these domains.\\n\\nHowever, despite preliminary efforts, a foundation model in the graph domain has arguably yet to be\\nproposed. In the graph domain, data are highly flexible and dynamic. For example, social networks\\nreceive millions of new connections daily (Hardiman & Katzir, 2013), and novel molecules and\\nprotein structures are frequently discovered (Abramson et al., 2024; Gilmer et al., 2017). While\\npast researchers have proposed specialized models to learn graph data (Ying et al., 2021; Kipf &\\nWelling, 2017), the models require retraining to accommodate new graphs (Dai et al., 2022; Mo et al.,\\n2022). Moreover, trained models are usually tied to specific applications and cannot be generalized\\nto new domains and tasks. It becomes increasingly difficult for models to adjust to the ever-evolving\\n\\n∗Contributed equally. Listing order is random.\\n†Corresponding author\\n\\n1\\n\\n \\n \\n \\n \\n \\n \\n\\x0cPublished as a conference paper at ICLR 2025\\n\\nFigure 1: Examples of our pre-training tasks.\\n\\nnature of graph data. Hence, a graph foundation model (GFM) applicable to new domains/tasks\\nwith minimal or no adaptation costs is urgently needed, spurring recent endeavors to study general\\ngraph models. In particular, a strong zero-shot ability is both challenging and fascinating for GFM\\nresearchers.\\n\\nThe success of LLMs inspired a series of preliminary attempts which use LLMs to develop general\\ngraph models. They can be roughly divided into two categories: LLM as a predictor and LLM\\nas an enhancer (Chen et al., 2023). The LLM as a predictor approach transforms graph data\\ninto representations that LLMs can understand and use LLMs to generate predictions (Tang et al.,\\n2023). However, as suggested by a recent study (Wang et al., 2023), such an approach falls short of\\nunderstanding graph structures. This inspired the LLM as an enhancer approach, which adopts\\nLLM to process and unify diverse graph data and feeds them to a GNN to train general graph\\nmodels (Liu et al., 2023a; Huang et al., 2023a). Nevertheless, because GNN outputs fixed-sized\\nrepresentations/predictions, they can only handle specific tasks such as classification, and cannot\\ngeneralize to arbitrary, new tasks due to the lack of generation ability. In summary, the current two\\napproaches cannot fully utilize structural information and be generative simultaneously. We discuss\\nthe pros and cons of existing approaches in detail in Section 2.\\n\\nIn this paper, we first identify three desirable properties of a graph foundation model (GFM), namely\\nlarge-scale self-supervised pre-training, fluidity in tasks, and graph understanding. To achieve the\\nfirst property, we propose a generic graph self-supervised learning problem similar to the next-\\ntoken prediction problem in LLMs, allowing label-agnostic and continual training on highly diverse\\ngraph data. We then propose a generative model termed Generative One-For-All (GOFA) that\\ninterleaves GNN layers into an LLM to achieve the second and third properties. Such a novel design\\nsystematically integrates GNN into an LLM, granting the LLM graph structural learning ability\\nwhile keeping LLM’s original free-form text generation ability. Meanwhile, this design allows the\\npipeline of the original LLM to remain intact, giving GOFA a close-to-LLM level of task fluidity. We\\npre-train the model with large-scale real-world graph data, Question-Answer (QA) chain data adopted\\nfrom the NLP domain, and graph structural data to empower the model with the aforementioned\\nfoundational abilities in the graph domain (Examples in Figure 1). After pre-training, we further\\ninstruction fine-tune the model on a small amount of data (relative to the pre-training data) to make it\\nunderstand task formats. The fine-tuned model is finally evaluated on various downstream datasets\\nunseen during pre-training and fine-tuning. GOFA achieved impressive results on the zero-shot\\nscenario, which demonstrates the strong potential of GOFA to serve as a graph foundation model.\\n\\n2 A DESIRED FOUNDATION MODEL FOR GRAPH\\n\\nIn this section, we elaborate on three crucial properties a true graph foundation model should possess\\nto motivate our GOFA model design. We note that many contemporary works (partly) propose similar\\nideas to ours and thus we do not claim the credit. We kindly refer readers to the latest surveys (Liu\\net al., 2023b; Jin et al., 2023; Zhang et al., 2023d) for more discussions on GFMs.\\n\\n2\\n\\n📦📦📦📦💬💬💬💬🌐🌐🌐🌐📄📄📄Sentence Completion TaskQuestion Answering TaskStructural Understanding TaskInformation Retrieval TaskWhich type of Rock is commonly used for construction and why?Sedimentary rock. It is easy to extract, cut, and shape.Are there any other types of rocks used for construction?Yes. Igneous rocks like granite are used for their durability.QAQAPABCBACDPABCThis is [Node D]. Wikipedia entry: quickdraw. A graphics software …DThis is [Node C]. Wikipedia entry: system_7. Seventh major release of …AThis is [Node A].  Product: Wireless Controller for Switch or OLED… DThis is [Node D]. Product: Amazon Fire TV, 4-series 4K UHD smart TV…QAQAThis is [Node B]. Title: Attention is all you need. Abstract: The dominant sequence transduction models …Abstract: We present graph attention networks (GATs), novel neural network architectures that operate on graph …PDo certain regions or cultures have preference of rocks?Yes, limestone is commonly used in UK because it can withstand high levels of rainfall and humidity.PCompute the shortest path between [Node A] and [Node D] and generate all shortest paths from [Node A] to [Node D].The shortest path distance is 2. Shortest path: [Node A] -> [Node B] -> [Node D] .PATAG Raw TextPromptAnswerTAGTaskBThis is [Node A]. Title: Graph Attention Networks.CThis is [Node C]. Title: Adam: A method for stochastic optimization. Abstract: We introduce Adam, an algorithm for …Please output the content of [Node D] .Wikipedia entry: system_7. Seventh major release of theclassic Mac OSoperating systemforMacintosh …This is [Node A]. Wikipedia entry: unix. Unix is a family of multitasking…This is [Node B]. Product: Nintendo Switch with Blue and Red Joy-Con…APDCB📄DNo prompt for sentence completion task.\\x0cPublished as a conference paper at ICLR 2025\\n\\nLarge-Scale Self-Supervised Pre-training: One fundamental design of LLM is that it unifies all\\nNLP tasks into a single next-token-prediction paradigm, which enables self-supervised pre-training\\non a large corpus collected from different sources. For pre-training graph models, while numerous\\nefforts have been made from both the LLM as a predictor and LLM as an enhancer approaches,\\nthese attempts usually require the learning target to be labeled (Liu et al., 2023a; Chen et al., 2023).\\nHowever, a graph foundation model should have no constraint on the input graph (has labels or not)\\nand can learn cross-domain knowledge from large-scale graph data in a self-supervised fashion.\\n\\nFluidity in Tasks: A graph foundation model should also possess the same level of versatility and\\nfluidity in handling different tasks as an LLM. Specifically, such ability can be broken down into\\nthree levels: (a) The graph foundation model can naturally respond appropriately to different graph\\ntasks based on user instructions without requiring task-specific adjustment (e.g., the same model\\nperforms classification and question-answering tasks without any modification.) (b) With appropriate\\ninstruction-tuning, the model should have in-context learning ability on unseen tasks (e.g., a model\\ntuned on citation network also performs well on knowledge graphs with proper instructions). (c) Users\\nshould be able to define new, previously unseen tasks by modifying the graph structure and features\\nin a way that aligns with the universal input representation of the model. They can continuously train\\nthe model on new data without special adaptation. Existing approaches that use GNN models as the\\npredictors are usually either restricted in the output format (Liu et al., 2023a; Xia et al., 2024; He\\net al., 2024a) or need additional fine-tuning on the task head (Sun et al., 2023; Wang et al., 2022).\\nConsequently, despite having better structural modeling ability, such models cannot accommodate\\ntask changes or deal with novel tasks, e.g., shifting from a classification task to a question-answering\\ntask that requires outputting all shortest paths between two nodes.\\n\\nGraph Understanding: Since the LLM as a predictor approach uses a generative LLM to take\\ntext input and produce text output, it naturally has the fluidity to accept varied prompts to tackle\\ndifferent tasks. However, such an approach processes the structural information poorly (Wang et al.,\\n2023), making the utility of these models limited on many graph tasks. More importantly, even\\nthough some recent variants can use auxiliary graph models (such as GNNs) to incorporate structural\\ninformation (Tang et al., 2023; He & Hooi, 2024; Zhang et al., 2024), the graph models are frozen\\nand not responsive to different prompts, and the output from the graph models may not be the most\\nrelevant to the input prompt. On the contrary, a graph foundation model should account for the\\nunique structural information of graphs such as node degrees, shortest paths, common neighbors,\\netc., and generate graph representations dependent on the input prompt. It should not only have\\nLLM’s prompt learning capability but also learn graph structure and semantic information jointly.\\n\\n3 METHOD\\n\\nIn this section, we first propose a generative modeling framework for graphs, serving as the graph\\ncounterpart of traditional language modeling. Next, we introduce a novel GNN-LLM architecture for\\nthe proposed graph generative modeling problem. Finally, we describe the unified pre-training tasks\\nto train GOFA towards the proposed GFM properties.\\n\\n3.1 GENERATIVE MODELING FOR GRAPH\\n\\nUnifed task formats. A generative model usually takes existing contexts, such as user prompts\\nand passages, as input to generate conditional output related to the contexts, such as answers and\\ncompleted sentences. Defining unified input and output formats for tasks in language applications\\nis easy, as they are purely text-based. Further, because both the pre-training and downstream tasks\\nare constructed in the same format (i.e., next-token-prediction), the downstream tasks conveniently\\nadapt the knowledge from pre-training tasks, resulting in surprising capabilities, such as zero-shot\\nlearning. However, graph data from different domains vary significantly by input feature (e.g., nodes\\nin a citation network have completely different vector representations as nodes in a knowledge graph)\\nand output target, preventing direct knowledge transfer between tasks. Hence, the first challenge is\\nto define a unified format for graph tasks, such that the model can do large-scale self-supervised\\npre-training on arbitrary graphs and transfer to downstream tasks seamlessly.\\n\\nTo unify graph task input, we follow the previous work OFA (Liu et al., 2023a) and extend the\\ndefinition of Text-Attribute Graph (TAG) beyond graphs with text features such as citation and\\n\\n3\\n\\n\\x0cPublished as a conference paper at ICLR 2025\\n\\nproduct networks. In fact, any node and edge features can be represented by texts. For example, in\\nairline networks, airport and flight route details can be converted into textual descriptions for nodes\\nand edges. Non-textural features, like numerical data, can also be transformed into text strings, as in\\nLLMs. Even for graphs without any features, we can still attach sentences like \"The degree of this\\nnode is 3\" to nodes. Formally, a TAG is a graph G = {V, E, XV , XE} where V and E are the sets\\nof nodes and edges. Each node v ∈ V (edge e ∈ E) corresponds to a text description x(v) ∈ XV\\n(x(e) ∈ XE). Such a format encodes almost all existing graph data and serves well as a general input\\nrepresentation.\\n\\nFor self-supervis\\n...[truncated]',\n",
       " 'https://yaoma24.github.io/': 'SNIPPET: 01/2025 Two papers on graph data vaulation accepted by ICLR 2025. 01 ... 02/2024 Check out our preprints on Graph Data Valuation, Graph Foundation Model ...\\n\\nTITLE: Research Interests\\n\\nBODY:\\nYao Ma is an assistant professor in the Department of Computer Science at Rensselaer Polytechnic Institute (RPI). Before joining RPI, he worked as an Assistant Professor at New Jersey Institute of Technology.\\nHe got his PhD from Michigan State University in 2021 under the supervision of Dr. Jiliang Tang. Before that, he completed his MS (2016) in Statistics, Probability & Operations Research at Eindhoven University of Technology and\\nBS (2015) in Mathematics and Applied Mathematics at Zhejiang University.\\n01/2025 Two papers on graph data vaulation accepted by ICLR 2025.\\n01/2025 One paper accepted by NAACL 2025.\\n11/2024 One paper accepted by LoG 2024.\\n11/2024 Honored to receive the National AI Research Resource Pilot Award to support our research in Foundation Models for Graph-Sturctued Data.\\n11/2024 Invited to give a lecture on Introduction to Machine Learning to High School seniors in the Questar III New Vision Program.\\n08/2024 I gave a lecture on Machine Learning to high school students through the PREFACE program.\\n07/2024 Honored to receive a research grant from DHS for the project Adaptive Graph-based Framework for Multi-Source Digital Forensic Analysis as PI.\\n05/2024 We are presenting a tutorial on Safe Multi-Modal Learning at KDD2024. Check out the details here.\\n05/2024 Our survey A Survey on Safe Multi-Modal Learning System is accepted by KDD2024.\\n05/2024 Three papers accepted by KDD2024.\\n05/2024 Two papers accepted by ICML2024.\\n02/2024 Check out our preprints on Graph Data Valuation, Graph Foundation Model, Mechanism of In-contex Learning, Graph Condensation, Safe Multi-Modal Learning System, and Graph Contrastive Learning Benchmarks!\\n01/2024 One paper accepted by ACM-WebSci2024.\\n01/2024 One paper accepted by WWW2024.\\n01/2024 One paper accepted by ICLR2024.\\n12/2023 One paper accepted by SDM2024.\\n09/2023 Two papers accepted by NeurIPS2023.\\n08/2023 One paper accepted by CIKM2023.\\n07/2023 One paper accepted by S&P2024.\\n05/2023 Honored to receive a research grant from NSF for the project SHINE: Prediction of Coronal Mass Ejections and Interplanetary Magnetic Fields Using Advanced Artificial Intelligence Techniques as a Co-PI.\\n05/2023 Gave a lecture on machine learning for the ISWS-REU program.\\n05/2023 One paper accepted by ACL2023.\\n04/2023 One paper accepted by TKDE.\\n04/2023 Invited to give a talk at IEEE CAS Seasonal School.\\n04/2023 One paper accepted by SIGIR2023.\\n01/2023 Receving a gift grant from Shell for the project Supply Chain Network Optimization. Thanks Shell!\\n12/2022 Invited to serve as a grant proposal panelist for NSF.\\n11/2022 Our workshop Data Science for Smart Manufacturing and Healthcare was accepted by SDM2023.\\n10/2022 Our workshop Machine Learning on Graphs (MLoG) is accepted by WSDM2023.\\n08/2022 Honored to receive a research grant from NSF for the project Collaborative Research: III: Medium: Graph Neural Networks for Heterophilous Data: Advancing the Theory, Models, and Applications as the Site PI at NJIT.\\n08/2022 Invited to give a talk for the Data Science Show at AT&T Labs.\\n08/2022 New preprint on graph contrastive learning is available here.\\n07/2022 Invited to serve as Senior PC member for AAAI2023.\\n05/2022 One paper accepted by KDD2022.\\n04/2022 Our workshop Machine Learning on Graphs (MLoG) is accepted at ICDM2022.\\n03/2022 Invited to serve as a PC menber for KDD2022, ICML2022, IJCAI2022, and NeurIPS2022.\\n02/2022 Gratefully received an NSF CRII grant (IIS-2153326) as the PI to support our research on advancing graph neural networks.\\n01/2022 Two papers accepted by ICLR2022.\\n11/2021 Our workshop “Machine Learning on Graphs (MLoG)” has been accpeted by WSDM2022.\\n10/2021 Invited to serve as Proceddings Co-chair of KDD2022.\\n10/2021 Invited to serve as PC member for SDM 2022.\\n10/2021 Invited to serve as PC menber for The Web Conference 2022.\\n09/2021 One paper accepted by NeurIPS2021.\\n08/2021 Two papers accepted by CIKM2021.\\n\\nSOURCE: https://yaoma24.github.io/',\n",
       " 'https://www.researchgate.net/publication/382654651_Boosting_Graph_Foundation_Model_from_Structural_Perspective': 'SNIPPET: To address the problem, in this paper, we boost graph foundation model from structural perspective and propose BooG. The model constructs virtual super ...\\n\\nTITLE: 382654651 Boosting Graph Foundation Model from Structural Perspective\\n\\nSOURCE: https://www.researchgate.net/publication/382654651_Boosting_Graph_Foundation_Model_from_Structural_Perspective',\n",
       " 'https://link.springer.com/article/10.1007/s11227-025-07029-9': 'SNIPPET: by M Lupo Pasini · 2025 · Cited by 5 — ... graph foundation model, along with their curation and preprocessing. ... Accepted: 01 February 2025. Published: 14 March 2025. DOI : https ...\\n\\nTITLE: Scalable training of trustworthy and energy-efficient predictive graph foundation models for atomistic materials modeling: a case study with HydraGNN\\n\\nBODY:\\nAbstract\\nWe present our work on developing and training scalable, trustworthy, and energy-efficient predictive graph foundation models (GFMs) using HydraGNN, a multi-headed graph convolutional neural network architecture. HydraGNN expands the boundaries of graph neural network (GNN) computations in both training scale and data diversity. It abstracts over message passing algorithms, allowing both reproduction of and comparison across algorithmic innovations that define nearest-neighbor convolution in GNNs. This work discusses a series of optimizations that have allowed scaling up the GFMs training to tens of thousands of GPUs on datasets consisting of hundreds of millions of graphs. Our GFMs use multitask learning (MTL) to simultaneously learn graph-level and node-level properties of atomistic structures, such as energy and atomic forces. Using over 154 million atomistic structures for training, we illustrate the performance of our approach along with the lessons learned on two state-of-the-art US Department of Energy (US-DOE) supercomputers, namely the Perlmutter petascale system at the National Energy Research Scientific Computing Center and the Frontier exascale system at Oak Ridge Leadership Computing Facility. The HydraGNN architecture enables the GFM to achieve near-linear strong scaling performance using more than 2000 GPUs on Perlmutter and 16,000 GPUs on Frontier.\\nSimilar content being viewed by others\\nAvoid common mistakes on your manuscript.\\n1 Introduction\\nDiscovery of new materials with desired properties and accurate predictions of a material’s behavior throughout its entire life span are crucial to fundamental scientific progress in energy generation, transportation, electronics, and information technology [1]. Machine learning (ML) has shown great potential in accelerating the screening and preselection of materials for further experimental testing. In particular, deep learning (DL) models effectively captured relevant underlying relationships due to the arrangement of atoms of different constituents within various atomistic structures [2,3,4,5,6,7,8,9,10,11,12]. DL models trained on data generated from experiments and/or first-principles calculations can predict the properties of interest to be used as new inputs. This inference takes only a fraction of the time it would take to run an experiment or a full first-principles calculation while still producing sufficiently accurate results. This drastic reduction in time to predict material properties using atomistic information results in a promising path toward accelerating material discovery and design [13, 14].\\nHowever, generating vast volumes of experimental and/or first-principles training data is challenging even with sophisticated experimental facilities and powerful supercomputers. Recently, foundation models (FMs) have shown the promise to navigate around this challenge: Once pre-trained over a large volume of available open-source data [15], an FM would provide a jump-start to refined models by fine-tuning on significantly smaller amounts of data for customized applications (also called downstream tasks). Reducing the number of simulations and/or experiments for generating domain-specific training data also drastically reduces the energy costs of developing domain-specific DL models.\\nWhile state-of-the-art language-based FMs with a transformer architecture have reached promising results in several domains [16,17,18,19,20,21,22,23,24,25,26,27], they fail to capture important topological aspects of the atomistic structures. Therefore, alternative DL architectures need to be considered for the development of trustworthy (henceforth understood as simultaneously accurate and highly confident) FMs for materials using atomistic-scale information.\\nSince atomistic material structures for a generic type of compound can be mapped onto a graph (where atoms can be treated as nodes and interatomic bonds as edges), graph foundation models (GFMs) [28, 29], which are FMs that operate on data structures as graphs, are the candidate of choice for these applications. Currently, GFMs proposed in the literature are developed by training graph neural network (GNN) architectures on a sufficiently large and comprehensive dataset for the domain of interest. While several efforts have already been made to develop GFMs for atomistic materials modeling applications [30,31,32,33], the existing work is still at an incipient stage. Current efforts do not yet ensure that their proposed approach achieves trustworthiness. Moreover, serious concerns have been recently raised about the unaffordable amount of energy requested by proliferating AI centers due to computationally intensive tasks of training large FMs on large volumes of data.\\nHere we describe our work toward developing scalable and trustworthy supervised GFMs for the simultaneous prediction of energies and atomic forces with careful attention to energy consumption. The GFMs have been constructed using HydraGNN [34, 35], an open-source, fully scalable GNN architecture developed at Oak Ridge National Laboratory (ORNL). Experiments were conducted on two large US-DOE supercomputers: the Perlmutter petascale machine at National Energy Research Scientific Computing Center (NERSC) and the Frontier exascale system at Oak Ridge Leadership Computing Facility (OLCF).\\nThe remaining of the paper is organized as follows. We discuss the current state of the art in Sect. 2. In Sect. 3, we discuss our approach toward developing a scalable framework and list the different optimizations for scalable training using HydraGNN. We discuss our use of large-scale hyperparameter optimization (HPO) and ensemble epistemic uncertainty quantification (UQ) to develop a trustworthy GFM. Section 4 shows the performance of different components of this work: reading large data, scaling the training process, performing HPO at large scale, and measuring epistemic uncertainty using an ensemble of HPO trials with high predictive accuracy and low energy costs. We conclude our study and discuss future work in Sect. 5.\\n2 Current state of the art\\n2.1 GNN training on open-source atomistic materials data\\nTo date, there have been a few approaches proposed in the literature to develop GFMs for atomistic materials modeling. In [30], the authors proposed a multimodal approach where multiple encoding DL architectures are trained on different types of data representations and describing different physical quantities. The models are aligned to each other through a penalization term in the training loss function that forces latent vectors from each embedding space to coincide. However, the datasets used comprise only organic molecules, which cover only a relatively small set of natural elements on the periodic table.\\nIn [32], the authors collected open-source datasets that provide labels for different properties of organic molecules. Using such a diverse collection of datasets, a GNN architecture is used for MTL in order to identify embedding spaces that capture meaningful correlations between the different labeled properties, with the promise that such an embedding space would reduce the data requirement on downstream tasks specific to organic chemistry. Since the model is trained on open-source datasets that describe only organic molecules, this approach is not transferable to inorganic compounds. Moreover, the authors compare the performance of different message passing neural network (MPNN) layers to construct the GNN architecture by performing computationally inexpensive hyperparameter tuning on small models with few parameters and transfer the use of such hyperparameters to models of much larger scale. While this approach helps limit the computational burden of HPO on large-scale GFMs, the best performing configuration of hyperparameters at small scale is not guaranteed to be the best performing configuration of hyperparameters at a larger scale and on a larger set of data, because the conclusions drawn from the HPO study are model and data dependent.\\nIn [36], the authors developed a GFM trained on the Materials Project Trajectories (MPTrj) dataset [37], using an MPNN layer that is capable of modeling four-body interactions. As the authors themselves recognize in their conclusions, while the approach sheds light onto a promising path toward building effective GFMs for atomistic materials modeling, the impact of their work is limited by the fact that the GFM has a very small number of parameters that was deliberately maintained low due to computational limitations. Moreover, this reduces the expressivity of the GFM.\\nIn [38], the authors propose a new kernel function for GNNs for modeling potential energy surfaces, which exploits chemical species information, and illustrate its efficacy in the context of transferable GFMs by pre-training the GFM on the Open Catalyst 2020 (OC2020) dataset [39] and illustrating its performance on a set of fine-tuning tasks. Since the GFM was pre-trained only on the OC2020 dataset, the applicability of this GFM is restricted to inorganic compounds.\\nWhile not explicitly presented by their developers as GFMs, there have been other models that cover broader sets of elements of the periodic table compared to the approaches mentioned in the previous paragraphs. In [31], the authors built a GNN model using MTL for simultaneous predictions of several material properties by training the GNN model on multiple datasets, including OC2020 [39] and Open Catalyst 2022 (OC2022) [40]. However, the approach considers only a single GNN architecture without performing HPO. Moreover, the set of parameters in the GNN model is relatively small, of the order of a few million parameters, which limits the attainable accuracy on large volumes of data.\\nIn [33], the authors studied the scaling behavior of 2D molecular GNNs under various settings of depth, width, number of molecules, number of labels, the diversity in dataset, and architectural choice. The authors showed that supervised pre-training of large GNNs on molecular datasets provides a rich fingerprint embedding, which is useful for 38 downstream tasks. Even if this work very systematically studied the effect of GNN model size over the predictive performance in the pre-training and fine-tuning stage with diverse downstream tasks, the work has two important limitations: It only considers 2D graphs and it addressed only organic compounds.\\nSeveral UQ methods have been applied to GNNs [41], including Bayesian GNNs [42], prediction interval methods [43], and deep ensemble methods [44]. Bayesian methods are theoretically rigorous but challenging to scale to high-dimensional data. Prediction interval methods are cost-effective but often require tedious tuning of heuristic parameters.\\nCompared to the scientific contributions mentioned above, our work distinguishes itself by leveraging extreme-scale supercomputing resources to ensure trustworthiness of the GFMs by performing (i) a scalable data management and in-memory data store, (ii) a systematic large-scale HPO across a broad set of GNN architectures, and (iii) a large-scale ensemble learning (EL) for UQ, which realizes a judicious compromise between cost and performance.\\n2.2 Scalability and GPU optimization for GNN training\\nThe effect of the specific algorithmic characteristics of GNNs on performance benchmarking has been carried out on GPUs [45], where the authors noted that GNN training differs significantly from conventional convolutional networks (CNNs) in that only 25% of the execution time is spent on dense and sparse matrix multiplications compared to 50% in CNNs. Moreover, the execution time to process graph samples in GNNs was noted to vary greatly according to the size of the graph (number of nodes and number of edges) of the input data. The studies conducted in this work showed that the majority of the time during GNN training was spent on integer operations, sorting, index selection, reductions, and scatter–gather operations needed for nodal and edge feature updates with message passing. Multi-GPU scaling was reported using up to 4 GPUs, showing about 20-50% strong scaling efficiency between 1 and 4 GPUs. Similar remarks apply to refs. [46,47,48,49], which characterize subdivision of large graphs among processors and parallel aggregation during convolution steps.\\nThese are useful conclusions for optimization of GNN training on large graphs (i.e., with millions of nodes), but need to be re-evaluated for our datasets. Indeed, training on large graphs is highly sensitive to the splitting scheme used to partition the graph into subgraphs and to distribute them among processors. However, for the atomistic materials modeling applications addressed in our work, the graph samples are small (with at most a few hundreds of nodes). In particular, the GNNs convolution on a batch of such samples will have a much more local, block diagonal structure. Therefore, throughput should be less sensitive to the choice of atomistic structures per batch.\\nUsing a larger number of GPUs, the developers of the PyTorch framework for distributed data parallelism (DDP) showed the benefit of overlapping computation with communication, showing near-linear scaling using up to 256 NVIDIA Tesla V100 GPUs [50]. These preliminary scaling results focused on DDP for training of DL model using a moderate volume of data.\\n2.3 HydraGNN\\nThe complexity of the physics and the scale at which atomistic structures must be studied in response to US-DOE needs in materials science makes it compelling to develop GNN capabilities that simultaneously satisfy several important algorithmic and computer science requirements. Specifically, a GNN architecture must provide: (1) capabilities to read and process data from multiple sources simultaneously, (2) flexibility to support diverse DOE-relevant scientific applications, (3) capabilities to scale the training on leadership class supercomputing facilities, (4) portability across heterogeneous computing environments, (5) continuous software maintenance by ensuring support and compatibility with upgraded software dependencies, and (6) maintained documentation to support new users across a broad set of international institutions.\\nWhile several GNN architectures have been made available as open-source tools to the scientific community in the last few years [51,52,53,54], none of these tools satisfies all of the above requirements. Moreover, including missing capabilities on these well-established GNN libraries requires invasive and laborious modifications for software redesign. These challenges arising\\n...[truncated]',\n",
       " 'https://yixinliu233.github.io/': 'SNIPPET: 2025/02: Our paper on graph foundation model has been accepted by PAKDD 2025. 2025/01: Our benchmark on graph-level anomaly/OOD detection has been accepted by ...\\n\\nTITLE: About Me\\n\\nBODY:\\nAbout Me\\nHi there! I am Yixin Liu, a research fellow at the School of Information and Communication Technology (ICT), Griffith University, Australia, advised by Prof. Shirui Pan. Previous to that, I obtained my Ph.D. degree from the Faculty of Information Technology, Monash University, Australia in 2024, and obtained my B.S.&M.E. degree from Beihang University, China in 2017 and 2020, respectively. My research interests are in graph neural networks, large language models, agentic AI, and anomaly detection. I am a recipient of the Google Ph.D. Fellowship in 2022.\\nNews\\n- 2025/08: Our paper on LLM multi-agent system design has been accepted by EMNLP 2025.\\n- 2025/08: Our paper on graph anomaly detection has been accepted by CIKM 2025.\\n- 2025/06: Our paper on graph neural network has been accepted by TNNLS.\\n- 2025/02: Our paper on graph foundation model has been accepted by PAKDD 2025.\\n- 2025/01: Our benchmark on graph-level anomaly/OOD detection has been accepted by ICLR 2025.\\n- 2024/12: Our paper on graph fraud detection has been accepted by AAAI 2025.\\n- 2024/09: Our paper on generalist graph anomaly detection with in-context learning has been accepted by NeurIPS 2024.\\n- 2024/08: Our paper on data-efficient graph learning has been accepted by AI Magazine.\\n- 2024/07: Our paper on graph representation learning has been accepted by CIKM 2024.\\n- 2024/07: Our paper on data imputation has been accepted by CIKM 2024.\\n- 2024/05: Our paper on GNN against label noise has been accepted by KDD 2024.\\n- 2024/03: Our paper on diffusion model for data imputation has been accepted by ICLR 2024 GenAI4DM Workshop.\\n- 2024/02: Our survey on federated learning has been accepted by IJMLC.\\n- 2023/12: Our paper on graph OOD detection has been accepted by AAAI 2024.\\n- 2023/11: I give an invited talk at LoG Conference 2023 Shanghai meetup.\\n- 2023/11: Our paper on graph+LLM has been accepted by IEEE Intelligent Systems.\\n- 2023/09: Our paper on explainable graph anomaly detection has been accepted by NeurIPS 2023.\\n- 2023/09: Our survey on data-centric graph machine learning is now on arXiv.\\n- 2023/09: Our paper on graph anomaly detection has been accepted by ICDM 2023.\\n- 2023/06: We present a tutorial on graph self-supervised learning at IJCNN 2023.\\n- 2023/05: Our paper on weak information graph learning has been accepted by KDD 2023.\\n- 2022/11: Our paper on graph representation learning has been accepted by AAAI 2023.\\n- 2022/11: Our paper on federated graph learning has been accepted by AAAI 2023.\\n- 2022/10: Our paper on graph OOD detection has been accepted by WSDM 2023.\\n- 2022/08: I am honored to receive the Google Ph.D. Fellowship in 2022.\\n- 2022/05: Our survey on graph self-supervised learning has been accepted by IEEE TKDE.\\n- 2022/01: Our paper on graph structure learning has been accepted by WWW 2022.\\n- 2021/11: Our paper on dynamic graph has been accepted by IEEE TKDE.\\n- 2021/10: Our paper on graph anomaly detection has been accepted by IEEE TKDE.\\n- 2021/08: Our paper on graph anomaly detection has been accepted by CIKM 2021.\\n- 2021/06: Our paper on label propagation has been accepted by World Wide Web.\\n- 2021/03: Our paper on graph anomaly detection has been accepted by IEEE TNNLS.\\nSelected Papers (first-author/co-first-author)\\n- Unifying Unsupervised Graph-Level Anomaly Detection and Out-of-Distribution Detection: A Benchmark\\n- Yili Wang*, Yixin Liu*, Xu Shen*, Chenyu Li*, Kaize Ding, Rui Miao, Ying Wang, Shirui Pan, Xin Wang\\n- International Conference on Learning Representations (ICLR), 2025\\n- [Paper] [Code]\\n- ARC: A Generalist Graph Anomaly Detector with in-Context Learning\\n- Yixin Liu, Shiyuan Li, Yu Zheng, Qingfeng Chen, Chengqi Zhang, Shirui Pan\\n- Advances in Neural Information Processing Systems (NeurIPS), 2024\\n- [Paper] [Code]\\n- Self-supervision Improves Diffusion Models for Tabular Data Imputation\\n- Yixin Liu, Thalaiyasingam Ajanthan, Hisham Husain, Vu Nguyen\\n- ACM International Conference on Information & Knowledge Management (CIKM), 2024\\n- [Paper] [Code]\\n- Noise-Resilient Unsupervised Graph Representation Learning via Multi-Hop Feature Quality Estimation\\n- Shiyuan Li*, Yixin Liu*, Qingfeng Chen, Geoffrey I Webb, Shirui Pan\\n- ACM International Conference on Information & Knowledge Management (CIKM), 2024\\n- [Paper] [Code]\\n- Towards Self-Interpretable Graph-Level Anomaly Detection\\n- Yixin Liu, Kaize Ding, Qinghua Lu, Fuyi Li, Leo Yu Zhang, Shirui Pan\\n- Advances in Neural Information Processing Systems (NeurIPS), 2023\\n- [Paper] [Code]\\n- Learning Strong Graph Neural Networks with Weak Information\\n- Yixin Liu, Kaize Ding, Jianling Wang, Vincent Lee, Huan Liu, Shirui Pan\\n- ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), 2023\\n- [Paper] [Code]\\n- Beyond Smoothing: Unsupervised Graph Representation Learning with Edge Heterophily Discriminating\\n- Yixin Liu, Yizhen Zheng, Daokun Zhang, Vincent Lee, Shirui Pan\\n- AAAI Conference on Artificial Intelligence (AAAI), 2023 (Oral)\\n- [Paper] [Code]\\n- Federated Learning on Non-IID Graphs via Structural Knowledge Sharing\\n- Yue Tan*, Yixin Liu*, Guodong Long, Jing Jiang, Qinghua Lu, Chengqi Zhang\\n- AAAI Conference on Artificial Intelligence (AAAI), 2023 (Oral)\\n- [Paper] [Code]\\n- GOOD-D: On Unsupervised Graph Out-Of-Distribution Detection\\n- Yixin Liu, Kaize Ding, Huan Liu, Shirui Pan\\n- ACM International Conference on Web Search and Data Mining (WSDM), 2023 (Oral)\\n- [Paper] [Code]\\n- Graph Self-Supervised Learning: A Survey\\n- Yixin Liu, Ming Jin, Shirui Pan, Chuan Zhou, Yu Zheng, Feng Xia, Philip S. Yu\\n- IEEE Transactions on Knowledge and Data Engineering (TKDE), 2022\\n- [Paper]\\n- Towards Unsupervised Deep Graph Structure Learning\\n- Yixin Liu, Yu Zheng, Daokun Zhang, Hongxu Chen, Hao Peng, Shirui Pan\\n- The Web Conference (WWW), 2022 (Best Paper Award candidate)\\n- [Paper] [Code]\\n- Anomaly Detection in Dynamic Graphs via Transformer\\n- Yixin Liu, Shirui Pan, Yu Guang Wang, Fei Xiong, Liang Wang, Qingfeng Chen, Vincent Lee\\n- IEEE Transactions on Knowledge and Data Engineering (TKDE), 2021\\n- [Paper] [Code]\\n- Anomaly Detection on Attributed Networks via Contrastive Self-Supervised Learning\\n- Yixin Liu, Zhao Li, Shirui Pan, Chen Gong, Chuan Zhou, George Karypis\\n- IEEE Transactions on Neural Networks and Learning Systems (TNNLS), 2021\\n- [Paper] [Code]\\nEducation\\nPh.D. (2021-2024) in Monash University\\nM.S. (2017-2020) in Beihang University\\nB.S. (2013-2017) in Beihang University\\nExperience\\nGriffith University, ARC Research Fellow, 2024-present.\\nAmazon, Applied Scientist Intern, 2023.\\nMonash University, Research Assistant, 2021.\\nAlibaba, Research Intern, 2020.\\nContact\\nEmail: yixin.liu[at]griffith[dot]edu[dot]au\\nOffice: G23 2.38, 1 Parkland Dr, Southport, QLD 4215\\n\\nSOURCE: https://yixinliu233.github.io/',\n",
       " 'https://link.springer.com/content/pdf/10.1007/978-981-96-8197-6_13.pdf?pdf=inline%20link': 'SNIPPET: 3.1 Graph Foundation Model. The Graph Foundation Model (GFM) approach adapts GraphAny to our travel recommendation task, focusing on the bipartite structure ...\\n\\nTITLE: Investigating the\\xa0Limits of\\xa0Graph Foundation Model in\\xa0Real-World Travel Recommendation Systems\\n\\nBODY:\\nAbstract\\nGraph foundation models (GFMs) have demonstrated remarkable potential in capturing intricate relational patterns, achieving state-of-the-art results in numerous graph-centric tasks. However, their real-world applicability remains underexplored in highly domain-specific contexts, such as travel recommendation. In this paper, we present a comprehensive evaluation of GFMs for large-scale travel recommendation tasks using a bipartite user–destination dataset of 86,761 travelers within South Korea. We compare representative GFM against both conventional graph-based methods and vector-based methods. Contrary to the prevailing expectation that GFMs should outperform traditional architectures, our empirical findings reveal that domain-specific constraints can dilute the benefits of extensive multi-hop message passing, leading to suboptimal performance. Our work highlights a critical need to validate GFMs against domain-specific constraints, offering a roadmap for their future adaptation and optimization in real-world applications.\\nAccess this chapter\\nTax calculation will be finalised at checkout\\nPurchases are for personal use only\\nSimilar content being viewed by others\\nReferences\\nAsratian, A.S., Denley, T.M., Häggkvist, R.: Bipartite Graphs And Their Applications, vol. 131. Cambridge university press (1998)\\nBank, D., Koenigstein, N., Giryes, R.: Autoencoders. Machine learning for Data Science Handbook: Data Mining and Knowledge Discovery Handbook, pp. 353–374 (2023)\\nChen, L., Cao, J., Liang, W., Ye, Q.: Geography-aware heterogeneous graph contrastive learning for travel recommendation. ACM Trans. Spatial Algorithms Syst. 10(3), 27:1–27:22 (2024)\\nChen, L., Cao, J., Wang, Y., Liang, W., Zhu, G.: Multi-view graph attention network for travel recommendation. Expert Syst. Appl. 191, 116234 (2022)\\nDai, E., Jin, W., Liu, H., Wang, S.: Towards robust graph neural networks for noisy graphs with sparse labels. CoRR abs/2201.00232 (2022)\\nGrover, A., Leskovec, J.: node2vec: scalable feature learning for networks. In: Krishnapuram, B., Shah, M., Smola, A.J., Aggarwal, C.C., Shen, D., Rastogi, R. (eds.) Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 13-17, 2016, pp. 855–864. ACM (2016)\\nHamilton, W.L., Ying, Z., Leskovec, J.: Inductive representation learning on large graphs. In: Guyon, I., von Luxburg, U., Bengio, S., Wallach, H.M., Fergus, R., Vishwanathan, S.V.N., Garnett, R. (eds.) Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 1024–1034 (2017)\\nHiggins, I., et al.: Beta-VAE: learning basic visual concepts with a constrained variational framework. In: 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net (2017)\\nJadon, A., Patil, A.: A comprehensive survey of evaluation techniques for recommendation systems. CoRR abs/2312.16015 (2023)\\nKingma, D.P., Welling, M.: Auto-encoding variational bayes. In: Bengio, Y., LeCun, Y. (eds.) 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings (2014)\\nKipf, T.N., Welling, M.: Semi-supervised classification with graph convolutional networks. CoRR abs/1609.02907 (2016)\\nLichtenwalter, R., Lussier, J.T., Chawla, N.V.: New perspectives and methods in link prediction. In: Rao, B., Krishnapuram, B., Tomkins, A., Yang, Q. (eds.) Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Washington, DC, USA, July 25-28, 2010, pp. 243–252. ACM (2010)\\nLiu, J., et al.: Towards graph foundation models: a survey and beyond. CoRR abs/2310.11829 (2023)\\nMostafa, H., Nassar, M., Majumdar, S.: On local aggregation in heterophilic graphs. CoRR abs/2106.03213 (2021)\\nScarselli, F., Gori, M., Tsoi, A.C., Hagenbuchner, M., Monfardini, G.: The graph neural network model. IEEE Trans. Neural Netw. 20(1), 61–80 (2009)\\nWu, S., Zhang, W., Sun, F., Cui, B.: Graph neural networks in recommender systems: a survey. CoRR abs/2011.02260 (2020)\\nZhao, J., Zhu, Z., Galkin, M., Mostafa, H., Bronstein, M., Tang, J.: Fully-inductive node classification on arbitrary graphs. In: International Conference on Learning Representations (ICLR) (2025)\\nZheng, Y.: Trajectory data mining: an overview. ACM Trans. Intell. Syst. Technol. 6(3), 29:1–29:41 (2015)\\nAcknowledgments\\nWe would like to acknowledge Bosung Jung, and Gyumin Lee (Nara information) for their valuable contributions to the implementation of the VAE model and the construction of the dataset. Their insights and technical expertise significantly enhanced the quality of this work.\\nAuthor information\\nAuthors and Affiliations\\nCorresponding author\\nEditor information\\nEditors and Affiliations\\nRights and permissions\\nCopyright information\\n© 2025 The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.\\nAbout this paper\\nCite this paper\\nLee, N., Lee, G., Lee, D. (2025). Investigating the Limits of Graph Foundation Model in Real-World Travel Recommendation Systems. In: Yuan, S., Malliaros, F., Zheng, X. (eds) Trends and Applications in Knowledge Discovery and Data Mining. PAKDD 2025. Lecture Notes in Computer Science(), vol 15835. Springer, Singapore. https://doi.org/10.1007/978-981-96-8197-6_13\\nDownload citation\\nDOI: https://doi.org/10.1007/978-981-96-8197-6_13\\nPublished:\\nPublisher Name: Springer, Singapore\\nPrint ISBN: 978-981-96-8196-9\\nOnline ISBN: 978-981-96-8197-6\\neBook Packages: Computer ScienceComputer Science (R0)\\n\\nSOURCE: https://link.springer.com/content/pdf/10.1007/978-981-96-8197-6_13.pdf?pdf=inline%20link',\n",
       " 'https://lechengkong.github.io/': 'SNIPPET: Very recent interests on developing universal graph foundation model! Our work accepted as ICLR-24 Spotlight is here. I am excited to talk about related ...\\n\\nTITLE: Lecheng Kong (孔乐成)\\n\\nBODY:\\nI am a fifth-year Ph.D. candidate at Washington University in St. Louis. I focus on studying and improving Graph Neural Networks (GNNs) and Graph Foundation Models (GFM). My recent research interest is in the intersection of Graph Models and Large Language Models, from GFM to graph-based retrieval augmented generation.\\nVery recent interests on developing universal graph foundation model! Our work accepted as ICLR-24 Spotlight is here. I am excited to talk about related ideas.\\nI obtained my Bachelor’s degree also from Washington University in St. Louis. It is my honor to be advised by Dr. Yixin Chen towards my Ph.D. degree. For details, check out my CV.\\nI am actively looking for Full-time research-oriented position starting Summer/Fall 2025. If you are interested in my work, I would love to chat and discuss potential opportunities. You can contact me through email.\\n🔥 News\\n- 2025.01: 🔥🔥 GOFA was accepted by ICLR2025!\\n- 2024.08: 🔥🔥 GOFA released, we propose a generative approach to solve a wide variety of tasks in the graph domain. GOFA can generate free-form graph-related response to arbitrary human input.\\n- 2024.08: 🔥🔥 TAGLAS released, TAGLAS aim to provide a graph dataset with high diversity from synthetic to real-world data. Check it out if you are building graph foundation models.\\n- 2024.01: 🎉🎉 COLA accpeted by WWW2024! Congratulations to Hao!\\n- 2024.01: 🎉🎉 OneForAll paper accepted as Spotlight (5%) by ICLR2024. Thanks everyone for the great teamwork!\\n📝 Publications\\n(* Equal contribution)\\nICLR 2025\\nGOFA: A Generative One-For-All Model for Joint Graph Language Modeling, Lecheng Kong*, Jiarui Feng*, Hao Liu*, Chengsong Huang, Jiaxin Huang, Yixin Chen, Muhan Zhang (To appear)ICLR 2024\\nOne for All: Towards Training One Graph Model for All Classification Tasks, Hao Liu*, Jiarui Feng*, Lecheng Kong*, Ningyue Liang, Dacheng Tao, Yixin Chen, Muhan ZhangWWW 2024\\nGraph Contrastive Learning Meets Graph Meta Learning: A Unified Method for Few-shot Node Tasks, Hao Liu, Jiarui Feng, Lecheng Kong, Dacheng Tao, Yixin Chen, Muhan ZhangNeurIPS 2023\\nMAG-GNN: Reinforcement Learning Boosted Graph Neural Network, Lecheng Kong, Jiarui Feng, Hao Liu, Dacheng Tao, Yixin Chen, Muhan ZhangNeurIPS 2023\\nExtending the Design Space of Graph Neural Networks by Rethinking Folklore Weisfeiler-Lehman, Jiarui Feng, Lecheng Kong, Hao Liu, Dacheng Tao, Fuhai Li, Muhan Zhang, Yixin ChenNeurIPS 2022\\nGeodesic Graph Neural Network for Efficient Graph Representation Learning, Lecheng Kong, Yixin Chen, Muhan ZhangIJCAI 2022\\nManipulating Elections by Changing Voter Perceptions, Junlin Wu, Andrew Estornell, Lecheng Kong, Yevgeniy VorobeychikPreprint\\nTAGLAS: An atlas of text-attributed graph datasets in the era of large graph and language models, Jiarui Feng, Hao Liu, Lecheng Kong, Yixin Chen, Muhan ZhangPreprint\\nA Multi-View Joint Learning Framework for Embedding Clinical Codes and Text Using Graph Neural Networks, Lecheng Kong, Christopher King, Bradley Fritz, Yixin ChenPreprint\\nTime Associated Meta Learning for Clinical Prediction, Hao Liu, Muhan Zhang, Zehao Dong, Lecheng Kong, Yixin Chen, Bradley Fritz, Dacheng Tao, Christopher King\\n💻 Internships\\n- 2024.05 - 2024.08, Applied Scientist Intern, Amazon, Santa Clara.\\n- 2019.05 - 2019.08, Software Engineer Intern, Google, Mountain View.\\n🎖 Honors and Awards\\n- 2022/2023 NeurIPS travel award.\\n- 2007.08 Chunjianghuayue Young Residents Swimming Competition, Runner-up.\\n🤝 Services\\n- Reviewer for conferences: CVPR23/24; NeurIPS23/24; ICLR24/25；KDD24/25; ECCV24;\\n📖 Educations\\n- 2020.09 - (now), Ph.D., Computer Science, Washington University in St. Louis.\\n- 2016.09 - 2020.06, Bachelor’s/Master’s, Computer Science, Washington University in St. Louis.\\n- 2010.9 - 2016.06, High School, Academic, Hangzhou Foreign Language School.\\nPersonal\\n- I am from Hangzhou, China.\\n- I enjoy badminton🏸 and skiing⛷️.\\n- Game enthusiast with horrible gaming skills.\\n\\nSOURCE: https://lechengkong.github.io/'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.0 paper name提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'main' from '/home/zxia545/_Code/Research_repo/25_0819_kdd_hr/kdd_hr_system/search_dev/talent_search_modules/main.py'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import config\n",
    "import utils\n",
    "import schemas\n",
    "import llm\n",
    "import search\n",
    "import extraction\n",
    "import graph\n",
    "import main\n",
    "\n",
    "import importlib\n",
    "importlib.reload(config)\n",
    "importlib.reload(utils)\n",
    "importlib.reload(extraction)\n",
    "importlib.reload(schemas)\n",
    "importlib.reload(llm)\n",
    "importlib.reload(search)\n",
    "importlib.reload(graph)\n",
    "importlib.reload(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[extract_paper_name] Have paper name: True, extracted paper: GOFA: A Generative One-For-All Model for Joint Graph Language Modeling\n",
      "新增论文: GOFA: A Generative One-For-All Model for Joint Graph Language Modeling\n",
      "[extract_paper_name] Have paper name: True, extracted paper: Boosting Graph Foundation Model from Structural Perspective\n",
      "新增论文: Boosting Graph Foundation Model from Structural Perspective\n",
      "[extract_paper_name] Have paper name: True, extracted paper: GOFA: A GENERATIVE ONE-FOR-ALL MODEL FOR JOINT GRAPH LANGUAGE MODELING\n",
      "新增论文: GOFA: A GENERATIVE ONE-FOR-ALL MODEL FOR JOINT GRAPH LANGUAGE MODELING\n",
      "[extract_paper_name] Have paper name: True, extracted paper: Turning Tabular Foundation Models into Graph Foundation Models\n",
      "新增论文: Turning Tabular Foundation Models into Graph Foundation Models\n",
      "[extract_paper_name] Have paper name: False, extracted paper: \n",
      "[extract_paper_name] Have paper name: True, extracted paper: How Expressive are Knowledge Graph Foundation Models?\n",
      "新增论文: How Expressive are Knowledge Graph Foundation Models?\n",
      "[extract_paper_name] Have paper name: False, extracted paper: \n",
      "[extract_paper_name] Have paper name: True, extracted paper: A Survey on Self-Supervised Graph Foundation Models: Knowledge-Based Perspective\n",
      "新增论文: A Survey on Self-Supervised Graph Foundation Models: Knowledge-Based Perspective\n",
      "[extract_paper_name] Have paper name: True, extracted paper: A Prompt-Based Knowledge Graph Foundation Model for Universal In-Context Reasoning\n",
      "新增论文: A Prompt-Based Knowledge Graph Foundation Model for Universal In-Context Reasoning\n",
      "[extract_paper_name] Have paper name: True, extracted paper: OpenGraph: Towards Open Graph Foundation Models\n",
      "新增论文: OpenGraph: Towards Open Graph Foundation Models\n",
      "[extract_paper_name] Have paper name: True, extracted paper: AnyGraph: Graph Foundation Model in the Wild\n",
      "新增论文: AnyGraph: Graph Foundation Model in the Wild\n",
      "[extract_paper_name] Have paper name: True, extracted paper: GOFA: A Generative One-For-All Model for Joint Graph Language Modeling\n",
      "论文已存在，添加新URL: GOFA: A Generative One-For-All Model for Joint Graph Language Modeling\n",
      "[extract_paper_name] Have paper name: True, extracted paper: Towards Graph Foundation Models: Learning Generalities Across Graphs via Task-Trees\n",
      "新增论文: Towards Graph Foundation Models: Learning Generalities Across Graphs via Task-Trees\n",
      "[extract_paper_name] Have paper name: True, extracted paper: Graph Foundation Models: Concepts, Opportunities and Challenges\n",
      "新增论文: Graph Foundation Models: Concepts, Opportunities and Challenges\n",
      "[extract_paper_name] Have paper name: True, extracted paper: SAMGPT: Text-free Graph Foundation Model for Multi-domain Pre-training and Cross-domain Adaptation\n",
      "新增论文: SAMGPT: Text-free Graph Foundation Model for Multi-domain Pre-training and Cross-domain Adaptation\n",
      "[extract_paper_name] Have paper name: True, extracted paper: Graph Foundation Models: Concepts, Opportunities and Challenges\n",
      "论文已存在，添加新URL: Graph Foundation Models: Concepts, Opportunities and Challenges\n",
      "[extract_paper_name] Have paper name: True, extracted paper: GOFA: A GENERATIVE ONE-FOR-ALL MODEL FOR JOINT GRAPH LANGUAGE MODELING\n",
      "论文已存在，添加新URL: GOFA: A GENERATIVE ONE-FOR-ALL MODEL FOR JOINT GRAPH LANGUAGE MODELING\n",
      "[extract_paper_name] Have paper name: False, extracted paper: \n",
      "[extract_paper_name] Have paper name: True, extracted paper: Boosting Graph Foundation Model from Structural Perspective\n",
      "论文已存在，添加新URL: Boosting Graph Foundation Model from Structural Perspective\n",
      "[extract_paper_name] Have paper name: True, extracted paper: Scalable training of trustworthy and energy-efficient predictive graph foundation models for atomistic materials modeling: a case study with HydraGNN\n",
      "新增论文: Scalable training of trustworthy and energy-efficient predictive graph foundation models for atomistic materials modeling: a case study with HydraGNN\n",
      "[extract_paper_name] Have paper name: True, extracted paper: Unifying Unsupervised Graph-Level Anomaly Detection and Out-of-Distribution Detection: A Benchmark\n",
      "新增论文: Unifying Unsupervised Graph-Level Anomaly Detection and Out-of-Distribution Detection: A Benchmark\n",
      "[extract_paper_name] Have paper name: True, extracted paper: Investigating the Limits of Graph Foundation Model in Real-World Travel Recommendation Systems\n",
      "新增论文: Investigating the Limits of Graph Foundation Model in Real-World Travel Recommendation Systems\n",
      "[extract_paper_name] Have paper name: True, extracted paper: One for All: Towards Training One Graph Model for All Classification Tasks\n",
      "新增论文: One for All: Towards Training One Graph Model for All Classification Tasks\n",
      "\n",
      "共找到 16 篇唯一论文\n"
     ]
    }
   ],
   "source": [
    "# 使用新的结构化方法管理论文信息，支持自动去重\n",
    "paper_collection = schemas.PaperCollection()\n",
    "\n",
    "# 从源中提取论文名称并添加到集合中\n",
    "for fetch_item in fetch_sources.items():\n",
    "    paper_name_spec_item = extraction.extract_paper_name_from_sources(fetch_item, mock_spec)\n",
    "    if paper_name_spec_item.have_paper_name:\n",
    "        # 使用新的 add_paper 方法，会自动去重\n",
    "        is_new = paper_collection.add_paper(\n",
    "            paper_name=paper_name_spec_item.paper_name,\n",
    "            url=fetch_item[0]\n",
    "        )\n",
    "        if is_new:\n",
    "            print(f\"新增论文: {paper_name_spec_item.paper_name}\")\n",
    "        else:\n",
    "            print(f\"论文已存在，添加新URL: {paper_name_spec_item.paper_name}\")\n",
    "\n",
    "# 获取所有唯一的论文名称用于后续处理\n",
    "unique_paper_names = paper_collection.get_paper_names()\n",
    "print(f\"\\n共找到 {len(unique_paper_names)} 篇唯一论文\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 创建 URL 到论文名称的映射，用于语义搜索\n",
    "final_fetch_paper_name = {}\n",
    "for paper_info in paper_collection.get_all_papers():\n",
    "    # 使用第一个URL作为代表\n",
    "    primary_url = paper_info.primary_url or paper_info.urls[0]\n",
    "    final_fetch_paper_name[primary_url] = paper_info.paper_name\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'https://iclr.cc/virtual/2025/poster/28473': 'GOFA: A Generative One-For-All Model for Joint Graph Language Modeling',\n",
       " 'https://dl.acm.org/doi/10.1145/3711896.3736568': 'Boosting Graph Foundation Model from Structural Perspective',\n",
       " 'https://proceedings.iclr.cc/paper_files/paper/2025/file/652c104b5b0652a03684efeaf805463b-Paper-Conference.pdf': 'GOFA: A GENERATIVE ONE-FOR-ALL MODEL FOR JOINT GRAPH LANGUAGE MODELING',\n",
       " 'https://arxiv.org/html/2508.20906v1': 'Turning Tabular Foundation Models into Graph Foundation Models',\n",
       " 'https://icml.cc/virtual/2025/poster/44147': 'How Expressive are Knowledge Graph Foundation Models?',\n",
       " 'https://arxiv.org/pdf/2403.16137': 'A Survey on Self-Supervised Graph Foundation Models: Knowledge-Based Perspective',\n",
       " 'https://neurips.cc/virtual/2024/poster/94900': 'A Prompt-Based Knowledge Graph Foundation Model for Universal In-Context Reasoning',\n",
       " 'https://aclanthology.org/2024.findings-emnlp.132.pdf': 'OpenGraph: Towards Open Graph Foundation Models',\n",
       " 'https://www.researchgate.net/publication/383267044_AnyGraph_Graph_Foundation_Model_in_the_Wild': 'AnyGraph: Graph Foundation Model in the Wild',\n",
       " 'https://icml.cc/virtual/2025/poster/46113': 'Towards Graph Foundation Models: Learning Generalities Across Graphs via Task-Trees',\n",
       " 'https://arxiv.org/html/2310.11829v4': 'Graph Foundation Models: Concepts, Opportunities and Challenges',\n",
       " 'https://www.yfang.site/publications': 'SAMGPT: Text-free Graph Foundation Model for Multi-domain Pre-training and Cross-domain Adaptation',\n",
       " 'https://link.springer.com/article/10.1007/s11227-025-07029-9': 'Scalable training of trustworthy and energy-efficient predictive graph foundation models for atomistic materials modeling: a case study with HydraGNN',\n",
       " 'https://yixinliu233.github.io/': 'Unifying Unsupervised Graph-Level Anomaly Detection and Out-of-Distribution Detection: A Benchmark',\n",
       " 'https://link.springer.com/content/pdf/10.1007/978-981-96-8197-6_13.pdf?pdf=inline%20link': 'Investigating the Limits of Graph Foundation Model in Real-World Travel Recommendation Systems',\n",
       " 'https://lechengkong.github.io/': 'One for All: Towards Training One Graph Model for All Classification Tasks'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_fetch_paper_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 作者提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 演示新的语义搜索功能 ===\n",
      "使用新的批量搜索方法：\n",
      "\n",
      "搜索结果（共 16 篇论文）：\n",
      "\n",
      "📖 论文: GOFA: A Generative One-For-All Model for Joint Graph Language Modeling\n",
      "   URL: https://iclr.cc/virtual/2025/poster/28473\n",
      "   找到: 是\n",
      "   匹配度: 248.019\n",
      "   论文ID: 055af7789c29452bf808a48f74ec25e01049425e\n",
      "   年份: 2024\n",
      "   会议: International Conference on Learning Representations\n",
      "   作者列表:\n",
      "     • Lecheng Kong (ID: 2164063663)\n",
      "     • Jiarui Feng (ID: 2239091724)\n",
      "     • Hao Liu (ID: 2264134998)\n",
      "     • Chengsong Huang (ID: 31937655)\n",
      "     • Jiaxin Huang (ID: 2300246276)\n",
      "     • Yixin Chen (ID: 2266810999)\n",
      "     • Muhan Zhang (ID: 2239188141)\n",
      "\n",
      "📖 论文: Boosting Graph Foundation Model from Structural Perspective\n",
      "   URL: https://dl.acm.org/doi/10.1145/3711896.3736568\n",
      "   找到: 是\n",
      "   匹配度: 212.0903\n",
      "   论文ID: 1051287627ca209b0fdc7899d6abf366f076cb59\n",
      "   年份: 2024\n",
      "   会议: arXiv.org\n",
      "   作者列表:\n",
      "     • Yao Cheng (ID: 2165643958)\n",
      "     • Yige Zhao (ID: 2266749154)\n",
      "     • Jianxiang Yu (ID: 2198507600)\n",
      "     • Xiang Li (ID: 2258789569)\n",
      "\n",
      "📖 论文: GOFA: A GENERATIVE ONE-FOR-ALL MODEL FOR JOINT GRAPH LANGUAGE MODELING\n",
      "   URL: https://proceedings.iclr.cc/paper_files/paper/2025/file/652c104b5b0652a03684efeaf805463b-Paper-Conference.pdf\n",
      "   找到: 是\n",
      "   匹配度: 250.30241\n",
      "   论文ID: 055af7789c29452bf808a48f74ec25e01049425e\n",
      "   年份: 2024\n",
      "   会议: International Conference on Learning Representations\n",
      "   作者列表:\n",
      "     • Lecheng Kong (ID: 2164063663)\n",
      "     • Jiarui Feng (ID: 2239091724)\n",
      "     • Hao Liu (ID: 2264134998)\n",
      "     • Chengsong Huang (ID: 31937655)\n",
      "     • Jiaxin Huang (ID: 2300246276)\n",
      "     • Yixin Chen (ID: 2266810999)\n",
      "     • Muhan Zhang (ID: 2239188141)\n",
      "\n",
      "📖 论文: Turning Tabular Foundation Models into Graph Foundation Models\n",
      "   URL: https://arxiv.org/html/2508.20906v1\n",
      "   找到: 是\n",
      "   匹配度: 259.16794\n",
      "   论文ID: 03ee0f2482b544ef9705f31edeac5cf1c0197304\n",
      "   年份: 2025\n",
      "   作者列表:\n",
      "     • Dmitry Eremeev (ID: 2377795413)\n",
      "     • Gleb Bazhenov (ID: 2172208866)\n",
      "     • Oleg Platonov (ID: 2184723723)\n",
      "     • Artem Babenko (ID: 2377796287)\n",
      "     • Liudmila Prokhorenkova (ID: 2323374652)\n",
      "\n",
      "📖 论文: How Expressive are Knowledge Graph Foundation Models?\n",
      "   URL: https://icml.cc/virtual/2025/poster/44147\n",
      "   找到: 是\n",
      "   匹配度: 196.40707\n",
      "   论文ID: 9023a11bf44bf6f14c69d3b61ef3a7607ac1bbde\n",
      "   年份: 2025\n",
      "   会议: arXiv.org\n",
      "   作者列表:\n",
      "     • Xingyue Huang (ID: 2152662727)\n",
      "     • Pablo Barcel'o (ID: 2212526363)\n",
      "     • Michael M. Bronstein (ID: 2253393509)\n",
      "     • I. Ceylan (ID: 49633004)\n",
      "     • Mikhail Galkin (ID: 2066369448)\n",
      "     • Juan L. Reutter (ID: 2256998888)\n",
      "     • Miguel Romero (ID: 2256208484)\n",
      "\n",
      "📖 论文: A Survey on Self-Supervised Graph Foundation Models: Knowledge-Based Perspective\n",
      "   URL: https://arxiv.org/pdf/2403.16137\n",
      "   找到: 是\n",
      "   匹配度: 251.504\n",
      "   论文ID: f67f718288c545164704a3e253e165be5a6da59f\n",
      "   年份: 2024\n",
      "   会议: IEEE Transactions on Knowledge and Data Engineering\n",
      "   作者列表:\n",
      "     • Zi-qiang Zhao (ID: 2209282615)\n",
      "     • Yixin Su (ID: 2313910549)\n",
      "     • Yuhua Li (ID: 2260170121)\n",
      "     • Yixiong Zou (ID: 2275592358)\n",
      "     • Ruixuan Li (ID: 2283487404)\n",
      "     • Rui Zhang (ID: 2293586635)\n",
      "\n",
      "📖 论文: A Prompt-Based Knowledge Graph Foundation Model for Universal In-Context Reasoning\n",
      "   URL: https://neurips.cc/virtual/2024/poster/94900\n",
      "   找到: 是\n",
      "   匹配度: 242.06308\n",
      "   论文ID: c07151eb48e3923193ac7c50001c5bc240025b5f\n",
      "   年份: 2024\n",
      "   会议: Neural Information Processing Systems\n",
      "   作者列表:\n",
      "     • Yuanning Cui (ID: 1410917541)\n",
      "     • Zequn Sun (ID: 2109745316)\n",
      "     • Wei Hu (ID: 2311649985)\n",
      "\n",
      "📖 论文: OpenGraph: Towards Open Graph Foundation Models\n",
      "   URL: https://aclanthology.org/2024.findings-emnlp.132.pdf\n",
      "   找到: 是\n",
      "   匹配度: 214.75404\n",
      "   论文ID: feca496290013a250adb2c72ecd20c8d5fd30f24\n",
      "   年份: 2024\n",
      "   会议: Conference on Empirical Methods in Natural Language Processing\n",
      "   作者列表:\n",
      "     • Lianghao Xia (ID: 1830455155)\n",
      "     • Ben Kao (ID: 2289844482)\n",
      "     • Chao Huang (ID: 2261248317)\n",
      "\n",
      "📖 论文: AnyGraph: Graph Foundation Model in the Wild\n",
      "   URL: https://www.researchgate.net/publication/383267044_AnyGraph_Graph_Foundation_Model_in_the_Wild\n",
      "   找到: 是\n",
      "   匹配度: 173.78833\n",
      "   论文ID: 5aa4dc0f24343745175411ca47c8207b41618707\n",
      "   年份: 2024\n",
      "   会议: arXiv.org\n",
      "   作者列表:\n",
      "     • Lianghao Xia (ID: 1830455155)\n",
      "     • Chao Huang (ID: 2261248317)\n",
      "\n",
      "📖 论文: Towards Graph Foundation Models: Learning Generalities Across Graphs via Task-Trees\n",
      "   URL: https://icml.cc/virtual/2025/poster/46113\n",
      "   找到: 是\n",
      "   匹配度: 339.0381\n",
      "   论文ID: 6a1e0241d29484eece376ecedf2a812ca0e1e46c\n",
      "   年份: 2024\n",
      "   作者列表:\n",
      "     • Zehong Wang (ID: 2284578810)\n",
      "     • Zheyuan Zhang (ID: 2284184904)\n",
      "     • Tianyi Ma (ID: 2283516218)\n",
      "     • Nitesh V. Chawla (ID: 2292582566)\n",
      "     • Chuxu Zhang (ID: 2117879943)\n",
      "     • Yanfang Ye (ID: 2093920413)\n",
      "\n",
      "📖 论文: Graph Foundation Models: Concepts, Opportunities and Challenges\n",
      "   URL: https://arxiv.org/html/2310.11829v4\n",
      "   找到: 是\n",
      "   匹配度: 192.86075\n",
      "   论文ID: 6912a12f3710bf4bbdfd9f55f90311426fc1c32c\n",
      "   年份: 2023\n",
      "   会议: IEEE Transactions on Pattern Analysis and Machine Intelligence\n",
      "   作者列表:\n",
      "     • Jiawei Liu (ID: 2260178816)\n",
      "     • Cheng Yang (ID: 2257052319)\n",
      "     • Zhiyuan Lu (ID: 2110327382)\n",
      "     • Junze Chen (ID: 2260645232)\n",
      "     • Yibo Li (ID: 2274190647)\n",
      "     • Mengmei Zhang (ID: 16003017)\n",
      "     • Ting Bai (ID: 2300370790)\n",
      "     • Yuan Fang (ID: 2267220071)\n",
      "     • Lichao Sun (ID: 2257354508)\n",
      "     • Philip S. Yu (ID: 2258679535)\n",
      "     • Chuan Shi (ID: 2257131498)\n",
      "\n",
      "📖 论文: SAMGPT: Text-free Graph Foundation Model for Multi-domain Pre-training and Cross-domain Adaptation\n",
      "   URL: https://www.yfang.site/publications\n",
      "   找到: 是\n",
      "   匹配度: 313.2812\n",
      "   论文ID: b0f3a565dd644cdb135d68ec8c1510a08afb4482\n",
      "   年份: 2025\n",
      "   会议: The Web Conference\n",
      "   作者列表:\n",
      "     • Xingtong Yu (ID: 2204706384)\n",
      "     • Zechuan Gong (ID: 2344801250)\n",
      "     • Chang Zhou (ID: 2271793882)\n",
      "     • Yuan Fang (ID: 2307591774)\n",
      "     • Hui Zhang (ID: 2344826761)\n",
      "\n",
      "📖 论文: Scalable training of trustworthy and energy-efficient predictive graph foundation models for atomistic materials modeling: a case study with HydraGNN\n",
      "   URL: https://link.springer.com/article/10.1007/s11227-025-07029-9\n",
      "   找到: 是\n",
      "   匹配度: 354.61664\n",
      "   论文ID: 454e2d2c6799ff02e80fb41241f597e28d95bf11\n",
      "   年份: 2024\n",
      "   会议: Journal of Supercomputing\n",
      "   作者列表:\n",
      "     • Massimiliano Lupo Pasini (ID: 48460197)\n",
      "     • Jong Youl Choi (ID: 2155367341)\n",
      "     • Kshitij Mehta (ID: 2266401208)\n",
      "     • Pei Zhang (ID: 2266463752)\n",
      "     • David Rogers (ID: 2307466716)\n",
      "     • Jonghyun Bae (ID: 2266401334)\n",
      "     • Khaled Ibrahim (ID: 2266400332)\n",
      "     • Ashwin M. Aji (ID: 2256984159)\n",
      "     • Karl W. Schulz (ID: 2256977081)\n",
      "     • Jorda Polo (ID: 2307465603)\n",
      "     • Prasanna Balaprakash (ID: 2138151793)\n",
      "\n",
      "📖 论文: Unifying Unsupervised Graph-Level Anomaly Detection and Out-of-Distribution Detection: A Benchmark\n",
      "   URL: https://yixinliu233.github.io/\n",
      "   找到: 是\n",
      "   匹配度: 273.07202\n",
      "   论文ID: d212c555293b81f845b3c99af4e922b0fcdb4290\n",
      "   年份: 2024\n",
      "   会议: International Conference on Learning Representations\n",
      "   作者列表:\n",
      "     • Yili Wang (ID: 2298205559)\n",
      "     • Yixin Liu (ID: 2242962967)\n",
      "     • Xu Shen (ID: 2298108098)\n",
      "     • Chenyu Li (ID: 2308068445)\n",
      "     • Kaize Ding (ID: 2261590627)\n",
      "     • Rui Miao (ID: 2171753488)\n",
      "     • Ying Wang (ID: 2303364512)\n",
      "     • Shirui Pan (ID: 2298272389)\n",
      "     • Xin Wang (ID: 2298094103)\n",
      "\n",
      "📖 论文: Investigating the Limits of Graph Foundation Model in Real-World Travel Recommendation Systems\n",
      "   URL: https://link.springer.com/content/pdf/10.1007/978-981-96-8197-6_13.pdf?pdf=inline%20link\n",
      "   找到: 是\n",
      "   匹配度: 245.65686\n",
      "   论文ID: 567be24600ad3c075c431641ebddb5a0adfab0b5\n",
      "   年份: 2025\n",
      "   会议: Pacific-Asia Conference on Knowledge Discovery and Data Mining\n",
      "   作者列表:\n",
      "     • Nayoung Lee (ID: 2370619074)\n",
      "     • Gunmin Lee (ID: 2370075162)\n",
      "     • Donghun Lee (ID: 2370066402)\n",
      "\n",
      "📖 论文: One for All: Towards Training One Graph Model for All Classification Tasks\n",
      "   URL: https://lechengkong.github.io/\n",
      "   找到: 是\n",
      "   匹配度: 273.39832\n",
      "   论文ID: ab22d54dd13876d25c6c8f46c40fb9ac41c61ec5\n",
      "   年份: 2023\n",
      "   会议: International Conference on Learning Representations\n",
      "   作者列表:\n",
      "     • Hao Liu (ID: 2264134998)\n",
      "     • Jiarui Feng (ID: 48441034)\n",
      "     • Lecheng Kong (ID: 2164063663)\n",
      "     • Ningyue Liang (ID: 2270817857)\n",
      "     • Dacheng Tao (ID: 2244621611)\n",
      "     • Yixin Chen (ID: 2223152252)\n",
      "     • Muhan Zhang (ID: 2239188141)\n"
     ]
    }
   ],
   "source": [
    "import semantic_paper_search\n",
    "import importlib\n",
    "importlib.reload(semantic_paper_search)\n",
    "from semantic_paper_search import SemanticScholarClient\n",
    "\n",
    "\"\"\"演示新的语义搜索功能\"\"\"\n",
    "print(\"=== 演示新的语义搜索功能 ===\")\n",
    "\n",
    "# 初始化客户端\n",
    "client = SemanticScholarClient(api_key=\"17kd4WbzBh3qzF0kbVqf81zNFv7qg26mJJwTcAq9\")\n",
    "\n",
    "\n",
    "print(\"使用新的批量搜索方法：\")\n",
    "results = client.search_papers_with_authors_batch(\n",
    "\turl_to_title=final_fetch_paper_name,\n",
    "\tmin_score=0.80,\n",
    ")\n",
    "\n",
    "valid_results = [r for r in results if r.found]\n",
    "print(f\"\\n搜索结果（共 {len(valid_results)} 篇论文）：\")\n",
    "for result in results:\n",
    "\tprint(f\"\\n📖 论文: {result.paper_name}\")\n",
    "\tprint(f\"   URL: {result.url}\")\n",
    "\tprint(f\"   找到: {'是' if result.found else '否'}\")\n",
    "\n",
    "\tif result.found:\n",
    "\t\tprint(f\"   匹配度: {result.match_score}\")\n",
    "\t\tif result.paper_id:\n",
    "\t\t\tprint(f\"   论文ID: {result.paper_id}\")\n",
    "\t\tif result.year:\n",
    "\t\t\tprint(f\"   年份: {result.year}\")\n",
    "\t\tif result.venue:\n",
    "\t\t\tprint(f\"   会议: {result.venue}\")\n",
    "\t\tif result.authors:\n",
    "\t\t\tprint(\"   作者列表:\")\n",
    "\t\t\tfor author in result.authors:\n",
    "\t\t\t\tauthor_info = f\"{author.name}\"\n",
    "\t\t\t\tif author.author_id:\n",
    "\t\t\t\t\tauthor_info += f\" (ID: {author.author_id})\"\n",
    "\t\t\t\tprint(f\"     • {author_info}\")\n",
    "\telse:\n",
    "\t\tprint(\"   原因: 论文未找到或匹配度不足\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PaperAuthorsResult(url='https://iclr.cc/virtual/2025/poster/28473', paper_name='GOFA: A Generative One-For-All Model for Joint Graph Language Modeling', paper_id='055af7789c29452bf808a48f74ec25e01049425e', match_score=248.019, year=2024, venue='International Conference on Learning Representations', paper_url='https://www.semanticscholar.org/paper/055af7789c29452bf808a48f74ec25e01049425e', authors=[AuthorWithId(name='Lecheng Kong', author_id='2164063663'), AuthorWithId(name='Jiarui Feng', author_id='2239091724'), AuthorWithId(name='Hao Liu', author_id='2264134998'), AuthorWithId(name='Chengsong Huang', author_id='31937655'), AuthorWithId(name='Jiaxin Huang', author_id='2300246276'), AuthorWithId(name='Yixin Chen', author_id='2266810999'), AuthorWithId(name='Muhan Zhang', author_id='2239188141')], found=True),\n",
       " PaperAuthorsResult(url='https://dl.acm.org/doi/10.1145/3711896.3736568', paper_name='Boosting Graph Foundation Model from Structural Perspective', paper_id='1051287627ca209b0fdc7899d6abf366f076cb59', match_score=212.0903, year=2024, venue='arXiv.org', paper_url='https://www.semanticscholar.org/paper/1051287627ca209b0fdc7899d6abf366f076cb59', authors=[AuthorWithId(name='Yao Cheng', author_id='2165643958'), AuthorWithId(name='Yige Zhao', author_id='2266749154'), AuthorWithId(name='Jianxiang Yu', author_id='2198507600'), AuthorWithId(name='Xiang Li', author_id='2258789569')], found=True),\n",
       " PaperAuthorsResult(url='https://proceedings.iclr.cc/paper_files/paper/2025/file/652c104b5b0652a03684efeaf805463b-Paper-Conference.pdf', paper_name='GOFA: A GENERATIVE ONE-FOR-ALL MODEL FOR JOINT GRAPH LANGUAGE MODELING', paper_id='055af7789c29452bf808a48f74ec25e01049425e', match_score=250.30241, year=2024, venue='International Conference on Learning Representations', paper_url='https://www.semanticscholar.org/paper/055af7789c29452bf808a48f74ec25e01049425e', authors=[AuthorWithId(name='Lecheng Kong', author_id='2164063663'), AuthorWithId(name='Jiarui Feng', author_id='2239091724'), AuthorWithId(name='Hao Liu', author_id='2264134998'), AuthorWithId(name='Chengsong Huang', author_id='31937655'), AuthorWithId(name='Jiaxin Huang', author_id='2300246276'), AuthorWithId(name='Yixin Chen', author_id='2266810999'), AuthorWithId(name='Muhan Zhang', author_id='2239188141')], found=True),\n",
       " PaperAuthorsResult(url='https://arxiv.org/html/2508.20906v1', paper_name='Turning Tabular Foundation Models into Graph Foundation Models', paper_id='03ee0f2482b544ef9705f31edeac5cf1c0197304', match_score=259.16794, year=2025, venue='', paper_url='https://www.semanticscholar.org/paper/03ee0f2482b544ef9705f31edeac5cf1c0197304', authors=[AuthorWithId(name='Dmitry Eremeev', author_id='2377795413'), AuthorWithId(name='Gleb Bazhenov', author_id='2172208866'), AuthorWithId(name='Oleg Platonov', author_id='2184723723'), AuthorWithId(name='Artem Babenko', author_id='2377796287'), AuthorWithId(name='Liudmila Prokhorenkova', author_id='2323374652')], found=True),\n",
       " PaperAuthorsResult(url='https://icml.cc/virtual/2025/poster/44147', paper_name='How Expressive are Knowledge Graph Foundation Models?', paper_id='9023a11bf44bf6f14c69d3b61ef3a7607ac1bbde', match_score=196.40707, year=2025, venue='arXiv.org', paper_url='https://www.semanticscholar.org/paper/9023a11bf44bf6f14c69d3b61ef3a7607ac1bbde', authors=[AuthorWithId(name='Xingyue Huang', author_id='2152662727'), AuthorWithId(name=\"Pablo Barcel'o\", author_id='2212526363'), AuthorWithId(name='Michael M. Bronstein', author_id='2253393509'), AuthorWithId(name='I. Ceylan', author_id='49633004'), AuthorWithId(name='Mikhail Galkin', author_id='2066369448'), AuthorWithId(name='Juan L. Reutter', author_id='2256998888'), AuthorWithId(name='Miguel Romero', author_id='2256208484')], found=True),\n",
       " PaperAuthorsResult(url='https://arxiv.org/pdf/2403.16137', paper_name='A Survey on Self-Supervised Graph Foundation Models: Knowledge-Based Perspective', paper_id='f67f718288c545164704a3e253e165be5a6da59f', match_score=251.504, year=2024, venue='IEEE Transactions on Knowledge and Data Engineering', paper_url='https://www.semanticscholar.org/paper/f67f718288c545164704a3e253e165be5a6da59f', authors=[AuthorWithId(name='Zi-qiang Zhao', author_id='2209282615'), AuthorWithId(name='Yixin Su', author_id='2313910549'), AuthorWithId(name='Yuhua Li', author_id='2260170121'), AuthorWithId(name='Yixiong Zou', author_id='2275592358'), AuthorWithId(name='Ruixuan Li', author_id='2283487404'), AuthorWithId(name='Rui Zhang', author_id='2293586635')], found=True),\n",
       " PaperAuthorsResult(url='https://neurips.cc/virtual/2024/poster/94900', paper_name='A Prompt-Based Knowledge Graph Foundation Model for Universal In-Context Reasoning', paper_id='c07151eb48e3923193ac7c50001c5bc240025b5f', match_score=242.06308, year=2024, venue='Neural Information Processing Systems', paper_url='https://www.semanticscholar.org/paper/c07151eb48e3923193ac7c50001c5bc240025b5f', authors=[AuthorWithId(name='Yuanning Cui', author_id='1410917541'), AuthorWithId(name='Zequn Sun', author_id='2109745316'), AuthorWithId(name='Wei Hu', author_id='2311649985')], found=True),\n",
       " PaperAuthorsResult(url='https://aclanthology.org/2024.findings-emnlp.132.pdf', paper_name='OpenGraph: Towards Open Graph Foundation Models', paper_id='feca496290013a250adb2c72ecd20c8d5fd30f24', match_score=214.75404, year=2024, venue='Conference on Empirical Methods in Natural Language Processing', paper_url='https://www.semanticscholar.org/paper/feca496290013a250adb2c72ecd20c8d5fd30f24', authors=[AuthorWithId(name='Lianghao Xia', author_id='1830455155'), AuthorWithId(name='Ben Kao', author_id='2289844482'), AuthorWithId(name='Chao Huang', author_id='2261248317')], found=True),\n",
       " PaperAuthorsResult(url='https://www.researchgate.net/publication/383267044_AnyGraph_Graph_Foundation_Model_in_the_Wild', paper_name='AnyGraph: Graph Foundation Model in the Wild', paper_id='5aa4dc0f24343745175411ca47c8207b41618707', match_score=173.78833, year=2024, venue='arXiv.org', paper_url='https://www.semanticscholar.org/paper/5aa4dc0f24343745175411ca47c8207b41618707', authors=[AuthorWithId(name='Lianghao Xia', author_id='1830455155'), AuthorWithId(name='Chao Huang', author_id='2261248317')], found=True),\n",
       " PaperAuthorsResult(url='https://icml.cc/virtual/2025/poster/46113', paper_name='Towards Graph Foundation Models: Learning Generalities Across Graphs via Task-Trees', paper_id='6a1e0241d29484eece376ecedf2a812ca0e1e46c', match_score=339.0381, year=2024, venue='', paper_url='https://www.semanticscholar.org/paper/6a1e0241d29484eece376ecedf2a812ca0e1e46c', authors=[AuthorWithId(name='Zehong Wang', author_id='2284578810'), AuthorWithId(name='Zheyuan Zhang', author_id='2284184904'), AuthorWithId(name='Tianyi Ma', author_id='2283516218'), AuthorWithId(name='Nitesh V. Chawla', author_id='2292582566'), AuthorWithId(name='Chuxu Zhang', author_id='2117879943'), AuthorWithId(name='Yanfang Ye', author_id='2093920413')], found=True),\n",
       " PaperAuthorsResult(url='https://arxiv.org/html/2310.11829v4', paper_name='Graph Foundation Models: Concepts, Opportunities and Challenges', paper_id='6912a12f3710bf4bbdfd9f55f90311426fc1c32c', match_score=192.86075, year=2023, venue='IEEE Transactions on Pattern Analysis and Machine Intelligence', paper_url='https://www.semanticscholar.org/paper/6912a12f3710bf4bbdfd9f55f90311426fc1c32c', authors=[AuthorWithId(name='Jiawei Liu', author_id='2260178816'), AuthorWithId(name='Cheng Yang', author_id='2257052319'), AuthorWithId(name='Zhiyuan Lu', author_id='2110327382'), AuthorWithId(name='Junze Chen', author_id='2260645232'), AuthorWithId(name='Yibo Li', author_id='2274190647'), AuthorWithId(name='Mengmei Zhang', author_id='16003017'), AuthorWithId(name='Ting Bai', author_id='2300370790'), AuthorWithId(name='Yuan Fang', author_id='2267220071'), AuthorWithId(name='Lichao Sun', author_id='2257354508'), AuthorWithId(name='Philip S. Yu', author_id='2258679535'), AuthorWithId(name='Chuan Shi', author_id='2257131498')], found=True),\n",
       " PaperAuthorsResult(url='https://www.yfang.site/publications', paper_name='SAMGPT: Text-free Graph Foundation Model for Multi-domain Pre-training and Cross-domain Adaptation', paper_id='b0f3a565dd644cdb135d68ec8c1510a08afb4482', match_score=313.2812, year=2025, venue='The Web Conference', paper_url='https://www.semanticscholar.org/paper/b0f3a565dd644cdb135d68ec8c1510a08afb4482', authors=[AuthorWithId(name='Xingtong Yu', author_id='2204706384'), AuthorWithId(name='Zechuan Gong', author_id='2344801250'), AuthorWithId(name='Chang Zhou', author_id='2271793882'), AuthorWithId(name='Yuan Fang', author_id='2307591774'), AuthorWithId(name='Hui Zhang', author_id='2344826761')], found=True),\n",
       " PaperAuthorsResult(url='https://link.springer.com/article/10.1007/s11227-025-07029-9', paper_name='Scalable training of trustworthy and energy-efficient predictive graph foundation models for atomistic materials modeling: a case study with HydraGNN', paper_id='454e2d2c6799ff02e80fb41241f597e28d95bf11', match_score=354.61664, year=2024, venue='Journal of Supercomputing', paper_url='https://www.semanticscholar.org/paper/454e2d2c6799ff02e80fb41241f597e28d95bf11', authors=[AuthorWithId(name='Massimiliano Lupo Pasini', author_id='48460197'), AuthorWithId(name='Jong Youl Choi', author_id='2155367341'), AuthorWithId(name='Kshitij Mehta', author_id='2266401208'), AuthorWithId(name='Pei Zhang', author_id='2266463752'), AuthorWithId(name='David Rogers', author_id='2307466716'), AuthorWithId(name='Jonghyun Bae', author_id='2266401334'), AuthorWithId(name='Khaled Ibrahim', author_id='2266400332'), AuthorWithId(name='Ashwin M. Aji', author_id='2256984159'), AuthorWithId(name='Karl W. Schulz', author_id='2256977081'), AuthorWithId(name='Jorda Polo', author_id='2307465603'), AuthorWithId(name='Prasanna Balaprakash', author_id='2138151793')], found=True),\n",
       " PaperAuthorsResult(url='https://yixinliu233.github.io/', paper_name='Unifying Unsupervised Graph-Level Anomaly Detection and Out-of-Distribution Detection: A Benchmark', paper_id='d212c555293b81f845b3c99af4e922b0fcdb4290', match_score=273.07202, year=2024, venue='International Conference on Learning Representations', paper_url='https://www.semanticscholar.org/paper/d212c555293b81f845b3c99af4e922b0fcdb4290', authors=[AuthorWithId(name='Yili Wang', author_id='2298205559'), AuthorWithId(name='Yixin Liu', author_id='2242962967'), AuthorWithId(name='Xu Shen', author_id='2298108098'), AuthorWithId(name='Chenyu Li', author_id='2308068445'), AuthorWithId(name='Kaize Ding', author_id='2261590627'), AuthorWithId(name='Rui Miao', author_id='2171753488'), AuthorWithId(name='Ying Wang', author_id='2303364512'), AuthorWithId(name='Shirui Pan', author_id='2298272389'), AuthorWithId(name='Xin Wang', author_id='2298094103')], found=True),\n",
       " PaperAuthorsResult(url='https://link.springer.com/content/pdf/10.1007/978-981-96-8197-6_13.pdf?pdf=inline%20link', paper_name='Investigating the Limits of Graph Foundation Model in Real-World Travel Recommendation Systems', paper_id='567be24600ad3c075c431641ebddb5a0adfab0b5', match_score=245.65686, year=2025, venue='Pacific-Asia Conference on Knowledge Discovery and Data Mining', paper_url='https://www.semanticscholar.org/paper/567be24600ad3c075c431641ebddb5a0adfab0b5', authors=[AuthorWithId(name='Nayoung Lee', author_id='2370619074'), AuthorWithId(name='Gunmin Lee', author_id='2370075162'), AuthorWithId(name='Donghun Lee', author_id='2370066402')], found=True),\n",
       " PaperAuthorsResult(url='https://lechengkong.github.io/', paper_name='One for All: Towards Training One Graph Model for All Classification Tasks', paper_id='ab22d54dd13876d25c6c8f46c40fb9ac41c61ec5', match_score=273.39832, year=2023, venue='International Conference on Learning Representations', paper_url='https://www.semanticscholar.org/paper/ab22d54dd13876d25c6c8f46c40fb9ac41c61ec5', authors=[AuthorWithId(name='Hao Liu', author_id='2264134998'), AuthorWithId(name='Jiarui Feng', author_id='48441034'), AuthorWithId(name='Lecheng Kong', author_id='2164063663'), AuthorWithId(name='Ningyue Liang', author_id='2270817857'), AuthorWithId(name='Dacheng Tao', author_id='2244621611'), AuthorWithId(name='Yixin Chen', author_id='2223152252'), AuthorWithId(name='Muhan Zhang', author_id='2239188141')], found=True)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install objprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \".type\": \"PaperAuthorsResult\",\n",
      "  \"url\": \"https://link.springer.com/article/10.1007/s11227-025-07029-9\",\n",
      "  \"paper_name\": \"Scalable training of trustworthy and energy-efficient predictive graph foundation models for atomistic materials modeling: a case study with HydraGNN\",\n",
      "  \"paper_id\": \"454e2d2c6799ff02e80fb41241f597e28d95bf11\",\n",
      "  \"match_score\": 354.61664,\n",
      "  \"year\": 2024,\n",
      "  \"venue\": \"Journal of Supercomputing\",\n",
      "  \"paper_url\": \"https://www.semanticscholar.org/paper/454e2d2c6799ff02e80fb41241f597e28d95bf11\",\n",
      "  \"authors\": [\n",
      "    {\n",
      "      \".type\": \"AuthorWithId\",\n",
      "      \"name\": \"Massimiliano Lupo Pasini\",\n",
      "      \"author_id\": \"48460197\"\n",
      "    },\n",
      "    {\n",
      "      \".type\": \"AuthorWithId\",\n",
      "      \"name\": \"Jong Youl Choi\",\n",
      "      \"author_id\": \"2155367341\"\n",
      "    },\n",
      "    {\n",
      "      \".type\": \"AuthorWithId\",\n",
      "      \"name\": \"Kshitij Mehta\",\n",
      "      \"author_id\": \"2266401208\"\n",
      "    },\n",
      "    {\n",
      "      \".type\": \"AuthorWithId\",\n",
      "      \"name\": \"Pei Zhang\",\n",
      "      \"author_id\": \"2266463752\"\n",
      "    },\n",
      "    {\n",
      "      \".type\": \"AuthorWithId\",\n",
      "      \"name\": \"David Rogers\",\n",
      "      \"author_id\": \"2307466716\"\n",
      "    },\n",
      "    {\n",
      "      \".type\": \"AuthorWithId\",\n",
      "      \"name\": \"Jonghyun Bae\",\n",
      "      \"author_id\": \"2266401334\"\n",
      "    },\n",
      "    {\n",
      "      \".type\": \"AuthorWithId\",\n",
      "      \"name\": \"Khaled Ibrahim\",\n",
      "      \"author_id\": \"2266400332\"\n",
      "    },\n",
      "    {\n",
      "      \".type\": \"AuthorWithId\",\n",
      "      \"name\": \"Ashwin M. Aji\",\n",
      "      \"author_id\": \"2256984159\"\n",
      "    },\n",
      "    {\n",
      "      \".type\": \"AuthorWithId\",\n",
      "      \"name\": \"Karl W. Schulz\",\n",
      "      \"author_id\": \"2256977081\"\n",
      "    },\n",
      "    {\n",
      "      \".type\": \"AuthorWithId\",\n",
      "      \"name\": \"Jorda Polo\",\n",
      "      \"author_id\": \"2307465603\"\n",
      "    },\n",
      "    {\n",
      "      \".type\": \"AuthorWithId\",\n",
      "      \"name\": \"Prasanna Balaprakash\",\n",
      "      \"author_id\": \"2138151793\"\n",
      "    }\n",
      "  ],\n",
      "  \"found\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from objprint import objjson\n",
    "import json\n",
    "json_obj = objjson(results[12])\n",
    "\n",
    "print(json.dumps(json_obj, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PaperAuthorsResult(url='https://iclr.cc/virtual/2025/poster/28473', paper_name='GOFA: A Generative One-For-All Model for Joint Graph Language Modeling', paper_id='055af7789c29452bf808a48f74ec25e01049425e', match_score=248.019, year=2024, venue='International Conference on Learning Representations', paper_url='https://www.semanticscholar.org/paper/055af7789c29452bf808a48f74ec25e01049425e', authors=[AuthorWithId(name='Lecheng Kong', author_id='2164063663'), AuthorWithId(name='Jiarui Feng', author_id='2239091724'), AuthorWithId(name='Hao Liu', author_id='2264134998'), AuthorWithId(name='Chengsong Huang', author_id='31937655'), AuthorWithId(name='Jiaxin Huang', author_id='2300246276'), AuthorWithId(name='Yixin Chen', author_id='2266810999'), AuthorWithId(name='Muhan Zhang', author_id='2239188141')], found=True),\n",
       " PaperAuthorsResult(url='https://dl.acm.org/doi/10.1145/3711896.3736568', paper_name='Boosting Graph Foundation Model from Structural Perspective', paper_id='1051287627ca209b0fdc7899d6abf366f076cb59', match_score=212.0903, year=2024, venue='arXiv.org', paper_url='https://www.semanticscholar.org/paper/1051287627ca209b0fdc7899d6abf366f076cb59', authors=[AuthorWithId(name='Yao Cheng', author_id='2165643958'), AuthorWithId(name='Yige Zhao', author_id='2266749154'), AuthorWithId(name='Jianxiang Yu', author_id='2198507600'), AuthorWithId(name='Xiang Li', author_id='2258789569')], found=True),\n",
       " PaperAuthorsResult(url='https://proceedings.iclr.cc/paper_files/paper/2025/file/652c104b5b0652a03684efeaf805463b-Paper-Conference.pdf', paper_name='GOFA: A GENERATIVE ONE-FOR-ALL MODEL FOR JOINT GRAPH LANGUAGE MODELING', paper_id='055af7789c29452bf808a48f74ec25e01049425e', match_score=250.30241, year=2024, venue='International Conference on Learning Representations', paper_url='https://www.semanticscholar.org/paper/055af7789c29452bf808a48f74ec25e01049425e', authors=[AuthorWithId(name='Lecheng Kong', author_id='2164063663'), AuthorWithId(name='Jiarui Feng', author_id='2239091724'), AuthorWithId(name='Hao Liu', author_id='2264134998'), AuthorWithId(name='Chengsong Huang', author_id='31937655'), AuthorWithId(name='Jiaxin Huang', author_id='2300246276'), AuthorWithId(name='Yixin Chen', author_id='2266810999'), AuthorWithId(name='Muhan Zhang', author_id='2239188141')], found=True),\n",
       " PaperAuthorsResult(url='https://arxiv.org/html/2508.20906v1', paper_name='Turning Tabular Foundation Models into Graph Foundation Models', paper_id='03ee0f2482b544ef9705f31edeac5cf1c0197304', match_score=259.16794, year=2025, venue='', paper_url='https://www.semanticscholar.org/paper/03ee0f2482b544ef9705f31edeac5cf1c0197304', authors=[AuthorWithId(name='Dmitry Eremeev', author_id='2377795413'), AuthorWithId(name='Gleb Bazhenov', author_id='2172208866'), AuthorWithId(name='Oleg Platonov', author_id='2184723723'), AuthorWithId(name='Artem Babenko', author_id='2377796287'), AuthorWithId(name='Liudmila Prokhorenkova', author_id='2323374652')], found=True),\n",
       " PaperAuthorsResult(url='https://icml.cc/virtual/2025/poster/44147', paper_name='How Expressive are Knowledge Graph Foundation Models?', paper_id='9023a11bf44bf6f14c69d3b61ef3a7607ac1bbde', match_score=196.40707, year=2025, venue='arXiv.org', paper_url='https://www.semanticscholar.org/paper/9023a11bf44bf6f14c69d3b61ef3a7607ac1bbde', authors=[AuthorWithId(name='Xingyue Huang', author_id='2152662727'), AuthorWithId(name=\"Pablo Barcel'o\", author_id='2212526363'), AuthorWithId(name='Michael M. Bronstein', author_id='2253393509'), AuthorWithId(name='I. Ceylan', author_id='49633004'), AuthorWithId(name='Mikhail Galkin', author_id='2066369448'), AuthorWithId(name='Juan L. Reutter', author_id='2256998888'), AuthorWithId(name='Miguel Romero', author_id='2256208484')], found=True),\n",
       " PaperAuthorsResult(url='https://arxiv.org/pdf/2403.16137', paper_name='A Survey on Self-Supervised Graph Foundation Models: Knowledge-Based Perspective', paper_id='f67f718288c545164704a3e253e165be5a6da59f', match_score=251.504, year=2024, venue='IEEE Transactions on Knowledge and Data Engineering', paper_url='https://www.semanticscholar.org/paper/f67f718288c545164704a3e253e165be5a6da59f', authors=[AuthorWithId(name='Zi-qiang Zhao', author_id='2209282615'), AuthorWithId(name='Yixin Su', author_id='2313910549'), AuthorWithId(name='Yuhua Li', author_id='2260170121'), AuthorWithId(name='Yixiong Zou', author_id='2275592358'), AuthorWithId(name='Ruixuan Li', author_id='2283487404'), AuthorWithId(name='Rui Zhang', author_id='2293586635')], found=True),\n",
       " PaperAuthorsResult(url='https://neurips.cc/virtual/2024/poster/94900', paper_name='A Prompt-Based Knowledge Graph Foundation Model for Universal In-Context Reasoning', paper_id='c07151eb48e3923193ac7c50001c5bc240025b5f', match_score=242.06308, year=2024, venue='Neural Information Processing Systems', paper_url='https://www.semanticscholar.org/paper/c07151eb48e3923193ac7c50001c5bc240025b5f', authors=[AuthorWithId(name='Yuanning Cui', author_id='1410917541'), AuthorWithId(name='Zequn Sun', author_id='2109745316'), AuthorWithId(name='Wei Hu', author_id='2311649985')], found=True),\n",
       " PaperAuthorsResult(url='https://aclanthology.org/2024.findings-emnlp.132.pdf', paper_name='OpenGraph: Towards Open Graph Foundation Models', paper_id='feca496290013a250adb2c72ecd20c8d5fd30f24', match_score=214.75404, year=2024, venue='Conference on Empirical Methods in Natural Language Processing', paper_url='https://www.semanticscholar.org/paper/feca496290013a250adb2c72ecd20c8d5fd30f24', authors=[AuthorWithId(name='Lianghao Xia', author_id='1830455155'), AuthorWithId(name='Ben Kao', author_id='2289844482'), AuthorWithId(name='Chao Huang', author_id='2261248317')], found=True),\n",
       " PaperAuthorsResult(url='https://www.researchgate.net/publication/383267044_AnyGraph_Graph_Foundation_Model_in_the_Wild', paper_name='AnyGraph: Graph Foundation Model in the Wild', paper_id='5aa4dc0f24343745175411ca47c8207b41618707', match_score=173.78833, year=2024, venue='arXiv.org', paper_url='https://www.semanticscholar.org/paper/5aa4dc0f24343745175411ca47c8207b41618707', authors=[AuthorWithId(name='Lianghao Xia', author_id='1830455155'), AuthorWithId(name='Chao Huang', author_id='2261248317')], found=True),\n",
       " PaperAuthorsResult(url='https://icml.cc/virtual/2025/poster/46113', paper_name='Towards Graph Foundation Models: Learning Generalities Across Graphs via Task-Trees', paper_id='6a1e0241d29484eece376ecedf2a812ca0e1e46c', match_score=339.0381, year=2024, venue='', paper_url='https://www.semanticscholar.org/paper/6a1e0241d29484eece376ecedf2a812ca0e1e46c', authors=[AuthorWithId(name='Zehong Wang', author_id='2284578810'), AuthorWithId(name='Zheyuan Zhang', author_id='2284184904'), AuthorWithId(name='Tianyi Ma', author_id='2283516218'), AuthorWithId(name='Nitesh V. Chawla', author_id='2292582566'), AuthorWithId(name='Chuxu Zhang', author_id='2117879943'), AuthorWithId(name='Yanfang Ye', author_id='2093920413')], found=True),\n",
       " PaperAuthorsResult(url='https://arxiv.org/html/2310.11829v4', paper_name='Graph Foundation Models: Concepts, Opportunities and Challenges', paper_id='6912a12f3710bf4bbdfd9f55f90311426fc1c32c', match_score=192.86075, year=2023, venue='IEEE Transactions on Pattern Analysis and Machine Intelligence', paper_url='https://www.semanticscholar.org/paper/6912a12f3710bf4bbdfd9f55f90311426fc1c32c', authors=[AuthorWithId(name='Jiawei Liu', author_id='2260178816'), AuthorWithId(name='Cheng Yang', author_id='2257052319'), AuthorWithId(name='Zhiyuan Lu', author_id='2110327382'), AuthorWithId(name='Junze Chen', author_id='2260645232'), AuthorWithId(name='Yibo Li', author_id='2274190647'), AuthorWithId(name='Mengmei Zhang', author_id='16003017'), AuthorWithId(name='Ting Bai', author_id='2300370790'), AuthorWithId(name='Yuan Fang', author_id='2267220071'), AuthorWithId(name='Lichao Sun', author_id='2257354508'), AuthorWithId(name='Philip S. Yu', author_id='2258679535'), AuthorWithId(name='Chuan Shi', author_id='2257131498')], found=True),\n",
       " PaperAuthorsResult(url='https://www.yfang.site/publications', paper_name='SAMGPT: Text-free Graph Foundation Model for Multi-domain Pre-training and Cross-domain Adaptation', paper_id='b0f3a565dd644cdb135d68ec8c1510a08afb4482', match_score=313.2812, year=2025, venue='The Web Conference', paper_url='https://www.semanticscholar.org/paper/b0f3a565dd644cdb135d68ec8c1510a08afb4482', authors=[AuthorWithId(name='Xingtong Yu', author_id='2204706384'), AuthorWithId(name='Zechuan Gong', author_id='2344801250'), AuthorWithId(name='Chang Zhou', author_id='2271793882'), AuthorWithId(name='Yuan Fang', author_id='2307591774'), AuthorWithId(name='Hui Zhang', author_id='2344826761')], found=True),\n",
       " PaperAuthorsResult(url='https://link.springer.com/article/10.1007/s11227-025-07029-9', paper_name='Scalable training of trustworthy and energy-efficient predictive graph foundation models for atomistic materials modeling: a case study with HydraGNN', paper_id='454e2d2c6799ff02e80fb41241f597e28d95bf11', match_score=354.61664, year=2024, venue='Journal of Supercomputing', paper_url='https://www.semanticscholar.org/paper/454e2d2c6799ff02e80fb41241f597e28d95bf11', authors=[AuthorWithId(name='Massimiliano Lupo Pasini', author_id='48460197'), AuthorWithId(name='Jong Youl Choi', author_id='2155367341'), AuthorWithId(name='Kshitij Mehta', author_id='2266401208'), AuthorWithId(name='Pei Zhang', author_id='2266463752'), AuthorWithId(name='David Rogers', author_id='2307466716'), AuthorWithId(name='Jonghyun Bae', author_id='2266401334'), AuthorWithId(name='Khaled Ibrahim', author_id='2266400332'), AuthorWithId(name='Ashwin M. Aji', author_id='2256984159'), AuthorWithId(name='Karl W. Schulz', author_id='2256977081'), AuthorWithId(name='Jorda Polo', author_id='2307465603'), AuthorWithId(name='Prasanna Balaprakash', author_id='2138151793')], found=True),\n",
       " PaperAuthorsResult(url='https://yixinliu233.github.io/', paper_name='Unifying Unsupervised Graph-Level Anomaly Detection and Out-of-Distribution Detection: A Benchmark', paper_id='d212c555293b81f845b3c99af4e922b0fcdb4290', match_score=273.07202, year=2024, venue='International Conference on Learning Representations', paper_url='https://www.semanticscholar.org/paper/d212c555293b81f845b3c99af4e922b0fcdb4290', authors=[AuthorWithId(name='Yili Wang', author_id='2298205559'), AuthorWithId(name='Yixin Liu', author_id='2242962967'), AuthorWithId(name='Xu Shen', author_id='2298108098'), AuthorWithId(name='Chenyu Li', author_id='2308068445'), AuthorWithId(name='Kaize Ding', author_id='2261590627'), AuthorWithId(name='Rui Miao', author_id='2171753488'), AuthorWithId(name='Ying Wang', author_id='2303364512'), AuthorWithId(name='Shirui Pan', author_id='2298272389'), AuthorWithId(name='Xin Wang', author_id='2298094103')], found=True),\n",
       " PaperAuthorsResult(url='https://link.springer.com/content/pdf/10.1007/978-981-96-8197-6_13.pdf?pdf=inline%20link', paper_name='Investigating the Limits of Graph Foundation Model in Real-World Travel Recommendation Systems', paper_id='567be24600ad3c075c431641ebddb5a0adfab0b5', match_score=245.65686, year=2025, venue='Pacific-Asia Conference on Knowledge Discovery and Data Mining', paper_url='https://www.semanticscholar.org/paper/567be24600ad3c075c431641ebddb5a0adfab0b5', authors=[AuthorWithId(name='Nayoung Lee', author_id='2370619074'), AuthorWithId(name='Gunmin Lee', author_id='2370075162'), AuthorWithId(name='Donghun Lee', author_id='2370066402')], found=True),\n",
       " PaperAuthorsResult(url='https://lechengkong.github.io/', paper_name='One for All: Towards Training One Graph Model for All Classification Tasks', paper_id='ab22d54dd13876d25c6c8f46c40fb9ac41c61ec5', match_score=273.39832, year=2023, venue='International Conference on Learning Representations', paper_url='https://www.semanticscholar.org/paper/ab22d54dd13876d25c6c8f46c40fb9ac41c61ec5', authors=[AuthorWithId(name='Hao Liu', author_id='2264134998'), AuthorWithId(name='Jiarui Feng', author_id='48441034'), AuthorWithId(name='Lecheng Kong', author_id='2164063663'), AuthorWithId(name='Ningyue Liang', author_id='2270817857'), AuthorWithId(name='Dacheng Tao', author_id='2244621611'), AuthorWithId(name='Yixin Chen', author_id='2223152252'), AuthorWithId(name='Muhan Zhang', author_id='2239188141')], found=True)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 拿到一作的各种link和这个其发的paper的list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[S2 Integration] Added 11 papers from Semantic Scholar\n",
      "[Homepage Candidate] Processing: https://lechengkong.github.io/\n",
      "[Homepage Identity] True (conf: 0.98, reason: Identity verified: The homepage explicitly states the name 'Lecheng Kong (孔乐成)', which matches the target author exactly. The research focus is on Graph Neural Networks (GNNs), Graph Foundation Models (GFM), and their intersection with Large Language Models, which aligns with the known paper 'GOFA: A Generative One-For-All Model for Joint Graph Language Modeling'. The homepage also mentions that the work was accepted by ICLR 2025, confirming publication overlap. The affiliation with Washington University in St. Louis and the timeline of being a fifth-year Ph.D. candidate are consistent with the expected background. There is no conflicting information or ambiguity about the identity.)\n",
      "[Homepage] Identity verified, starting comprehensive fetch\n",
      "[Homepage Fetcher] Starting comprehensive fetch for: https://lechengkong.github.io/\n",
      "[Homepage Fetcher] Extracted title: Lecheng Kong (孔乐成)\n",
      "[Platform Found] linkedin: https://www.linkedin.com/in/lecheng-kong-12a045152\n",
      "[Platform Found] dblp: https://dblp.org/pid/319/5576.html\n",
      "[Platform Found] github: https://github.com/LechengKong\n",
      "[Platform Found] scholar: https://scholar.google.com/citations?user=yk3-_EgAAAAJ\n",
      "[Platform Found] orcid: https://orcid.org/0000-0001-9427-8799\n",
      "[Homepage Fetcher] Found 5 social platforms\n",
      "[Homepage Fetcher] Found 1 email addresses\n",
      "[Homepage Fetcher] Summary:\n",
      "  - Title: Lecheng Kong (孔乐成)\n",
      "  - Social platforms: ['linkedin', 'dblp', 'github', 'scholar', 'orcid']\n",
      "  - Emails: ['jerry.kong@wustl.edu']\n",
      "  - Total links found: 5\n",
      "[Homepage Integration] Adding 5 social links and 1 emails\n",
      "[Homepage Direct] Added linkedin: https://www.linkedin.com/in/lecheng-kong-12a045152\n",
      "[Homepage Direct] Added dblp: https://dblp.org/pid/319/5576.html\n",
      "[Homepage Direct] Added github: https://github.com/LechengKong\n",
      "[Homepage Direct] Added scholar: https://scholar.google.com/citations?user=yk3-_EgAAAAJ\n",
      "[Homepage Direct] Added orcid: https://orcid.org/0000-0001-9427-8799\n",
      "[Homepage Direct] Added email: jerry.kong@wustl.edu\n",
      "[Homepage Success] Successfully processed homepage: https://lechengkong.github.io/\n",
      "[Candidate Success] Processed 1 candidates so far\n",
      "[Regular Candidate] Fetched 226 characters from https://scholar.google.com/citations?user=yk3-_EgAAAAJ&hl=en\n",
      "[Skipped Platform] scholar already protected by homepage\n",
      "[Candidate Success] Processed 2 candidates so far\n",
      "[Regular Candidate] Fetched 238 characters from https://scholar.google.com/citations?user=yk3-_EgAAAAJ&hl=zh-CN\n",
      "[Skipped Platform] scholar already protected by homepage\n",
      "[Candidate Success] Processed 3 candidates so far\n",
      "== PROFILE ==\n",
      "AuthorProfile(name='Lecheng Kong', aliases=[], platforms={'linkedin': 'https://www.linkedin.com/in/lecheng-kong-12a045152', 'dblp': 'https://dblp.org/pid/319/5576.html', 'github': 'https://github.com/LechengKong', 'scholar': 'https://scholar.google.com/citations?user=yk3-_EgAAAAJ', 'orcid': 'https://orcid.org/0000-0001-9427-8799'}, ids={'scholar': 'yk3-_EgAAAAJ'}, homepage_url='https://lechengkong.github.io/', affiliation_current='Washington University in St. Louis', emails=['jerry.kong@wustl.edu', 'Verified email at wustl.edu'], interests=['Graph Neural Networks (GNNs)', 'Graph Foundation Models (GFM)', 'Graph Models and Large Language Models', 'graph-based retrieval augmented generation', 'universal graph foundation model', 'graph learning', 'machine learning', 'graph language modeling'], selected_publications=[{'title': 'GOFA: A Generative One-For-All Model for Joint Graph Language Modeling', 'year': 2024, 'venue': 'International Conference on Learning Representations', 'url': 'https://www.semanticscholar.org/paper/055af7789c29452bf808a48f74ec25e01049425e', 'citations': 15}, {'title': 'TAGLAS: An atlas of text-attributed graph datasets in the era of large graph and language models', 'year': 2024, 'venue': 'arXiv.org', 'url': 'https://www.semanticscholar.org/paper/589d8c5afc05002326fcde59a278593f5472e997', 'citations': 3}, {'title': 'Extending the Design Space of Graph Neural Networks by Rethinking Folklore Weisfeiler-Lehman', 'year': 2023, 'venue': 'Neural Information Processing Systems', 'url': 'https://www.semanticscholar.org/paper/2c2a0534d7da4f7a5d6e009ae73fe352f4ebfe22', 'citations': 11}, {'title': 'MAG-GNN: Reinforcement Learning Boosted Graph Neural Network', 'year': 2023, 'venue': 'Neural Information Processing Systems', 'url': 'https://www.semanticscholar.org/paper/60953d9393a42fd6f84de80ffe8527dc10a7c857', 'citations': 14}, {'title': 'Towards Arbitrarily Expressive GNNs in O(n2) Space by Rethinking Folklore Weisfeiler-Lehman', 'year': 2023, 'venue': 'arXiv.org', 'url': 'https://www.semanticscholar.org/paper/9587f39c5a3aac49908fe94664661c7458798242', 'citations': 5}, {'title': 'One for All: Towards Training One Graph Model for All Classification Tasks', 'year': 2023, 'venue': 'International Conference on Learning Representations', 'url': 'https://www.semanticscholar.org/paper/ab22d54dd13876d25c6c8f46c40fb9ac41c61ec5', 'citations': 153}, {'title': 'Time Associated Meta Learning for Clinical Prediction', 'year': 2023, 'venue': 'arXiv.org', 'url': 'https://www.semanticscholar.org/paper/b83fd54b6ddc578307aed524ff89bf3e20bcb628', 'citations': 0}, {'title': 'A Multi-View Joint Learning Framework for Embedding Clinical Codes and Text Using Graph Neural Networks', 'year': 2023, 'venue': 'arXiv.org', 'url': 'https://www.semanticscholar.org/paper/cde6b633703eda8c3fbd7f76a967e07b19b7623d', 'citations': 1}, {'title': 'Graph Contrastive Learning Meets Graph Meta Learning: A Unified Method for Few-shot Node Tasks', 'year': 2023, 'venue': 'The Web Conference', 'url': 'https://www.semanticscholar.org/paper/d11a500d3f0dec27f0113d0f9a0893542d55ea0c', 'citations': 7}, {'title': 'Manipulating Elections by Changing Voter Perceptions', 'year': 2022, 'venue': 'International Joint Conference on Artificial Intelligence', 'url': 'https://www.semanticscholar.org/paper/4548d5611188d63219f5d39d6ab01181a1513e5e', 'citations': 5}], confidence=0.6, notable_achievements=['2022/2023 NeurIPS travel award', '2024.01: 🎉🎉 COLA accepted by WWW2024! Congratulations to Hao!', '2024.01: 🎉🎉 OneForAll paper accepted as Spotlight (5%) by ICLR2024. Thanks everyone for the great teamwork!', '2025.01: 🔥🔥 GOFA was accepted by ICLR2025!', '2024.08: 🔥🔥 GOFA released, we propose a generative approach to solve a wide variety of tasks in the graph domain. GOFA can generate free-form graph-related response to arbitrary human input.', '2024.08: 🔥🔥 TAGLAS released, TAGLAS aim to provide a graph dataset with high diversity from synthetic to real-world data. Check it out if you are building graph foundation models.'], social_impact=None, career_stage='student', overall_score=65.0)\n",
      "[S2 Integration] Added 16 papers from Semantic Scholar\n",
      "[Regular Candidate] Fetched 235 characters from https://scholar.google.com/citations?user=0BzLJqkAAAAJ&hl=en\n",
      "[Candidate Success] Processed 1 candidates so far\n",
      "[Regular Candidate] Fetched 227 characters from https://scholar.google.com/citations?user=0BzLJqkAAAAJ&hl=fr\n",
      "[Candidate Success] Processed 2 candidates so far\n",
      "[Regular Candidate] Fetched 232 characters from https://scholar.google.com/citations?user=0BzLJqkAAAAJ&hl=zh-CN\n",
      "[Candidate Success] Processed 3 candidates so far\n",
      "[Regular Candidate] Fetched 225 characters from https://scholar.google.com/citations?user=1d5qsg4AAAAJ&hl=en\n",
      "[Candidate Success] Processed 4 candidates so far\n",
      "== PROFILE ==\n",
      "AuthorProfile(name='Yao Cheng', aliases=[], platforms={'scholar': 'https://scholar.google.com/citations?user=0BzLJqkAAAAJ&hl=en'}, ids={'scholar': '0BzLJqkAAAAJ'}, homepage_url=None, affiliation_current='East China Normal University', emails=['Verified email at stu.ecnu.edu', 'Adresse e-mail validée', 'stu.ecnu.edu.cn 的电子邮件经过 ...', 'Verified email at sinica.edu.tw'], interests=['graph foundation model', 'structural perspective'], selected_publications=[{'title': 'Human Cognition Inspired RAG with Knowledge Graph for Complex Problem Solving', 'year': 2025, 'venue': 'arXiv.org', 'url': 'https://www.semanticscholar.org/paper/76e2e4641d3a8730a4b28fdd870f61344f363381', 'citations': 0}, {'title': 'Learning Prioritized Node-Wise Message Propagation in Graph Neural Networks (Extended Abstract)', 'year': 2025, 'venue': 'IEEE International Conference on Data Engineering', 'url': 'https://www.semanticscholar.org/paper/88bd5858f8df82141e5cbaedfe710eed0a8fd661', 'citations': 0}, {'title': 'Boosting Graph Foundation Model from Structural Perspective', 'year': 2024, 'venue': 'arXiv.org', 'url': 'https://www.semanticscholar.org/paper/1051287627ca209b0fdc7899d6abf366f076cb59', 'citations': 5}, {'title': 'SEAGraph: Unveiling the Whole Story of Paper Review Comments', 'year': 2024, 'venue': 'arXiv.org', 'url': 'https://www.semanticscholar.org/paper/28ab78e70472317d37fb421d7d21b73a2021c6f1', 'citations': 1}, {'title': 'Improving Graph Out-of-distribution Generalization Beyond Causality', 'year': 2024, 'venue': '', 'url': 'https://www.semanticscholar.org/paper/3c9208088d575f04aec19816a78ab46f828285a3', 'citations': 0}, {'title': 'Self-pro: A Self-prompt and Tuning Framework for Graph Neural Networks', 'year': 2024, 'venue': 'ECML/PKDD', 'url': 'https://www.semanticscholar.org/paper/89815ce09cea5bb20b9becccfdde0d193420772a', 'citations': 0}, {'title': 'Towards Learning from Graphs with Heterophily: Progress and Future', 'year': 2024, 'venue': 'arXiv.org', 'url': 'https://www.semanticscholar.org/paper/964e80abcde005e8bfa64337aab0b2dad4e33f8d', 'citations': 2}, {'title': 'A Survey on Learning from Graphs with Heterophily: Recent Advances and Future Directions', 'year': 2024, 'venue': '', 'url': 'https://www.semanticscholar.org/paper/c063d058f0d2151280ff63e045fb613970793460', 'citations': 6}, {'title': 'Can Large Language Models Act as Ensembler for Multi-GNNs?', 'year': 2024, 'venue': 'arXiv.org', 'url': 'https://www.semanticscholar.org/paper/d2113b74e2b4199024d108dcb67066c4b9077f11', 'citations': 0}, {'title': 'Improving Graph Out-of-distribution Generalization on Real-world Data', 'year': 2024, 'venue': 'arXiv.org', 'url': 'https://www.semanticscholar.org/paper/d4a60973e5b1577b7f8de01276d9ec6247b06778', 'citations': 0}], confidence=0.7, notable_achievements=[], social_impact=None, career_stage='student', overall_score=29.0)\n",
      "[S2 Integration] Added 11 papers from Semantic Scholar\n",
      "[Homepage Candidate] Processing: https://lechengkong.github.io/\n",
      "[Homepage Identity] True (conf: 0.98, reason: Identity verified: The homepage explicitly states the name 'Lecheng Kong (孔乐成)', which matches the target author exactly. The research focus includes Graph Neural Networks (GNNs), Graph Foundation Models (GFM), and their intersection with Large Language Models, aligning with the known paper 'GOFA: A Generative One-For-All Model for Joint Graph Language Modeling'. The homepage also mentions that the work was accepted by ICLR2025, confirming publication overlap. The affiliation with Washington University in St. Louis is consistent, and there is no conflicting biographical or timeline information.)\n",
      "[Homepage] Identity verified, starting comprehensive fetch\n",
      "[Homepage Fetcher] Starting comprehensive fetch for: https://lechengkong.github.io/\n",
      "[Homepage Fetcher] Extracted title: Lecheng Kong (孔乐成)\n",
      "[Platform Found] linkedin: https://www.linkedin.com/in/lecheng-kong-12a045152\n",
      "[Platform Found] dblp: https://dblp.org/pid/319/5576.html\n",
      "[Platform Found] github: https://github.com/LechengKong\n",
      "[Platform Found] scholar: https://scholar.google.com/citations?user=yk3-_EgAAAAJ\n",
      "[Platform Found] orcid: https://orcid.org/0000-0001-9427-8799\n",
      "[Homepage Fetcher] Found 5 social platforms\n",
      "[Homepage Fetcher] Found 1 email addresses\n",
      "[Homepage Fetcher] Summary:\n",
      "  - Title: Lecheng Kong (孔乐成)\n",
      "  - Social platforms: ['linkedin', 'dblp', 'github', 'scholar', 'orcid']\n",
      "  - Emails: ['jerry.kong@wustl.edu']\n",
      "  - Total links found: 5\n",
      "[Homepage Integration] Adding 5 social links and 1 emails\n",
      "[Homepage Direct] Added linkedin: https://www.linkedin.com/in/lecheng-kong-12a045152\n",
      "[Homepage Direct] Added dblp: https://dblp.org/pid/319/5576.html\n",
      "[Homepage Direct] Added github: https://github.com/LechengKong\n",
      "[Homepage Direct] Added scholar: https://scholar.google.com/citations?user=yk3-_EgAAAAJ\n",
      "[Homepage Direct] Added orcid: https://orcid.org/0000-0001-9427-8799\n",
      "[Homepage Direct] Added email: jerry.kong@wustl.edu\n",
      "[Homepage Success] Successfully processed homepage: https://lechengkong.github.io/\n",
      "[Candidate Success] Processed 1 candidates so far\n",
      "[Regular Candidate] Fetched 226 characters from https://scholar.google.com/citations?user=yk3-_EgAAAAJ&hl=en\n",
      "[Skipped Platform] scholar already protected by homepage\n",
      "[Candidate Success] Processed 2 candidates so far\n",
      "[Regular Candidate] Fetched 238 characters from https://scholar.google.com/citations?user=yk3-_EgAAAAJ&hl=zh-CN\n",
      "[Skipped Platform] scholar already protected by homepage\n",
      "[Candidate Success] Processed 3 candidates so far\n",
      "== PROFILE ==\n",
      "AuthorProfile(name='Lecheng Kong', aliases=[], platforms={'linkedin': 'https://www.linkedin.com/in/lecheng-kong-12a045152', 'dblp': 'https://dblp.org/pid/319/5576.html', 'github': 'https://github.com/LechengKong', 'scholar': 'https://scholar.google.com/citations?user=yk3-_EgAAAAJ', 'orcid': 'https://orcid.org/0000-0001-9427-8799'}, ids={'scholar': 'yk3-_EgAAAAJ'}, homepage_url='https://lechengkong.github.io/', affiliation_current='Washington University in St. Louis', emails=['jerry.kong@wustl.edu', 'Verified email at wustl.edu'], interests=['Graph Neural Networks (GNNs)', 'Graph Foundation Models (GFM)', 'Graph Models and Large Language Models', 'graph-based retrieval augmented generation', 'universal graph foundation model', 'graph learning', 'machine learning', 'graph language modeling'], selected_publications=[{'title': 'GOFA: A Generative One-For-All Model for Joint Graph Language Modeling', 'year': 2024, 'venue': 'International Conference on Learning Representations', 'url': 'https://www.semanticscholar.org/paper/055af7789c29452bf808a48f74ec25e01049425e', 'citations': 15}, {'title': 'TAGLAS: An atlas of text-attributed graph datasets in the era of large graph and language models', 'year': 2024, 'venue': 'arXiv.org', 'url': 'https://www.semanticscholar.org/paper/589d8c5afc05002326fcde59a278593f5472e997', 'citations': 3}, {'title': 'Extending the Design Space of Graph Neural Networks by Rethinking Folklore Weisfeiler-Lehman', 'year': 2023, 'venue': 'Neural Information Processing Systems', 'url': 'https://www.semanticscholar.org/paper/2c2a0534d7da4f7a5d6e009ae73fe352f4ebfe22', 'citations': 11}, {'title': 'MAG-GNN: Reinforcement Learning Boosted Graph Neural Network', 'year': 2023, 'venue': 'Neural Information Processing Systems', 'url': 'https://www.semanticscholar.org/paper/60953d9393a42fd6f84de80ffe8527dc10a7c857', 'citations': 14}, {'title': 'Towards Arbitrarily Expressive GNNs in O(n2) Space by Rethinking Folklore Weisfeiler-Lehman', 'year': 2023, 'venue': 'arXiv.org', 'url': 'https://www.semanticscholar.org/paper/9587f39c5a3aac49908fe94664661c7458798242', 'citations': 5}, {'title': 'One for All: Towards Training One Graph Model for All Classification Tasks', 'year': 2023, 'venue': 'International Conference on Learning Representations', 'url': 'https://www.semanticscholar.org/paper/ab22d54dd13876d25c6c8f46c40fb9ac41c61ec5', 'citations': 153}, {'title': 'Time Associated Meta Learning for Clinical Prediction', 'year': 2023, 'venue': 'arXiv.org', 'url': 'https://www.semanticscholar.org/paper/b83fd54b6ddc578307aed524ff89bf3e20bcb628', 'citations': 0}, {'title': 'A Multi-View Joint Learning Framework for Embedding Clinical Codes and Text Using Graph Neural Networks', 'year': 2023, 'venue': 'arXiv.org', 'url': 'https://www.semanticscholar.org/paper/cde6b633703eda8c3fbd7f76a967e07b19b7623d', 'citations': 1}, {'title': 'Graph Contrastive Learning Meets Graph Meta Learning: A Unified Method for Few-shot Node Tasks', 'year': 2023, 'venue': 'The Web Conference', 'url': 'https://www.semanticscholar.org/paper/d11a500d3f0dec27f0113d0f9a0893542d55ea0c', 'citations': 7}, {'title': 'Manipulating Elections by Changing Voter Perceptions', 'year': 2022, 'venue': 'International Joint Conference on Artificial Intelligence', 'url': 'https://www.semanticscholar.org/paper/4548d5611188d63219f5d39d6ab01181a1513e5e', 'citations': 5}], confidence=0.6, notable_achievements=['2022/2023 NeurIPS travel award', '2024.01: 🎉🎉 COLA accepted by WWW2024! Congratulations to Hao!', '2024.01: 🎉🎉 OneForAll paper accepted as Spotlight (5%) by ICLR2024. Thanks everyone for the great teamwork!', '2025.01: 🔥🔥 GOFA was accepted by ICLR2025!', '2024.08: 🔥🔥 GOFA released, we propose a generative approach to solve a wide variety of tasks in the graph domain. GOFA can generate free-form graph-related response to arbitrary human input.', '2024.08: 🔥🔥 TAGLAS released, TAGLAS aim to provide a graph dataset with high diversity from synthetic to real-world data. Check it out if you are building graph foundation models.'], social_impact=None, career_stage='student', overall_score=65.0)\n"
     ]
    }
   ],
   "source": [
    "from author_discovery import discover_author_profile, fetch_author_publications_via_s2\n",
    "\n",
    "\n",
    "\n",
    "hit = valid_results[0]\n",
    "paper_title = hit.paper_name\n",
    "first_author = hit.authors[0].name\n",
    "first_author_id = hit.authors[0].author_id\n",
    "profile_0 = discover_author_profile(first_author, paper_title, aliases=[first_author], author_id=first_author_id)\n",
    "print(\"== PROFILE ==\")\n",
    "print(profile_0)\n",
    "\n",
    "hit = valid_results[1]\n",
    "paper_title = hit.paper_name\n",
    "first_author = hit.authors[0].name\n",
    "first_author_id = hit.authors[0].author_id\n",
    "profile_1 = discover_author_profile(first_author, paper_title, aliases=[first_author], author_id=first_author_id)\n",
    "print(\"== PROFILE ==\")\n",
    "print(profile_1)\n",
    "\n",
    "hit = valid_results[2]\n",
    "paper_title = hit.paper_name\n",
    "first_author = hit.authors[0].name\n",
    "first_author_id = hit.authors[0].author_id\n",
    "profile_2 = discover_author_profile(first_author, paper_title, aliases=[first_author], author_id=first_author_id)\n",
    "print(\"== PROFILE ==\")\n",
    "print(profile_2)\n",
    "\n",
    "\n",
    "# hit = valid_results[2]\n",
    "# paper_title = hit.paper_name\n",
    "# first_author = hit.authors[0].name\n",
    "# profile_2 = discover_author_profile(first_author, paper_title, aliases=[first_author])\n",
    "# print(\"== PROFILE ==\")\n",
    "# print(profile_2)\n",
    "\n",
    "\n",
    "# hit = valid_results[3]\n",
    "# paper_title = hit.paper_name\n",
    "# first_author = hit.authors[0].name\n",
    "# first_author_id = hit.authors[0].author_id\n",
    "# profile_3 = discover_author_profile(first_author, paper_title, aliases=[first_author], author_id=first_author_id)\n",
    "# print(\"== PROFILE ==\")\n",
    "# print(profile_3)\n",
    "\n",
    "\n",
    "    # # 更多论文（若已拿到 author_id）\n",
    "    # if hit.authors[0].author_id:\n",
    "    #     pubs = fetch_author_publications_via_s2(hit.authors[0].author_id, k=8)\n",
    "    #     profile.selected_publications = (profile.selected_publications or []) + pubs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== PROFILE 0 ==\n",
      "\n",
      "\n",
      "\n",
      "{\n",
      "  \".type\": \"AuthorProfile\",\n",
      "  \"name\": \"Lecheng Kong\",\n",
      "  \"aliases\": [],\n",
      "  \"platforms\": {\n",
      "    \"linkedin\": \"https://www.linkedin.com/in/lecheng-kong-12a045152\",\n",
      "    \"dblp\": \"https://dblp.org/pid/319/5576.html\",\n",
      "    \"github\": \"https://github.com/LechengKong\",\n",
      "    \"scholar\": \"https://scholar.google.com/citations?user=yk3-_EgAAAAJ\",\n",
      "    \"orcid\": \"https://orcid.org/0000-0001-9427-8799\"\n",
      "  },\n",
      "  \"ids\": {\n",
      "    \"scholar\": \"yk3-_EgAAAAJ\"\n",
      "  },\n",
      "  \"homepage_url\": \"https://lechengkong.github.io/\",\n",
      "  \"affiliation_current\": \"Washington University in St. Louis\",\n",
      "  \"emails\": [\n",
      "    \"jerry.kong@wustl.edu\",\n",
      "    \"Verified email at wustl.edu\"\n",
      "  ],\n",
      "  \"interests\": [\n",
      "    \"Graph Neural Networks (GNNs)\",\n",
      "    \"Graph Foundation Models (GFM)\",\n",
      "    \"Graph Models and Large Language Models\",\n",
      "    \"graph-based retrieval augmented generation\",\n",
      "    \"universal graph foundation model\",\n",
      "    \"graph learning\",\n",
      "    \"machine learning\",\n",
      "    \"graph language modeling\"\n",
      "  ],\n",
      "  \"selected_publications\": [\n",
      "    {\n",
      "      \"title\": \"GOFA: A Generative One-For-All Model for Joint Graph Language Modeling\",\n",
      "      \"year\": 2024,\n",
      "      \"venue\": \"International Conference on Learning Representations\",\n",
      "      \"url\": \"https://www.semanticscholar.org/paper/055af7789c29452bf808a48f74ec25e01049425e\",\n",
      "      \"citations\": 15\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"TAGLAS: An atlas of text-attributed graph datasets in the era of large graph and language models\",\n",
      "      \"year\": 2024,\n",
      "      \"venue\": \"arXiv.org\",\n",
      "      \"url\": \"https://www.semanticscholar.org/paper/589d8c5afc05002326fcde59a278593f5472e997\",\n",
      "      \"citations\": 3\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Extending the Design Space of Graph Neural Networks by Rethinking Folklore Weisfeiler-Lehman\",\n",
      "      \"year\": 2023,\n",
      "      \"venue\": \"Neural Information Processing Systems\",\n",
      "      \"url\": \"https://www.semanticscholar.org/paper/2c2a0534d7da4f7a5d6e009ae73fe352f4ebfe22\",\n",
      "      \"citations\": 11\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"MAG-GNN: Reinforcement Learning Boosted Graph Neural Network\",\n",
      "      \"year\": 2023,\n",
      "      \"venue\": \"Neural Information Processing Systems\",\n",
      "      \"url\": \"https://www.semanticscholar.org/paper/60953d9393a42fd6f84de80ffe8527dc10a7c857\",\n",
      "      \"citations\": 14\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Towards Arbitrarily Expressive GNNs in O(n2) Space by Rethinking Folklore Weisfeiler-Lehman\",\n",
      "      \"year\": 2023,\n",
      "      \"venue\": \"arXiv.org\",\n",
      "      \"url\": \"https://www.semanticscholar.org/paper/9587f39c5a3aac49908fe94664661c7458798242\",\n",
      "      \"citations\": 5\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"One for All: Towards Training One Graph Model for All Classification Tasks\",\n",
      "      \"year\": 2023,\n",
      "      \"venue\": \"International Conference on Learning Representations\",\n",
      "      \"url\": \"https://www.semanticscholar.org/paper/ab22d54dd13876d25c6c8f46c40fb9ac41c61ec5\",\n",
      "      \"citations\": 153\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Time Associated Meta Learning for Clinical Prediction\",\n",
      "      \"year\": 2023,\n",
      "      \"venue\": \"arXiv.org\",\n",
      "      \"url\": \"https://www.semanticscholar.org/paper/b83fd54b6ddc578307aed524ff89bf3e20bcb628\",\n",
      "      \"citations\": 0\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"A Multi-View Joint Learning Framework for Embedding Clinical Codes and Text Using Graph Neural Networks\",\n",
      "      \"year\": 2023,\n",
      "      \"venue\": \"arXiv.org\",\n",
      "      \"url\": \"https://www.semanticscholar.org/paper/cde6b633703eda8c3fbd7f76a967e07b19b7623d\",\n",
      "      \"citations\": 1\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Graph Contrastive Learning Meets Graph Meta Learning: A Unified Method for Few-shot Node Tasks\",\n",
      "      \"year\": 2023,\n",
      "      \"venue\": \"The Web Conference\",\n",
      "      \"url\": \"https://www.semanticscholar.org/paper/d11a500d3f0dec27f0113d0f9a0893542d55ea0c\",\n",
      "      \"citations\": 7\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Manipulating Elections by Changing Voter Perceptions\",\n",
      "      \"year\": 2022,\n",
      "      \"venue\": \"International Joint Conference on Artificial Intelligence\",\n",
      "      \"url\": \"https://www.semanticscholar.org/paper/4548d5611188d63219f5d39d6ab01181a1513e5e\",\n",
      "      \"citations\": 5\n",
      "    }\n",
      "  ],\n",
      "  \"confidence\": 0.6,\n",
      "  \"notable_achievements\": [\n",
      "    \"2022/2023 NeurIPS travel award\",\n",
      "    \"2024.01: \\ud83c\\udf89\\ud83c\\udf89 COLA accepted by WWW2024! Congratulations to Hao!\",\n",
      "    \"2024.01: \\ud83c\\udf89\\ud83c\\udf89 OneForAll paper accepted as Spotlight (5%) by ICLR2024. Thanks everyone for the great teamwork!\",\n",
      "    \"2025.01: \\ud83d\\udd25\\ud83d\\udd25 GOFA was accepted by ICLR2025!\",\n",
      "    \"2024.08: \\ud83d\\udd25\\ud83d\\udd25 GOFA released, we propose a generative approach to solve a wide variety of tasks in the graph domain. GOFA can generate free-form graph-related response to arbitrary human input.\",\n",
      "    \"2024.08: \\ud83d\\udd25\\ud83d\\udd25 TAGLAS released, TAGLAS aim to provide a graph dataset with high diversity from synthetic to real-world data. Check it out if you are building graph foundation models.\"\n",
      "  ],\n",
      "  \"social_impact\": null,\n",
      "  \"career_stage\": \"student\",\n",
      "  \"overall_score\": 65.0\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "== PROFILE 1 ==\n",
      "\n",
      "\n",
      "\n",
      "{\n",
      "  \".type\": \"AuthorProfile\",\n",
      "  \"name\": \"Yao Cheng\",\n",
      "  \"aliases\": [],\n",
      "  \"platforms\": {\n",
      "    \"scholar\": \"https://scholar.google.com/citations?user=0BzLJqkAAAAJ&hl=en\"\n",
      "  },\n",
      "  \"ids\": {\n",
      "    \"scholar\": \"0BzLJqkAAAAJ\"\n",
      "  },\n",
      "  \"homepage_url\": null,\n",
      "  \"affiliation_current\": \"East China Normal University\",\n",
      "  \"emails\": [\n",
      "    \"Verified email at stu.ecnu.edu\",\n",
      "    \"Adresse e-mail valid\\u00e9e\",\n",
      "    \"stu.ecnu.edu.cn \\u7684\\u7535\\u5b50\\u90ae\\u4ef6\\u7ecf\\u8fc7 ...\",\n",
      "    \"Verified email at sinica.edu.tw\"\n",
      "  ],\n",
      "  \"interests\": [\n",
      "    \"graph foundation model\",\n",
      "    \"structural perspective\"\n",
      "  ],\n",
      "  \"selected_publications\": [\n",
      "    {\n",
      "      \"title\": \"Human Cognition Inspired RAG with Knowledge Graph for Complex Problem Solving\",\n",
      "      \"year\": 2025,\n",
      "      \"venue\": \"arXiv.org\",\n",
      "      \"url\": \"https://www.semanticscholar.org/paper/76e2e4641d3a8730a4b28fdd870f61344f363381\",\n",
      "      \"citations\": 0\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Learning Prioritized Node-Wise Message Propagation in Graph Neural Networks (Extended Abstract)\",\n",
      "      \"year\": 2025,\n",
      "      \"venue\": \"IEEE International Conference on Data Engineering\",\n",
      "      \"url\": \"https://www.semanticscholar.org/paper/88bd5858f8df82141e5cbaedfe710eed0a8fd661\",\n",
      "      \"citations\": 0\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Boosting Graph Foundation Model from Structural Perspective\",\n",
      "      \"year\": 2024,\n",
      "      \"venue\": \"arXiv.org\",\n",
      "      \"url\": \"https://www.semanticscholar.org/paper/1051287627ca209b0fdc7899d6abf366f076cb59\",\n",
      "      \"citations\": 5\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"SEAGraph: Unveiling the Whole Story of Paper Review Comments\",\n",
      "      \"year\": 2024,\n",
      "      \"venue\": \"arXiv.org\",\n",
      "      \"url\": \"https://www.semanticscholar.org/paper/28ab78e70472317d37fb421d7d21b73a2021c6f1\",\n",
      "      \"citations\": 1\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Improving Graph Out-of-distribution Generalization Beyond Causality\",\n",
      "      \"year\": 2024,\n",
      "      \"venue\": \"\",\n",
      "      \"url\": \"https://www.semanticscholar.org/paper/3c9208088d575f04aec19816a78ab46f828285a3\",\n",
      "      \"citations\": 0\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Self-pro: A Self-prompt and Tuning Framework for Graph Neural Networks\",\n",
      "      \"year\": 2024,\n",
      "      \"venue\": \"ECML/PKDD\",\n",
      "      \"url\": \"https://www.semanticscholar.org/paper/89815ce09cea5bb20b9becccfdde0d193420772a\",\n",
      "      \"citations\": 0\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Towards Learning from Graphs with Heterophily: Progress and Future\",\n",
      "      \"year\": 2024,\n",
      "      \"venue\": \"arXiv.org\",\n",
      "      \"url\": \"https://www.semanticscholar.org/paper/964e80abcde005e8bfa64337aab0b2dad4e33f8d\",\n",
      "      \"citations\": 2\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"A Survey on Learning from Graphs with Heterophily: Recent Advances and Future Directions\",\n",
      "      \"year\": 2024,\n",
      "      \"venue\": \"\",\n",
      "      \"url\": \"https://www.semanticscholar.org/paper/c063d058f0d2151280ff63e045fb613970793460\",\n",
      "      \"citations\": 6\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Can Large Language Models Act as Ensembler for Multi-GNNs?\",\n",
      "      \"year\": 2024,\n",
      "      \"venue\": \"arXiv.org\",\n",
      "      \"url\": \"https://www.semanticscholar.org/paper/d2113b74e2b4199024d108dcb67066c4b9077f11\",\n",
      "      \"citations\": 0\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Improving Graph Out-of-distribution Generalization on Real-world Data\",\n",
      "      \"year\": 2024,\n",
      "      \"venue\": \"arXiv.org\",\n",
      "      \"url\": \"https://www.semanticscholar.org/paper/d4a60973e5b1577b7f8de01276d9ec6247b06778\",\n",
      "      \"citations\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"confidence\": 0.7,\n",
      "  \"notable_achievements\": [],\n",
      "  \"social_impact\": null,\n",
      "  \"career_stage\": \"student\",\n",
      "  \"overall_score\": 29.0\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "== PROFILE 2 ==\n",
      "\n",
      "\n",
      "\n",
      "{\n",
      "  \".type\": \"AuthorProfile\",\n",
      "  \"name\": \"Lecheng Kong\",\n",
      "  \"aliases\": [],\n",
      "  \"platforms\": {\n",
      "    \"linkedin\": \"https://www.linkedin.com/in/lecheng-kong-12a045152\",\n",
      "    \"dblp\": \"https://dblp.org/pid/319/5576.html\",\n",
      "    \"github\": \"https://github.com/LechengKong\",\n",
      "    \"scholar\": \"https://scholar.google.com/citations?user=yk3-_EgAAAAJ\",\n",
      "    \"orcid\": \"https://orcid.org/0000-0001-9427-8799\"\n",
      "  },\n",
      "  \"ids\": {\n",
      "    \"scholar\": \"yk3-_EgAAAAJ\"\n",
      "  },\n",
      "  \"homepage_url\": \"https://lechengkong.github.io/\",\n",
      "  \"affiliation_current\": \"Washington University in St. Louis\",\n",
      "  \"emails\": [\n",
      "    \"jerry.kong@wustl.edu\",\n",
      "    \"Verified email at wustl.edu\"\n",
      "  ],\n",
      "  \"interests\": [\n",
      "    \"Graph Neural Networks (GNNs)\",\n",
      "    \"Graph Foundation Models (GFM)\",\n",
      "    \"Graph Models and Large Language Models\",\n",
      "    \"graph-based retrieval augmented generation\",\n",
      "    \"universal graph foundation model\",\n",
      "    \"graph learning\",\n",
      "    \"machine learning\",\n",
      "    \"graph language modeling\"\n",
      "  ],\n",
      "  \"selected_publications\": [\n",
      "    {\n",
      "      \"title\": \"GOFA: A Generative One-For-All Model for Joint Graph Language Modeling\",\n",
      "      \"year\": 2024,\n",
      "      \"venue\": \"International Conference on Learning Representations\",\n",
      "      \"url\": \"https://www.semanticscholar.org/paper/055af7789c29452bf808a48f74ec25e01049425e\",\n",
      "      \"citations\": 15\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"TAGLAS: An atlas of text-attributed graph datasets in the era of large graph and language models\",\n",
      "      \"year\": 2024,\n",
      "      \"venue\": \"arXiv.org\",\n",
      "      \"url\": \"https://www.semanticscholar.org/paper/589d8c5afc05002326fcde59a278593f5472e997\",\n",
      "      \"citations\": 3\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Extending the Design Space of Graph Neural Networks by Rethinking Folklore Weisfeiler-Lehman\",\n",
      "      \"year\": 2023,\n",
      "      \"venue\": \"Neural Information Processing Systems\",\n",
      "      \"url\": \"https://www.semanticscholar.org/paper/2c2a0534d7da4f7a5d6e009ae73fe352f4ebfe22\",\n",
      "      \"citations\": 11\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"MAG-GNN: Reinforcement Learning Boosted Graph Neural Network\",\n",
      "      \"year\": 2023,\n",
      "      \"venue\": \"Neural Information Processing Systems\",\n",
      "      \"url\": \"https://www.semanticscholar.org/paper/60953d9393a42fd6f84de80ffe8527dc10a7c857\",\n",
      "      \"citations\": 14\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Towards Arbitrarily Expressive GNNs in O(n2) Space by Rethinking Folklore Weisfeiler-Lehman\",\n",
      "      \"year\": 2023,\n",
      "      \"venue\": \"arXiv.org\",\n",
      "      \"url\": \"https://www.semanticscholar.org/paper/9587f39c5a3aac49908fe94664661c7458798242\",\n",
      "      \"citations\": 5\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"One for All: Towards Training One Graph Model for All Classification Tasks\",\n",
      "      \"year\": 2023,\n",
      "      \"venue\": \"International Conference on Learning Representations\",\n",
      "      \"url\": \"https://www.semanticscholar.org/paper/ab22d54dd13876d25c6c8f46c40fb9ac41c61ec5\",\n",
      "      \"citations\": 153\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Time Associated Meta Learning for Clinical Prediction\",\n",
      "      \"year\": 2023,\n",
      "      \"venue\": \"arXiv.org\",\n",
      "      \"url\": \"https://www.semanticscholar.org/paper/b83fd54b6ddc578307aed524ff89bf3e20bcb628\",\n",
      "      \"citations\": 0\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"A Multi-View Joint Learning Framework for Embedding Clinical Codes and Text Using Graph Neural Networks\",\n",
      "      \"year\": 2023,\n",
      "      \"venue\": \"arXiv.org\",\n",
      "      \"url\": \"https://www.semanticscholar.org/paper/cde6b633703eda8c3fbd7f76a967e07b19b7623d\",\n",
      "      \"citations\": 1\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Graph Contrastive Learning Meets Graph Meta Learning: A Unified Method for Few-shot Node Tasks\",\n",
      "      \"year\": 2023,\n",
      "      \"venue\": \"The Web Conference\",\n",
      "      \"url\": \"https://www.semanticscholar.org/paper/d11a500d3f0dec27f0113d0f9a0893542d55ea0c\",\n",
      "      \"citations\": 7\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Manipulating Elections by Changing Voter Perceptions\",\n",
      "      \"year\": 2022,\n",
      "      \"venue\": \"International Joint Conference on Artificial Intelligence\",\n",
      "      \"url\": \"https://www.semanticscholar.org/paper/4548d5611188d63219f5d39d6ab01181a1513e5e\",\n",
      "      \"citations\": 5\n",
      "    }\n",
      "  ],\n",
      "  \"confidence\": 0.6,\n",
      "  \"notable_achievements\": [\n",
      "    \"2022/2023 NeurIPS travel award\",\n",
      "    \"2024.01: \\ud83c\\udf89\\ud83c\\udf89 COLA accepted by WWW2024! Congratulations to Hao!\",\n",
      "    \"2024.01: \\ud83c\\udf89\\ud83c\\udf89 OneForAll paper accepted as Spotlight (5%) by ICLR2024. Thanks everyone for the great teamwork!\",\n",
      "    \"2025.01: \\ud83d\\udd25\\ud83d\\udd25 GOFA was accepted by ICLR2025!\",\n",
      "    \"2024.08: \\ud83d\\udd25\\ud83d\\udd25 GOFA released, we propose a generative approach to solve a wide variety of tasks in the graph domain. GOFA can generate free-form graph-related response to arbitrary human input.\",\n",
      "    \"2024.08: \\ud83d\\udd25\\ud83d\\udd25 TAGLAS released, TAGLAS aim to provide a graph dataset with high diversity from synthetic to real-world data. Check it out if you are building graph foundation models.\"\n",
      "  ],\n",
      "  \"social_impact\": null,\n",
      "  \"career_stage\": \"student\",\n",
      "  \"overall_score\": 65.0\n",
      "}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from objprint import objjson\n",
    "import json\n",
    "\n",
    "print(\"== PROFILE 0 ==\")\n",
    "print('\\n\\n')\n",
    "json_obj_0 = objjson(profile_0)\n",
    "print(json.dumps(json_obj_0, indent=2))\n",
    "print('\\n\\n')\n",
    "print(\"== PROFILE 1 ==\")\n",
    "print('\\n\\n')\n",
    "json_obj_1 = objjson(profile_1)\n",
    "print(json.dumps(json_obj_1, indent=2))\n",
    "print('\\n\\n')\n",
    "\n",
    "print(\"== PROFILE 2 ==\")\n",
    "print('\\n\\n')\n",
    "json_obj_2 = objjson(profile_2)\n",
    "print(json.dumps(json_obj_2, indent=2))\n",
    "print('\\n\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
