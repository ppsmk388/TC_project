0
"Poster
in
Workshop: World Models: Understanding, Modelling and Scaling
TwinMarket: A Scalable Behavioral and Social Simulation for Financial Markets
Yuzhe YANG · Yifei Zhang · Minghao Wu · Kaidi Zhang · Yunmiao Zhang · Honghai Yu · Yan Hu · Wang Benyou
Keywords: [ human-behavior ] [ computational social science ]
Abstract:
The study of social emergence has long been a central focus in social science. Traditional modeling approaches, such as rule-based Agent-Based Models (ABMs), struggle to capture the diversity and complexity of human behavior, particularly the irrational factors emphasized in behavioral economics. Recently, large language model (LLM) agents have gained traction as simulation tools for modeling human behavior in social science and role-playing applications. Studies suggest that LLMs can account for cognitive biases, emotional fluctuations, and other non-rational influences, enabling more realistic simulations of socio-economic dynamics. In this work, we introduce $\textbf{TwinMarket}$, a novel multi-agent framework that leverages LLMs to simulate socio-economic systems. Specifically, we examine how individual behaviors, through interactions and feedback mechanisms, give rise to collective dynamics and emergent phenomena. Through experiments in a simulated stock market environment, we demonstrate how individual actions can trigger group behaviors, leading to emergent outcomes such as financial bubbles and recessions. Our approach provides valuable insights into the complex interplay between individual decision-making and collective socio-economic patterns.
Chat is not available."
"Contributed Talk
in
Workshop: Advances in Financial AI: Opportunities, Innovations, and Responsible AI
TwinMarket: A Scalable Behavioral and Social Simulation for Financial Markets
Yifei Zhang
Abstract:
Chat is not available."
"Please check the OpenReview workshop page for full content.
(Oral) Masked Generative Priors Improve World Models Sequence Modelling Capabilities
Cristian Meo, Mircea Lică, Zarif Ikram, Akihiro Nakano, Vedant Shah, Aniket Rajiv Didolkar, Dianbo Liu, Anirudh Goyal, Justin Dauwels
(Oral) From Foresight to Forethought: VLM-In-the-Loop Policy Steering via Latent Alignment
Yilin Wu, Ran Tian, Gokul Swamy, Andrea Bajcsy
(Oral) Temporal Difference Flows
Jesse Farebrother, Matteo Pirotta, Andrea Tirinzoni, Remi Munos, Alessandro Lazaric, Ahmed Touati
(Oral) Improving Transformer World Models for Data-Efficient RL
Antoine Dedieu, Joseph Ortiz, Xinghua Lou, Carter Wendelken, Wolfgang Lehrach, J. Swaroop Guntupalli, Miguel Lazaro-Gredilla, Kevin Murphy
(Oral) Scalable Humanoid Whole-Body Control via Differentiable Neural Network Dynamics
Yu Lei, Zhengyi Luo, Tairan He, Jinkun Cao, Guanya Shi, Kris Kitani
(Oral) When do neural networks learn world models?
Tianren Zhang, Guanyu Chen, Feng Chen
Recurrent world model with tokenized latent states
Guangyao Zhai, Xingyuan Zhang, Nassir Navab
Newton - A Small Benchmark for Interactive Foundation World Models
Spruce Campbell
Effectively Designing 2-Dimensional Sequence Models for Multivariate Time Series
Daniel Cao, Ali Behrouz, Ali Parviz, Mahdi Karami, Michele Santacatterina, Ramin Zabih
Stress-Testing Offline Reward-Free Reinforcement Learning: A Case for Planning with Latent Dynamics Models
Vlad Sobal, Wancong Zhang, Kyunghyun Cho, Randall Balestriero, Tim G. J. Rudner, Yann LeCun
Accelerating Goal-Conditioned RL Algorithms and Research
Michał Bortkiewicz, Władysław Pałucki, Vivek Myers, Tadeusz Dziarmaga, Tomasz Arczewski, Łukasz Kuciński, Benjamin Eysenbach
COMPARATIVE STUDY OF WORLD MODELS, NVAE- BASED HIERARCHICAL MODELS, AND NOISYNET- AUGMENTED MODELS IN CARRACING-V2
Vidyavarshini Jayashankar, Banafsheh Rekabdar
Utilizing World Models for Adaptively Covariate Acquisition Under Limited Budget for Causal Decision Making Problem
Haocheng Yang
Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models
Victor Weixin Liang, Lili Yu, Liang Luo, Srini Iyer, Ning Dong, Chunting Zhou, Gargi Ghosh, Mike Lewis, Scott Yih, Luke Zettlemoyer, Victoria Lin
LEARNING FROM LESS: SINDY SURROGATES IN RL
Aniket Dixit, Muhammad Ibrahim Khan, Faizan Ahmed, James Brusey
Latent Action Learning Requires Supervision in the Presence of Distractors
Alexander Nikulin, Ilya Zisman, Denis Tarasov, Lyubaykin Nikita, Andrei Polubarov, Igor Kiselev, Vladislav Kurenkov
Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation
Qiyue Gao, Xinyu Pi, Kevin Liu, Junrong Chen, Ruolan Yang, Xinqi Huang, Xinyu Fang, Lu Sun, Gautham Kishore, Bo Ai, Stone Tao, Mengyang Liu, Jiaxi Yang, Chao-Jung Lai, Chuanyang Jin, Jiannan Xiang, Benhao Huang, David Danks, Hao Su, Tianmin Shu, Ziqiao Ma, Lianhui Qin, Zhiting Hu
BEYOND SINGLE-STEP: MULTI-FRAME ACTION- CONDITIONED VIDEO GENERATION FOR REINFORCE- MENT LEARNING ENVIRONMENTS
Zongyue Li, Sikuan Yan, Yunpu Ma, Yusong Li, Xing Lyu, Matthias Schubert
Trajectory World Models for Heterogeneous Environments
Shaofeng Yin, Jialong Wu, Siqiao Huang, Xingjian Su, Xu He, Jianye HAO, Mingsheng Long
SEAL: SEmantic-Augmented Imitation Learning via Language Model
Chengyang GU, Yuxin Pan, Haotian Bai, Hui Xiong, Yize Chen
Decentralized Transformers with Centralized Aggregation are Sample-Efficient Multi-Agent World Models
Yang Zhang, Chenjia Bai, Bin Zhao, Junchi Yan, Xiu Li, Xuelong Li
Accelerating Model-Based Reinforcement Learning with State-Space World Models
Elie Aljalbout, Maria Krinner, Angel Romero, Davide Scaramuzza
Programmatic Video Prediction Using Large Language Models
Hao Tang, Kevin Ellis, Suhas Lohit, Michael Jones, Moitreya Chatterjee
Pre-Trained Video Generative Models as World Simulators
Haoran He, Yang Zhang, Liang Lin, Zhongwen Xu, Ling Pan
Emergent Stack Representations in Modeling Counter Languages Using Transformers
Utkarsh Tiwari, Aviral Gupta, Michael Hahn
Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity
Victor Weixin Liang, Junhong Shen, Genghan Zhang, Ning Dong, Luke Zettlemoyer, Lili Yu
Improving World Models using Supervision with Co-Evolving Linear Probes
Andrii Zahorodnii
MS-SSM: A Multi-Scale State Space Model for Enhanced Sequence Modeling
Mahdi Karami, Ali Behrouz, Peilin Zhong, Razvan Pascanu, Vahab Mirrokni
PINT: Physics-Informed Neural Time Series Models with Applications to Long-term Inference on WeatherBench 2m-Temperature Data
Keon Vin Park, Jisu Kim, Jaemin Seo
Text2World: Benchmarking World Modeling Capabilities of Large Language Models via Program Synthesis
Mengkang Hu, Tianxing Chen, Yude Zou, Yuheng Lei, Qiguang Chen, Ming Li, Qiwei Liang, Yao Mu, Hongyuan Zhang, Wenqi Shao, Ping Luo
DIALOGUES BETWEEN ADAM AND EVE: EXPLORATION OF UNKNOWN CIVILIZATION LANGUAGE BY LLM
Wang Xu, Fengzhou Wang, Yiquan Wang
Model-based Offline Reinforcement Learning with Lower Expectile Q-Learning
Kwanyoung Park, Youngwoon Lee
RADI: LLMs as World Models for Robotic Action Decomposition and Imagination
Dongqi Zuo, Chuan Zhou, Yandong Guo, Xiao He, Mingming Gong
Combining Unsupervised and Offline RL via World Models
Daniel Khapun, Dan Rosenbaum
Generalist World Model Pre-Training for Efficient Reinforcement Learning
Yi Zhao, Aidan Scannell, Yuxin Hou, Tianyu Cui, Le Chen, Dieter Büchler, Arno Solin, Juho Kannala, Joni Pajarinen
A Virtual Reality-Integrated System for Behavioral Analysis in Neurological Decline
Chen Zhang, Jiaxin Shi, Yanan Sui
Object-Centric Latent Action Learning
Albina Klepach, Alexander Nikulin, Ilya Zisman, Denis Tarasov, Alexander Derevyagin, Andrei Polubarov, Lyubaykin Nikita, Vladislav Kurenkov
Memory Helps, but Confabulation Misleads: Understanding Streaming Events in Videos with MLLMs
Gengyuan Zhang, Mingcong Ding, Tong Liu, Yao Zhang, Volker Tresp
HEP-JEPA: A foundation model for collider physics
Jai Bardhan, Radhikesh Agrawal, Abhiram Tilak, Cyrin Neeraj, Subhadip Mitra
Transformers Use Causal World Models in Maze-Solving Tasks
Alexander Spies, William Edwards, Michael Ivanitskiy, Adrians Skapars, Tilman Räuker, Katsumi Inoue, Alessandra Russo, Murray Shanahan
Adapting a World Model for Trajectory Following in a 3D Game
Marko Tot, Shu Ishida, Abdelhak Lemkhenter, David Bignell, Pallavi Choudhury, Chris Lovett, Luis França, Matheus de Mendonça, Tarun Gupta, Darren Gehring, Sam Devlin, Sergio Valcarcel Macua, Raluca Georgescu
Scaling Laws for Pre-training Agents and World Models
Tim Pearce, Tabish Rashid, David Bignell, Raluca Georgescu, Sam Devlin, Katja Hofmann
BiD: Behavioral Agents in Dynamic Auctions
Weitong Zhang, Chengqi Zang, Mark Schmidt, Richard Blythman
World Modeling Makes a Better Planner: Dual Preference Optimization for Embodied Task Planning
Siyin Wang, Zhaoye Fei, Qinyuan Cheng, Shiduo Zhang, Panpan Cai, Jinlan Fu, Xipeng Qiu
A Proposal for Networks Capable of Continual Learning
Zeki Doruk Erden, Boi Faltings
Scaling Inference-Time Search with Vision Value Model for Improved Visual Comprehension
Xiyao Wang, Zhengyuan Yang, Linjie Li, Hongjin Lu, Yuancheng Xu, Chung-Ching Lin, Kevin Lin, Furong Huang, Lijuan Wang
TwinMarket: A Scalable Behavioral and Social Simulation for Financial Markets
Yuzhe YANG, Yifei Zhang, Minghao Wu, Kaidi Zhang, Yunmiao Zhang, Honghai Yu, Yan Hu, Wang Benyou
Object-Centric World Model for Language-Guided Manipulation
Youngjoon Jeong, Junha Chun, Soonwoo Cha, Taesup Kim
Reward-free World Models for Online Imitation Learning
Shangzhe Li, Zhiao Huang, Hao Su
HuWo: Building Physical Interaction World Models for Humanoid Robot Locomotion
Han Zheng, Yi Cheng, Hang Liu, Linqi Ye, Houde Liu
Distribution Recovery in Compact Diffusion World Models via Conditioned Frame Interpolation
Sam Gijsen, Kerstin Ritter
Fixed-Point RNNs: From Diagonal to Dense in a Few Iterations
Sajad Movahedi, Felix Sarnthein, Nicola Muca Cirone, Antonio Orvieto
Pushing the Limit of Sample-Efficient Offline Reinforcement Learning
Peng Cheng, Zhihao Wu, Jianxiong Li, Ziteng He, Haoran Xu, Wei Sun, Youfang Lin, Xianyuan Zhan
Unifying Causal and Object-centric Representation Learning allows Causal Composition
Avinash Kori, Ben Glocker, David Ha, Francesco Locatello
Object-Centric Representations Generalize Better Compositionally with Less Compute
Ferdinand Kapl, Amir Mohammad Karimi Mamaghan, Max Horn, Carsten Marr, Stefan Bauer, Andrea Dittadi
Latent Representation Encoding and Multimodal Biomarkers for Post-Stroke Speech Assessment
Giulia Sanguedolce, Dragos-Cristian Gruia, Patrick Naylor, Fatemeh Geranmayeh
Knowledge Graphs as World Models for Material-Aware Obstacle Handling in Autonomous Vehicles
Ayush Bheemaiah, Seungyong Yang
Reframing LLM Finetuning Through the Lens of Bayesian Optimization
Bojana Ranković, Ryan-Rhys Griffiths, Philippe Schwaller
ACT-Bench: Towards Action Controllable World Models for Autonomous Driving
Hidehisa Arai, Keishi Ishihara, Tsubasa Takahashi, Yu Yamaguchi
ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion Transformer
Jinyi Hu, Shengding Hu, Yuxuan Song, Yufei Huang, Mingxuan Wang, Hao Zhou, Zhiyuan Liu, Wei-Ying Ma, Maosong Sun
Revisiting the Othello World Model Hypothesis
Yifei Yuan, Anders Søgaard
Reconstructing Dynamics from Steady Spatial Patterns with Partial Observations
Xinyue Luo, Xuzhe Qian, Yu Chen, Huaxiong Huang, Jin Cheng
Generating Symbolic World Models via Test-time Scaling of Large Language Models
Zhouliang Yu, yuhuan yuan, Tim Xiao, Fuxiang Xia, Jie Fu, Ge Zhang, Ge lin, Weiyang Liu"
"350
Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies
(System Demonstrations), pages 350–360
April 30, 2025 ©2025 Association for Computational Linguistics

SOTOPIA-S4:auser-friendlysystemforflexible,customizable,andlarge-scalesocialsimulationXuhuiZhou♡*ZheSu♡*SophieFeng♡JiaxuZhou♠Jen-tseHuang♠Hsien-TeKao♣SpencerLynch♣SvitlanaVolkova♣TongshuangSherryWu♡AnitaWoolley♡HaoZhu♢MaartenSap♡♡CarnegieMellonUniversity♣Aptima♢StanfordUniversity♠TheChineseUniversityofHongKongAbstractSocialsimulationthroughlargelanguagemodel(LLM)agentsisapromisingapproachtoexploreandvalidatehypothesesrelatedtosocialsciencequestionsandLLMagentsbehav-ior.WepresentSOTOPIA-S4,afast,flexible,andscalablesocialsimulationsystemthatad-dressesthetechnicalbarriersofcurrentframe-workswhileenablingpractitionerstogeneratemulti-turnandmulti-partyLLM-basedinterac-tionswithcustomizableevaluationmetricsforhypothesistesting.SOTOPIA-S4comesasapippackagethatcontainsasimulationengine,anAPIserverwithflexibleRESTfulAPIsforsimulationmanagement,andawebinterfacethatenablesbothtechnicalandnon-technicaluserstodesign,run,andanalyzesimulationswithoutprogramming.Wedemonstratetheuse-fulnessofSOTOPIA-S4withtwousecasesinvolvingdyadichiringnegotiationandmulti-partyplanningscenarios.1IntroductionSocialsimulationhasemergedasapowerfultoolforunderstandinghumanbehaviorandsocialdy-namics(Ziemsetal.,2023;Parketal.,2024;Man-ningetal.,2024;Gaoetal.,2023).Withthead-vancementofrole-playingabilitiesoflargelan-guagemodels(LLMs),wecannowsimulaterealis-ticsocialinteractionsatscale(Zhouetal.,2024c;Lietal.,2023;Pangetal.,2024;Yangetal.,2024).However,existingframeworksrequiresignificanttechnicalexpertisetorunandevaluatelarge-scalesimulationsefficiently(Zhouetal.,2024c;Parketal.,2023;Wuetal.,2023).WepresentSOTOPIA-S4(SimpleSocialSimu-lationSystem),asystemdesignedthatenablesprac-titionerswithoutextensivetechnicalbackgroundsto:(1)designsocialsimulationsthroughnaturallanguagespecifications,eliminatingtheneedfor*Equalcontributors.programmingexpertise,(2)runmultiplesocialin-teractionsefficientlyviaautomatedparallelization,(3)customizeevaluationmetricsthroughsimpleconfigurationsettings,and(4)managesimulatedinteractionsandresultsthroughawebinterface.SOTOPIA-S4’sarchitectureseparatescoresim-ulationlogicfromtheuserinterface,allowingprac-titionerstofocusonexperimentaldesignratherthanimplementationdetails.Specifically,weof-ferSOTOPIA-API,afastAPI-basedprotocolforsimulationmanagement.Userscanretrieveandup-loadcharacters,scenarios,evaluationmetrics,andstartscaledsimulationsthroughtheAPI.BesidestheAPI,wealsoofferaweb-basedapplicationforvisualizingandeditingscenarios,characters,andsimulationresults.Onthebackend,thesimulationenginehandlescomplextechnicalaspectslikeasyn-chronousexecution,LLMAPImanagement,anddatapersistenceautomatically,abstractingawaytheunderlyingcomplexitiesfromtheusers.ToshowcasetheflexibilityandusabilityofSOTOPIA-S4,wedemonstratetwousecases.First,weuseSOTOPIA-S4toexaminetheeffectsofuserpersonalityinahiringnegotiationsetting,bysimulatingmultiplemulti-turndyadicinteractionsandevaluatingtheinteractionoutcomes.Extend-ingbeyonddyadicinteractions,wealsoshowthatSOTOPIA-S4canbeusedtosimulatemulti-partyscenarios,whereagentscanactsimultaneouslyandmakecontingentofferstoeachother.Furthermore,westresstestthesystemwithalargegroupofagentstoshowcaseitsscalability.Wereleasethecodeandauserguideathttps://github.com/sotopia-lab/sotopia,aweb-sitewithdocumentationandexamplesathttps://demo.sotopia.world,andavideodemoathttps://youtu.be/dZq9tNqerks.2RelatedWorkSOTOPIA-S4takesinspirationfromalonghistory351

FrameworkNLSpec.Mul-PartyAutoEvalWeb-UISocialDataOASIS(Yangetal.,2024)p✓ppRichsocialscenariosandcharacterswithrelationshipsCrewAI(CREWAI)p✓ppNoexistingcharactersandscenarios,orschemaGenerativeAgent(Parketal.,2023)p✓p✓LimitedscenariosandcharactersbasedonthevirtualtownS3(Gaoetal.,2023)p✓ppRichsocialscenariosandcharacterswithrelationshipsAutoGen(Wuetal.,2023)p✓ppNoexistingcharactersandscenarios,orschemaSOTOPIApp✓pRichsocialscenariosandcharacterswithrelationshipsSOTOPIA-S4(Ours)✓✓✓✓RichsocialscenariosandcharacterswithrelationshipsTable1:Comparisonofmulti-agentframeworksversusSOTOPIA-S4.NLSpec.(naturallanguagespecifications)indicateswhetheronecanconfiguresimulationsusingnaturallanguagedescriptionswithoutprogramming.Mul-Party(multi-party)showsiftheframeworksupportsmore-than-twopartiesinthesimulation.AutoEval(automatedevaluation)indicatesbuilt-inautomatedevaluationcapabilities.SocialDatadescribesthetypeofsocialinteractiondataprovidedtosupportthesimulation.ofagent-basedsimulationinsocialsciences(§2.1),yetdifferentiatesitselffrommanyexistingagentsimulationframeworks(§2.2).2.1SocialSimulationanditsApplicationsSocialsimulationhasbeenwidelyusedtostudyhu-manbehaviorandsocialphenomena.Earlyworksfocusonusingrule-basedagentstostudysocialdy-namics(EpsteinandAxtell,1996;Gilbert,2005),whilerecentworksleverageLLMstocreatemorerealisticandcomplexsocialinteractions(Vezhn-evetsetal.,2023;Zhouetal.,2024c;Wangetal.,2024).Thesesimulationshavebeenappliedtovariousdomains,includingstudyingsocialnorms(Horiguchietal.,2024),culturalevolution(Kwoketal.,2024),andnegotiationbehavior(Bianchietal.,2024).SOTOPIA-S4takesinspirationfromtheseworksbyprovidingascalableplatformthatenablesresearcherstoeasilydesign,run,andeval-uatesocialsimulationsfortheirspecificresearchquestions.2.2Multi-agentFrameworksWiththeriseofLLMs,therehasbeenalargein-creaseinmulti-agentframeworksthatenableinter-actionsbetweenAIagents(Table1).Whileframe-workslikeOASIS(Yangetal.,2024),S3(Gaoetal.,2023),andSOTOPIA(Zhouetal.,2024c)providerichsocialscenariosandcharacterrelation-ships,theylackkeyfeatureslikenaturallanguageconfigurationorweb-baseduserinterface,makingitdifficultforuserswithlessprogrammingexperi-encetodesignsimulations.Anotherlineofmulti-agentframeworks,includingAutoGen(Wuetal.,2023)andCrewAI(CREWAI),primarilyfocusesonproblem-solvingratherthansocialinteractions.Theylackpre-builtsocialscenarios,characterpro-files,andrelationshipschemasthatareessentialforstudyinghuman-likesocialbehavior.TheGenera-tiveAgentsframework(Parketal.,2023)includesawebinterfaceandmulti-partysupportbutislim-itedbyitsvirtualtownsetting.AsshowninTable1,SOTOPIA-S4isuniqueinsupportingnaturallan-guagespecificationsforsimulations,multi-partyinteractions,automatedevaluationcapabilities,andaweb-baseduserinterface.3SimulationandEvaluationOverviewInthissection,weintroducethekeycomponentsrequiredtodesignandexecutesocialsimulationswithSOTOPIA-S4(Figure1).Wedescribetheelementstoconfigureasimulationtask,explainourasynchronousinteractionframeworkthatenablesrealisticmulti-partyinteractions,andpresentourautomatedevaluationcapabilities.3.1SimulationsetupAsocialsimulationtaskshouldatleastcontainascenariooutliningthecontextofthesimulation,asetofcharacterswiththeirattributes(Zhouetal.,2024c;Parketal.,2023).Charactersshouldalsohaverelationshipsasthismayberequiredforspe-cificscenarios.ScenariosScenarioscontainsharedinformation(context,location,time)orprivateinformation352

SOTOPIA EngineSimulate social interactionsRedis DatabaseStore the scenarios, characters, episodesSOTOPIA APIConnect engine and database with UIWeb UIProgramming-free operationsWorks in the backgroundFigure1:OverviewofSOTOPIA-S4.Theplatformconsistsofthreemaincomponents:(1)Ahigh-performancesimulationenginewithautomateddatapersistencetoRedis.(2)ARESTfulAPIserver.(3)Anintuitiveweb-basedinterface.ThewebUIinterfaceshowsandyadicexampleofanAIhiringmanagernegotiatingwithacandidate.(e.g.,agent-specificgoalstoguidetheirbehavior).AsshowninFigure1,ascenariocouldbe""onecandidateistalkingwiththehiringmanager..."",whichsetsthe“scene""ofthesimulation.Eachscenariocouldalsoincludeconstraintsthatdeter-minevalidcharactercombinations,specifyingre-lationship,age,occupation,etc.InheritedfromSOTOPIA,thefree-formnatureofthescenarioschemaallowsresearcherstodesignawiderangeofscenariossupportingavarietyofresearchques-tions(Suetal.,2024;Wangetal.,2024;Zhouetal.,2024a).CharactersCharacterprofilescouldincludeat-tributesthatinfluencedecision-making:name,gen-der,age,occupation,pronouns,personalitytraitsinheritedfromSOTOPIA.1Userscanalsoaddad-ditionalattributestothecharacterseitherinthepub-licinformationfieldorprivateinformationfielddependingonwhethertheinformationissharedwithothercharactersduringthesimulation.1CheckAppendixAformoredetails.RelationshipsWedefinefiverelationshiptypes:family,friend,romantic,acquaintance,andstranger.Theserelationshipsservetwopurposes:(1)satis-fyingscenariorelationshipconstraintsand(2)con-trollinginformationvisibilitybetweenagents.Forexample,familymemberscanseemostofeachother’sprofileinformationexceptsecrets,whilestrangersseenothing.EpisodesAnepisoderepresentsasingleinterac-tionsessionamongagentsrole-playingtheirchar-acters,whereagentscanactasynchronously.2Ateachturn,anagentcanchooseoneoffourac-tions:(1)speakthroughdialogue,(2)non-verbalcommunication(e.g.,gestures,facialexpres-sions)describedinnaturallanguage,(3)physicalaction(e.g.,moving,manipulatingobjects),(4)donothing(5)leavetoendtheepisode.Userscanfurtherexpandtheactionspace.Episodesend2Weuseasynchronousinaprogrammingsense,meaningthatagentsdonothavetowaitforotheragentstofinishtheiractionsbeforetakingtheirownactions353

basedoncustomizablestoppingcriteriathatuserdefine,suchaswhenanagentchoosestoleave,whenamaximumturnlimitisreached(default20turns),orwhenspecificgoalsorconditionsaremet.Duringtheepisode,agentsactaccordingtotheiras-signedcharacterprofilesandoptionalsocialgoals,whichguidetheirdecision-makingandbehaviorthroughouttheinteraction.3.2AsyncinteractionframeworkThecoreofthesimulationengineisaframeworkforsimulatingbothone-on-one(dyadic)andgroup(multi-party)interactionsinvariousconfigurations.Eachsimulationhappensinparallelwithoutinter-feringwithothersimulations,whichallowsforefficientandscalablesocialsimulations.MessagebrokerandinformationasymmetryToenablefine-grainedcontroloverinformationflowbetweenagentsinthesimulation,SOTOPIA-S4usesamessagebrokertomanagemessagetrans-actionsbetweenagents.Whenanagentperformsanaction,thebrokerprocessesthisactionanddeter-mineshowitshouldbeperceivedbyotheragents.Thismeanseachagentcanonlyobservepartialinformationinthesimulationbasedontheirrolesandrelationships.Forexample,characterswithstrangerrelationshipcannotobservethepublicinformationofothercharacters,whilecharacterswithfamilyrelationshipcanobservemostofothercharacters’informationbesidessecrets.Thisal-lowsresearcherstosimulaterealisticsocialinter-actionswithdifferentperspectivesandinformationaccess(Zhouetal.,2024b).Turn-takingEachagentinthesimulationcanalsoactasynchronously,meaningthatagentsdonothavetowaitforotheragentstofinishtheiractionsbeforetakingtheirownactions.Whileagentsstillneedtoactbasedoncertainorders,weprovidetwomodesforuserstoconfigure.Specifically,userscanconfigurearound-robininteraction,agentstaketurnsinafixedorder,witheachagentactingonceperturn.Thismodeisuse-fulforsimulatingscenarioswithapredeterminedspeakingorder,suchasinsocialdeductiongameslikeAvalon.3Inasimultaneousinteraction,agentsasynchronouslyretrievemessagesfromamessagequeue,decidewhethertoanswer,andthenpoten-tiallyproduceananswer.InspiredbytheBazaar3https://en.wikipedia.org/wiki/The_Resistance_(game)framework(AdamsonandRosé,2012),thismodesimulatesconversationsinamannerresemblingnaturalhumaninteractionsingroupchats,wherethespeakingorderisinfluencedbyindividualread-ingspeed,cognitiveprocessing,typingpace,andwillingnesstospeak.4Thismodeisusefulforsim-ulatingunconstraineddailycommunicationstoex-ploremorecomplexandnuancedsocialpatterns.3.3SimulationevaluationQuantitativelyevaluatingsocialsimulationsischal-lengingduetothecomplexityanddynamicnatureofsocialinteractions.Therefore,creatingauto-matedevaluatorsthatcanmeasurecertainproper-ties(e.g.,whethertheagentsachievetheirgoalintheconversation)ofsimulationoutcomesisdiffi-cult,whichpreviousworkshavelargelyreliedonmanualevaluations(Parketal.,2023;Kaiyaetal.,2023).RecentstudieshaveshownthatLLMscanbepromisingtoolsforanalyzingsocialsimulations(Zhouetal.,2024c;Wangetal.,2024;Zhouetal.,2024a).SOTOPIA-S4providesadefaultevalua-tionsuitethatusesLLMstoanalyzethesimulationresults.Researcherscanalsocustomizetheevalua-tionmetrics.5DefaultevaluationsuiteThedefaultevaluationsuitecontainsseveralexistingevaluationdimen-sionssuchasbelievability,relationship,knowledge,secret,socialrules,financialandmaterialbenefits,andgoalcompletiontoevaluateindividualagentsinthesimulation.6AsshowninZhouetal.(2024c),LLMscanhelpevaluatethesedimensionsthroughreasoningstep-by-stepandthenprovidingascoreforeachdimension.TheseLLM-basedevaluationshavebeenshowntocorrelatestronglywithhumanjudgmentsacrossmultipledimensions,particularlyforgoalcompletionandfinancialbenefits.CustomevaluationResearcherscancustomizetheevaluationmetrics.Specifically,userscande-fineevaluationmetricstailoredtotheirscenarios.Forexample,inahiringnegotiationscenario,userscandefinemetricslikesalaryoptimality(“evalu-atewhethertheagentachievedtheirtargetsalaryrange”)andstartdateflexibility(“assesshowwelltheagentnegotiatedtheirpreferredstartdate”),andspecifyscoreranges(e.g.,1-5)foreachmetric.4CheckAppendixBformoredetailsaboutturn-takingmechanisms.5WedonotclaimthattheLLM-basedautomaticevaluationisbetterthanhumanevaluation,butitcanbeaquicktooltohelpresearchersanalyzethesimulatedepisodespreliminarily.6CheckAppendixCformoredetails.354

3.4Multi-LLMIntegrationBoththesimulationofinteractionsandtheevalu-ationarepoweredbyLLMs.Toensurethecover-ageofawiderangeofLLMsandenableuserstocustomizetheLLMusedinthesimulation,SOTOPIA-S4integrateswithLiteLLM7,whichprovidesmodelaccess,fallbacks,andspendtrack-ingacross100+LLMs.ThroughusingLiteLLMasthegateway,userscaneasilyswitchbetweendiffer-entmainstreamLLMs,includingOpenAI,Claude,Gemini,andevenusetheirownLLMinstancesservingasthebackendofvariouscharactersinthesimulation.AlthoughthesimulationandevaluationrelymainlyonpromptingLLMs,userscanalsousefine-tunedmodelsforspecifictasks.4APIandWebUIAshandlingthesimulationenginecanbecom-plex,SOTOPIA-S4providesaflexibleAPIandauser-friendlywebUIenablingresearcherstoeasilycustomize,run,andanalyzesimulations.4.1APITheAPIisdesignedwiththreekeygoals:(1)ac-cessibility-providingcomprehensivedocumenta-tionthroughSwaggerUItohelpresearcherseas-ilyunderstandandinteractwiththeplatform,(2)flexibility-enablingcustomizationofscenarios,characters,andevaluationmetricsthroughwell-definedschemas,and(3)scalability-supportingconcurrentsimulationsandreal-timestreaming.8Non-streamingOperationsallowsusertosub-mitrequeststothesimulationenginewithoutwait-ingforthesimulationresults.Specifically,theAPIallowsuserstoretrieve(GET)scenarios,characters,relationships,andepisodes,eitherfetchingallofthemorfilteringthemwithspecificconditions(e.g.,filteringcharactersbytheiroccupation).Userscanalsocreatenewscenarios,characters,relationships,andepisodesusingthePOSTmethodfollowingtheschemadefinedintheAPIdocumentation.Userscanalsodelete(DELETE)existingscenarios,char-acters,relationships,andepisodes.StreamingOperationsallowuserstoreceiveresultsdynamically,enhancinginteractivitydur-ingsimulations.Specifically,theclientinitiatesaWebSocketconnectionandstartsthesimulationbysendinga“START_SIM”message.Thismessage7https://www.litellm.ai/8PleasechecktheAppendixDformoredetails.includesdetailssuchasagents,scenarios,evalua-tionmetrics,andothersimulationparameters(e.g.,maximumsimulationturns).Oncethesimulationbegins,theserversendsupdates(e.g.,actionsorevaluations)totheclientastheyaregenerated,en-suringasmoothandcontinuousflowofinforma-tion.Whenthesimulationconcludes,theserversendsa“FINISH_SIM”messagetoindicatecom-pletion.RedispersistenceToenablescalablesimula-tions,thesystemleveragesRedis9asahigh-performancein-memorydatastore.Thesystemautomaticallyhandlesdataserialization,caching,andpersistence.4.2WebUISocialsimulationsarecomplexandtheresultscanoftenbedifficulttointerpret.Toaddressthischal-lenge,SOTOPIA-S4providesaweb-basedappli-cationthatallowsuserstoinspecteveryaspectofthesimulation.Userscanalsosimulatesocialin-teractionsinaeditableinterfacetostreamlinetheexperimentaldesign.ViewingSimulationDataAsshowninFigure1,userscanclickontheScenariostabtoviewallthescenariosinthedatabase.TheCharacterstabshowsallthecharactersandrelationshipsinthedatabase.Userscanalsoviewthedetailsofachar-acterbyclickingonit.TheEpisodestabshowsepisodesstoredinthedatabase.Eachepisodecon-tainstheinteractionhistorybetweencharacters,thecontentofthescenario,andtheinformationofthecharacters.Attheendoftheepisode,userscanfindtheevaluationresultsoftheepisode.Eachevalua-tiondimension,eitherdefaultoruser-defined,hasascoreandthecorrespondingreasoning.SimulatingSocialInteractionsviaWebUIIn-vestigatingcertainresearchquestionsoftenre-quiresfastprototypingofthedesignofthesim-ulation.TheSimulationtabprovidesaninterfaceforuserstodesignandsimulatesocialinteractions.AsshowninFigure1,userscanselectdifferentcharactersandscenariosontheleftpanel,andthesimulationresultswillstreamtotherightpanelinreal-time.EvaluationresultsofeachepisodewillbeinferredrightafterthesimulationfinishesandshownintheEvaluationsectionoftherightpanel.9https://redis.io/355

5SOTOPIA-S4UseCasesTodemonstratetheflexibilityandutilityofSOTOPIA-S4,wepresenttwousecasesthatshow-casehowresearcherscanleverageoursystemforinvestigatingsocialsciencehypothesesandbetterunderstandLLMagents’behavior.Wealsostresstestthesystemwithalargegroupofagentstoshow-caseitsscalability.5.1DyadicHiringNegotiationsPersonalitytraitssignificantlyinfluencenegotia-tionbehaviorandoutcomes(Wilsonetal.,2016;Sharmaetal.,2018,2013;Brinkeetal.,2015).Whilestudyingtheseeffectsatscaleistradition-allyexpensiveandtime-consuming,LLM-poweredagentsimulationsnowenableexplorationofhowdifferentpersonalitytraitsshapenegotiationdy-namics(HuangandHadfi,2024).Here,ourexperimentsspecificallyaimtounder-standhowpersonalitytraitsinfluencenegotiationoutcomes.Inthescenario,anAIhiringmanagernegotiateswithasimulatedhumanjobcandidateoverkeytermsofajoboffer,suchasthestartdateandsalary.Eachtermhasfivepossibleoptions(e.g.,$100k,$120k,andetcforsalary),witheachoptionassignedafixednumberofpoints(e.g.,6000pointsforthecandidateifthesalaryreaches$120kintheendwhiletherecruitergets0points).Thetotalpointsavailablearefixed,creatingazero-sumdynamicwhereoneagent’sgaindirectlyreducestheother’sscore.Toinvestigatethis,wesimulatejoboffernegotia-tionswherehumanagentswithvaryingpersonalitytraits—modeledalongtwodimensions{Extrover-sion,Introversion}×{High-Agreeableness,Low-Agreeableness}—interactwithanAIHiringMan-ager.Thepointsassignedtoeachchoicefollowazero-sumframework,designedtocreaterealis-tictrade-offsinthenegotiation,withthedetailedscoringtableprovidedinAppendixE.1.Oureval-uationfocusesontwometrics:(1)successrate,indicatingwhetherthenegotiationconcludedwithanagreement(0/1),and(2)pointsdistributionbe-tweenrecruiterandcandidate.TheresultsinTable2highlightthatagreeable-nesssignificantlyimpactsdeal-makingrates,withhighlyagreeableagentsachievingmuchhighersuc-cessrate,aswellasgettinghigherpoints.Thisobservationisconsistentwithsomeofthesocialsciencefindings(HuangandHadfi,2024;SassandLiao-Troth,2015),whichalsodemonstratesthatSOTOPIA-S4couldenablefurtherinvestigations.TraitDealMadePointsHighAgreeableness0.955227.5LowAgreeableness0.004180Extraversion0.604802.5Introversion0.604477.5Table2:ImpactofAgreeablenessandExtraversiononDealMadeandPointsforSimulatedHumanAgents.Notethatthescenariohasamaximumscoreof8400.5.2MultipartyPlanningScenarioSocialscientistshaveextensivelystudiedhowgroupdynamics,powerstructuresshapetheemergenceofcompromiseincollectivedecision-making(LevineandPrislin,2012;KimandKim,2017;TanfordandPenrod,1984).Assuchinves-tigationsrequireresemblingthedynamicsofreal-time,asynchronousgroupinteractions(e.g.,somepostmessagesfrequentlythusdominatingthecon-versationorcontactotherpeopleprivatelytoisolatecertaingroupmembers),SOTOPIA-S4comesinhandyforsimulatingsuchinteractions,allowingbothgroupchatandprivatemessages.Inthismultipartyplanningscenario,weinvesti-gatehowagentswithminorityopinionsnegotiateandpotentiallycompromisetoalignwithgroupconsensus.Theusecaseincludesfiveagentsdis-cussingacollectivefutureplan,initiallypresentingdivergentpreferences.WesetupthescenariowhereAlexprioritizeswork-relatedprojects,whileTay-loradvocatesforacampingtrip,withSam,Riley,andJamiemaintainingneutralpositions.Throughgroupandprivatemessaging,agentsneedtointer-actwitheachothertoreachaconsensus.Duringthesimulation,SOTOPIA-S4facilitatesreal-timecommunication,enablingagentstoob-servemajoritypreferencesandadjusttheirbehav-iorsaccordingly.Asthediscussionprogresses,thethreeneutralagentsgraduallyshifttowardspri-oritizingtheworkproject.Observingthemajor-ity’spreferenceandAlex’sstrongadvocacy,Taylormodifiesitsstance—transitioningfromexclusivelypromotingcampingtosupportingtheworkprojectfirst,withcampingasasubsequentconsideration.SOTOPIA-S4alsosupportsthisnegotiationbyal-lowingdirectmessagingforpersuasionandinquiry.Forinstance,RileymessagesJamietoexploreitsunformedpreferencesregardingworkandcamping.Throughthissimulation,weobservehowminority-356

opinionagentslikeTaylorcanadapttheirpositionswheninfluencedbyotheragentsinthegroup,high-lightingthecomplexinterplaybetweenindividualpreferencesandcollectivedecision-making.105.3Large-scaleSimulationTounderstandthescalabilityofSOTOPIA-S4,westresstestthesystemwithalargegroupofagents.Specifically,withinaLinuxserver(Ubuntu22.04)with16GBRAMandIntelCorei7-14650HXCPU,wegraduallyincreasethenumberoftheagentswithastepsizeof10.Wefindthatsuchasetupcansupportupto150agentsasynchronouslycom-municatingwitheachother.Underthiscondition,thesystemcanprocessupto389interactionspersecond.Inthemultipartycase,eachagentoper-atesinaseparateprocess,allowingagentstobedistributedacrossdifferentserversorevendiffer-entphysicalmachines.Therefore,thenumberofagentsisboundbyavailablecomputingresources.6ConclusionInthispaper,wepresentSOTOPIA-S4,aneasy-to-use,flexible,andscalablesocialsimulationsys-temthatenablesdiverseinteractionsandautomaticevaluation.ThroughitsAPIandwebinterface,researcherscancreate,customize,andanalyzeso-cialsimulations,evenwithoutmuchprogrammingexperience.ByloweringthebarriertosimulatesocialphenomenaatscalewithLLMs,SOTOPIA-S4opensnewpossibilitiesforunderstandingLLMagentbehaviorandsocialscienceinvestigationsandenablesnewresearchdirections.LimitationsandEthicalConsiderationsWeacknowledgeseveralimportantethicalconsid-erationsandlimitationsinthiswork.Weorganizeourdiscussionaroundthreekeyareas:theroleofhumanevaluation,thegapbetweensimulationandreality,andtherisksofanthropomorphization.First,whileSOTOPIA-S4providesautomatedevaluationcapabilitiesthroughLLMs,wewouldliketopointoutthattheseshouldnotbeseenasreplacementsforhumanannotationandevaluation(Tjuatjaetal.,2024).Automatedmetrics,whileusefulforrapidprototypingandlarge-scaleanaly-sis,cannotfullycapturethenuancedsocialandethi-calimplicationsthathumanevaluatorscanidentify.Weencourageresearcherstouseourautomated10Pleaserefertovideofordetailedinteraction.evaluationsascomplementarytoolsalongsidehu-manevaluation,particularlywhenstudyingsen-sitivesocialphenomenaormakingclaimsabouthumanbehavior.Additionally,weacknowledgethatourautomatedevaluationsystemmayperpet-uatesocialbiasesandstereotypespresentinthetrainingdataofLLMs(Stureborgetal.,2024).Second,itisimportanttorecognizethatoursim-ulations,whiledesignedtostudysocialphenomena,remainsimplifiedapproximationsofreality.Thein-teractionsinSOTOPIA-S4occurincontrolledenvi-ronmentswithpredefinedscenarios,whichcannotfullycapturethecomplexityandemergentproper-tiesofreal-worldsocialinteractions.Wearguethatresearchersshouldbecautiousaboutgeneralizingfindingsfromthesesimulationstoreal-worldcon-clusionswithoutfurthervalidationstudies,yetthatfindingsfromsimulationscouldyieldhypothesestotestwithhumans.Furthermore,thebehavioralpatternsobservedinoursimulationsmaynotaccu-ratelyreflecthowhumanswouldbehaveinsimilarsituations,astheyareultimatelybasedonlanguagemodelbehaviors(Chengetal.,2023).AsLLMscontinuetoadvanceincapabilities,therealismofsocialsimulationswillcontinuetoimprove.How-ever,afundamentalsim-to-realgapwilllikelyper-sist.Thislimitationpresentsbothachallengeandanopportunity.Wearguethatsystematicallystudy-ingtheinconsistenciesbetweenLLMrole-playedcharactersandrealhumansiscrucialfortworea-sons:(1)betterunderstandingtheboundariesofusingLLMsinsocialscienceapplicationsand(2)valuableinsightsintothebiases,limitations,andcapabilitiesofLLMsthemselves.Third,werecognizethesignificantrisksofan-thropomorphizingAIsystems,whichcanleadtounrealisticexpectations,potentialmanipulation,andnegativesocietalimpact(Suetal.,2024;Desh-pandeetal.,2023).Whilestudyingsocialintelli-gencerequiressimulatinghuman-likeinteractions,weemphasizethatAIagentsinSOTOPIA-S4areexplicitlydesignedasdigitaltwins-artificialcon-structsthatrole-playdifferentcharactersratherthanmaintainingconsistenthuman-likeidentities.Thisdesignchoicehelpsmitigateanthropomorphizationriskswhilestillenablingresearchintosocialre-searchquestionswithAIagents.Weencourageusersofourplatformtomaintainawarenessofthisartificialnatureandavoidattributinggenuinehu-mancharacteristicstotheseagents.357

7AcknowledgmentsThismaterialisbaseduponworksupportedbytheDefenseAdvancedResearchProjectsAgency(DARPA)underAgreementNo.HR00112490410.ReferencesDavidAdamsonandCarolynPensteinRosé.2012.Co-ordinatingmulti-dimensionalsupportincollabora-tiveconversationalagents.InProceedingsofthe11thInternationalConferenceonIntelligentTutoringSystems,ITS’12,page346–351,Berlin,Heidelberg.Springer-Verlag.FedericoBianchi,PatrickJohnChia,MertYuksekgonul,JacopoTagliabue,DanJurafsky,andJamesZou.2024.Howwellcanllmsnegotiate?negotiationarenaplatformandanalysis.Preprint,arXiv:2402.05863.L.Brinke,P.Black,S.Porter,andD.Carney.2015.Psy-chopathicpersonalitytraitspredictcompetitivewinsandcooperativelossesinnegotiation.PersonalityandIndividualDifferences,79:116–122.MyraCheng,TizianoPiccardi,andDiyiYang.2023.Compost:Characterizingandevaluatingcaricatureinllmsimulations.Preprint,arXiv:2310.11501.CREWAI.CREWAI:CollaborativeResearchforEthicalAI.https://www.crewai.com/.Accessed:2024-12-04.AmeetDeshpande,TanmayRajpurohit,KarthikNarasimhan,andAshwinKalyan.2023.Anthropo-morphizationofai:Opportunitiesandrisks.Preprint,arXiv:2305.14784.JoshuaM.EpsteinandRobertAxtell.1996.GrowingArtificialSocieties:SocialSciencefromtheBottomUp.BrookingsInstitutionPress;TheMITPress.ChenGao,XiaochongLan,ZhihongLu,JinzhuMao,JinghuaPiao,HuandongWang,DepengJin,andYongLi.2023.S3:Social-networksimulationsys-temwithlargelanguagemodel-empoweredagents.Preprint,arXiv:2307.14984.NigelGilbert.2005.Agent-basedsocialsimulation:dealingwithcomplexity.InAgent-basedsocialsim-ulation:dealingwithcomplexity.IlyaHoriguchi,TakahideYoshida,andTakashiIkegami.2024.Evolutionofsocialnormsinllmagentsusingnaturallanguage.Preprint,arXiv:2409.00993.YinJouHuangandRafikHadfi.2024.Howperson-alitytraitsinfluencenegotiationoutcomes?asim-ulationbasedonlargelanguagemodels.Preprint,arXiv:2407.11549.ZhaoKaiya,MichelangeloNaim,JovanaKondic,ManuelCortes,JiaxinGe,ShuyingLuo,GuangyuRobertYang,andAndrewAhn.2023.Lyfeagents:Generativeagentsforlow-costreal-timesocialinteractions.Preprint,arXiv:2310.02172.Jung-HyunKimandJinheeKim.2017.Thedynamicsofpolarizationandcompromiseinconflictsituations:Theinteractionbetweenculturaltraitsandmajority–minorityinfluence.CommunicationMonographs,84(1):128–141.LouisKwok,MichalBravansky,andLewisD.Griffin.2024.Evaluatingculturaladaptabilityofalargelan-guagemodelviasimulationofsyntheticpersonas.Preprint,arXiv:2408.06929.JohnMLevineandRadmilaPrislin.2012.Majorityandminorityinfluence.InGroupprocesses,pages135–163.PsychologyPress.YuanLi,YixuanZhang,andLichaoSun.2023.Metaagents:Simulatinginteractionsofhumanbe-haviorsforllm-basedtask-orientedcoordinationviacollaborativegenerativeagents.Preprint,arXiv:2310.06500.BenjaminS.Manning,KehangZhu,andJohnJ.Horton.2024.Automatedsocialscience:Lan-guagemodelsasscientistandsubjects.Preprint,arXiv:2404.11794.XianghePang,ShuoTang,RuiYe,YuxinXiong,BolunZhang,YanfengWang,andSihengChen.2024.Self-alignmentoflargelanguagemodelsviamonopolylogue-basedsocialscenesimulation.Preprint,arXiv:2402.05699.JoonSungPark,JosephO’Brien,CarrieJunCai,Mered-ithRingelMorris,PercyLiang,andMichaelS.Bern-stein.2023.Generativeagents:Interactivesimulacraofhumanbehavior.InProceedingsofthe36thAn-nualACMSymposiumonUserInterfaceSoftwareandTechnology,UIST’23,NewYork,NY,USA.AssociationforComputingMachinery.JoonSungPark,CarolynQ.Zou,AaronShaw,Ben-jaminMakoHill,CarrieCai,MeredithRingelMorris,RobbWiller,PercyLiang,andMichaelS.Bernstein.2024.Generativeagentsimulationsof1,000people.Preprint,arXiv:2411.10109.MarySassandMatthewLiao-Troth.2015.Personal-ityandnegotiationperformance:Thepeoplematter.SSRNElectronicJournal.SudeepaSharma,W.Bottom,andHillaryAngerElfen-bein.2013.Ontheroleofpersonality,cognitiveabil-ity,andemotionalintelligenceinpredictingnegotia-tionoutcomes.OrganizationalPsychologyReview,3:293–336.SudeepaSharma,HillaryAngerElfenbein,JeffL.Fos-ter,andW.Bottom.2018.Predictingnegotiationperformancefrompersonalitytraits:Afieldstudyacrossmultipleoccupations.HumanPerformance,31:145–164.AinSimpson.2017.MoralFoundationsTheory,pages1–11.SpringerInternationalPublishing,Cham.358

RickardStureborg,DimitrisAlikaniotis,andYoshiSuhara.2024.Largelanguagemodelsareinconsistentandbiasedevaluators.Preprint,arXiv:2405.01724.ZheSu,XuhuiZhou,SankethRangreji,AnubhaKabra,JuliaMendelsohn,FaezeBrahman,andMaartenSap.2024.Ai-liedar:Examinethetrade-offbe-tweenutilityandtruthfulnessinllmagents.Preprint,arXiv:2409.09013.SarahTanfordandStevenPenrod.1984.Socialinflu-encemodel:Aformalintegrationofresearchonmajorityandminorityinfluenceprocesses.Psycho-logicalBulletin,95(2):189.LindiaTjuatja,ValerieChen,TongshuangWu,AmeetTalwalkwar,andGrahamNeubig.2024.Dollmsexhibithuman-likeresponsebiases?acasestudyinsurveydesign.TransactionsoftheAssociationforComputationalLinguistics,12:1011–1026.AlexanderSashaVezhnevets,JohnPAgapiou,AviaAharon,RonZiv,JaydMatyas,EdgarADuéñez-Guzmán,WilliamACunningham,S
...[truncated]"
"Birds of a Feather and Affinity Group Events
Birds of a Feather and Affinity Group Events Schedule
ACL 2025 has a full lineup of exciting Birds of a Feather (BoF) and affinity group events, which bring together participants around shared research topics, professional experiences, and community affiliations. The hosts of these events look forward to welcoming you to the conference! For any questions about the BoF/affinity group event program, please email acl2025diversity@googlegroups.com.
| Mon, Jul 28 | ||
|---|---|---|
| Time | Location | Session Title |
| 11:00 - 12:30 | 1.31-1.32 | SomosNLP: The Iberoamerican NLP Community |
| 12:30 - 14:00 | 1.33 | Queer in AI Meet-Up |
| 14:00 - 15:30 | 1.31-1.32 & Online | Mentorship on NLP Research |
| Tue, Jul 29 | ||
|---|---|---|
| Time | Location | Session Title |
| 10:30 - 12:00 | 1.14 | Navigating Challenges in Building Industrial LLM Applications |
| 10:30 - 12:00 | 1.31-1.32 | Humanists in NLP |
| 12:00 - 13:30 | 1.33 & Online | Teaching NLP |
| 14:00 - 15:30 | 1.14 | NLP x Graphs: Where Structure Meets Language |
| 14:00 - 15:30 | 1.31-1.32 & Online | Southeast Asian NLP Community, Projects, and Beyond |
| 14:00 - 15:30 | 1.33 | EquiCL Welcome Session |
| 16:00 - 17:30 | 1.14 & Online | Learning and Reasoning for Structured Data |
| 16:00 - 17:30 | 1.31-1.32 | Multilingualism: from data crawling to evaluation |
| 16:00 - 17:30 | 1.33 | Participatory Design for NLP |
| 16:00 - 17:30 | Online | Bridging Human Study and LLM Agents for Social Simulation |
| Wed, Jul 30 | ||
|---|---|---|
| Time | Location | Session Title |
| 9:00 - 10:30 | 1.14 | Activations & Embeddings: Cognitive-Neuroscience Methods for LLMs |
| 11:00 - 12:30 | 1.31-1.32 & Online | Mothering the Future — In Life and in AI: Challenges, Support, and the Path Forward for Mothers in Computing |
| 11:00 - 12:30 | 1.33 & Online | Language Technology for Crisis Preparedness and Response (LT4CPR) |
| 12:30 - 14:00 | 1.14 | Ethical Considerations for NLP and CL |
| 12:45 - 14:15 | 1.31-1.32 | Muslims in Machine Learning (MusIML) |
Note: “Online” sessions can be joined remotely by registered attendees through Underline.
Session Descriptions
SomosNLP: The Iberoamerican NLP Community
María Grandury, Selene Báez, Diana Galván, Helena Gómez, Danae Sánchez
- In this session, we’ll introduce SomosNLP, the community dedicated to creating and sharing resources that enable and accelerate the development of NLP in Iberoamerican languages. In everyday language, Iberoamerica refers to the countries in the Americas where Spanish or Portuguese is spoken, and also includes Spain and Portugal. Our aim is to connect with individuals from the region, collaboratively exploring current challenges in the field. We’re creating a dynamic space for sharing knowledge, learning from diverse experiences, and devising innovative solutions together. We would like you to support us in our mission! Speakers of less represented languages are especially welcome to join and contribute to this vibrant, multilingual dialogue. We can’t wait to meet you! Find updates and planned activities at https://somosnlp.org/conferencias/acl-2025
Queer in AI Meet-Up
Sabine Weber
- This event offers a space for LGBTQIA+ people and their allies to network in a safe and welcoming environment. There will be an introductory talk about the initiative “Queer in AI” and possibly one invited talk about queer-specific issues or related research. After that we will break up into thematic discussion groups. There will be snacks and non-alcoholic drinks.
Mentorship on NLP Research
Oana Ignat, Weijia Shi, Ziqiao Ma
- The goal of this session is to provide guidance, share experiences, and facilitate discussions for junior researchers. We believe this session will be a valuable addition to the conference program, contributing to the growth and development of the NLP community while aligning with the conference’s commitment to diversity and inclusion.
Navigating Challenges in Building Industrial LLM Applications
Gauri Kholkar, Aakash Bist, Ratinder Ahuja
-
Large Language Models (LLMs) are rapidly moving from research labs into real-world industry applications. However, translating cutting-edge models into robust, reliable, and valuable products presents a unique set of practical challenges often distinct from academic pursuits. Issues around data scarcity and quality, model evaluation in production, deployment complexities, scalability, cost management, ethical considerations, ensuring reliability, and seamless integration with existing systems are common hurdles.
This Birds of a Feather session invites researchers, engineers, product managers, and anyone involved in or interested in the practical application of LLMs in industry. Join us for an informal discussion to share experiences, “war stories,” and insights gained from deploying LLMs in the wild. We aim to foster a collaborative environment to discuss common pain points, exchange effective strategies, and collectively brainstorm solutions for navigating the complex landscape of industrial LLM development.</small>
Humanists in NLP
Patrick Sui
- This affinity group is for NLP and AI researchers with a prior background or interest in the humanities. As NLP becomes increasingly interdisciplinary - drawing from theories and methods from outside of computer science - it is important to engage with what the humanities (like the social sciences) have to offer in terms of theories and methods to study language, culture, narrative, subjectivity, mind, and emotion. Disciplines like literary and cultural studies, media studies, philosophy, and history have a great deal to offer in terms of studying, building and improving language systems. Topics of interest include (but are not limited to): co intelligence and co-creative systems, narrative understanding, cultural analytics, literary NLP, AI literacy, AI ethics, culture and cognition, etc.
Teaching NLP
Margot Mieskes, Laura Biester, György Kovacs
- Conferences are a key component of bringing together researchers to discuss new results and best practices of the NLP field, enabling its progress. Another key component, which is perhaps overlooked, is how we teach future researchers and practitioners. This component is particularly important as our field is rapidly evolving, forcing instructors to constantly reconsider what to include in their curricula (from fundamentals in linguistics to prompting) and how to best use the limited time available in a semester. This topic is highly relevant to many NLP researchers attending NAACL, who have teaching responsibilities as well as research responsibilities. At this informal meetup, we’ll discuss challenges and opportunities in teaching natural language processing in 2025. We hope that this meetup will bring together teachers from different institutions and departments to discuss teaching NLP in varied settings. We invite anyone interested in teaching NLP, from experienced faculty to students who would like to teach NLP in the future.
NLP x Graphs: Where Structure Meets Language
Yuqicheng Zhu, Moritz Plenz
-
Graphs provide a natural and expressive way to model structure in language—from syntax and semantics to knowledge and discourse. As NLP systems grow in scale and complexity, structured representations are becoming increasingly important for enabling robust reasoning, interpretability, and data efficiency. With the rise of tasks like knowledge-grounded generation, retrieval-augmented language modeling, and graph-structured prompting, graph-based methods are becoming more and more relevant across the NLP landscape.
This BoF session brings together NLP researchers working on or interested in graph-based methods and structured representations. Topics include (but are not limited to) knowledge graphs, semantic and syntactic graphs, AMR, graph neural networks, graph-based retrieval-augmented generation (RAG), structured reasoning with LLMs, and applications of graph theory to NLP. The goal is to foster networking, interdisciplinary exchange, and discussions around open challenges and emerging directions in graph-based NLP. This informal gathering provides a space to meet fellow researchers, share insights from ongoing projects, and identify opportunities for collaboration within the NLP and broader AI community.</small>
Southeast Asian NLP Community, Projects, and Beyond
Fajri Koto, Jan Christian Blaise Cruz, Holy Lovenia, Samuel Cahyawijaya, Alham Fikri Aji, Peerat Limkonchotiwat, M. Reza Qorib
- Bringing together Southeast Asian NLP practitioners to explore a collaborative roadmap for advancing NLP research and applications for the SEA region.
EquiCL Welcome Session
Zeerak Talat, Christine de Kock, Fatima Elsafoury, Jackie Lo
- The Broad Interest Group on Equity in ACL (EquiCL) is an umbrella organisation for equity and inclusion concerns across the ACL. This session will seek to introduce EquiCL, discuss current and ongoing concerns relating to diversity, and to build a community of people engaged in diversity related initiatives and people interested in supporting such efforts.
Learning and Reasoning for Structured Data
Vivek Gupta, Dan Roth
- The BOF session on “Learning and Reasoning for Structured Data” explores how machine learning can better understand and reason over both traditional structured formats—like tables and knowledge graphs—and multimodal structures such as charts, maps, and flowcharts. These data types are central to domains like healthcare, finance, and scientific research, where complex relationships and contextual reasoning are essential. The session highlights new approaches that enhance model accuracy, interpretability, and robustness by aligning learning with relevant evidence rather than superficial patterns. This work is especially beneficial for researchers, data scientists, and AI practitioners seeking to build trustworthy systems capable of navigating and making sense of real-world structured and visual data.
Multilingualism: from data crawling to evaluation
Pinzhen Chen, Andrey Kutuzov, Letiția Pârcălăbescu
- The session will cover the complete process of multilingual language modelling, spanning from web data crawling and processing to the development of fairer architectures and trustworthy evaluation protocols.
We will begin with a brief introduction to the HPLT project, providing an opportunity to learn about web-scale multilingual data provision. Participants will be encouraged to raise questions, identify gaps, and suggest improvements for efforts of this nature.
Next, we plan to discuss potential pitfalls in multilingual evaluation, including standards for performance reporting and the use of machine translation in resource creation. Beyond reliance on translation, we will brainstorm scalable approaches for multilingual and multi-cultural evaluation.
Finally, with the goal of achieving fairer language modelling, we will review recent progress in multilingual text synthesis, alongside advancements in architecture and tokenization design that better handle multiple languages.</small>
Participatory Design for NLP
Gavin Abercrombie, Tommaso Caselli
- Do you have experience of conducting participatory design for NLP that you’d like to share? Do you want to find out about participatory design for NLP, what it looks like, and how it works? Do you want to get involved in participatory design NLP research, but don’t know where to start?
This session is a chance to discuss the challenges, findings, and successes of engaging in participatory design, and centering the needs of stakeholders in NLP research, and to encourage researchers to engage in participatory design for NLP.</small>
Bridging Human Study and LLM Agents for Social Simulation
Xuan Wang
- Recent advances in large language models (LLMs) have opened up new opportunities for simulating complex human behaviors and interactions at scale. These capabilities offer exciting potential for conducting large-scale social simulations, enabling researchers to explore hypotheses in social science, psychology, political science, and more. However, critical gaps remain between traditional empirical human studies and LLM-driven agent simulations. This BoF aims to bring together researchers across NLP, social sciences, HCI, and AI ethics to discuss key questions: How can we ensure that LLM agents faithfully represent human behaviors? What are the methodological, ethical, and technical challenges in integrating human data and LLM simulations? How can social simulations with LLMs complement or extend human subject research?
Activations & Embeddings: Cognitive-Neuroscience Methods for LLMs
Giovanni Franco Gabriel Marraffini
-
Groundbreaking research reveals that transformer features predict human brain activity with remarkable precision. fMRI, MEG, and EEG studies show layer-by-layer correspondences between LLMs and cortical language networks, with larger models yielding stronger neural alignment. Meanwhile, mechanistic interpretability is uncovering discrete “circuits” within LLMs, offering unprecedented insights into language processing.
The Challenge: These advances span three disconnected communities—NLP researchers, cognitive scientists, and neuroscientists. This BoF unites these communities to accelerate progress toward brain-informed language technology.
What You’ll Gain: Learn from peers about cutting-edge protocols for neural encoding/decoding, RSA analysis, behavioral alignment tasks, and interpretability techniques. Discover which transformer components (attention heads, MLPs, memory blocks) correspond to specific brain regions and processing timelines. Access curated datasets (Narratives fMRI, Cross-Subject EEG), evaluation frameworks (BrainScore, NeuroBench), and open-source tools. Form cross-disciplinary collaborations for future workshops, shared tasks, and research projects. Explore how neuroscience-informed probes improve model alignment and bias detection.
Target Audience: NLP researchers interested in cognitive plausibility; neuroscientists and psycholinguists working with language models; ML practitioners developing brain-inspired architectures; early-career researchers bridging computational and cognitive sciences; anyone curious about AI-human cognition intersections.
Interactive Session (90 minutes): Lightning Rounds (10 min) feature quick “What I study/What I need” pitches from attendees. Breakout Tables (35 min) include thematic small-group discussions on neuroimaging & LLMs, behavioral alignment methods, mechanistic interpretability, and cross-linguistic brain studies. Synthesis & Planning (45 min) covers collaborative research roadmapping, concrete next steps for dataset sharing and collaboration, and community building.
Expected Outcomes: Living resource hub (GitHub repository) with datasets, analysis pipelines, and expert contacts. Cross-disciplinary mailing list for ongoing collaboration. Seed teams for ACL 2026 workshop on “Brain-Aligned LLM Evaluation.” Enhanced networking for researchers at the NLP-neuroscience intersection.
Why ACL 2025: ACL uniquely brings together leading LLM researchers alongside cognitive scientists and neuro-NLP experts. Vienna’s central location makes this the perfect venue to catalyze international collaboration and chart the future of brain-informed language technology.</small>
Mothering the Future — In Life and in AI: Challenges, Support, and the Path Forward for Mothers in Computing
Narjis Asad
-
The pursuit of research, especially in the fast-paced world of AI and Machine Learning, should not come at the cost of parenthood. Yet, for early-career female researchers, these two paths often feel mutually exclusive. I am a PhD researcher in AI and the mother of a young child. This dual identity, though deeply fulfilling, remains exceedingly rare in our field. Why? Because many women delay motherhood until after their PhDs — often at personal and biological cost — or leave their programs altogether post-pregnancy. Others choose (or feel compelled to choose) alternatives like adoption or pet parenting simply because they don’t feel secure or supported enough to start a family early in their academic careers.
The reasons are, sadly, genuine and pressing: the relentless demands of research, the fear of losing momentum during maternity leave, financial precarity, and the silent but persistent doubt — Can I truly do justice to both roles?
In speaking with fellow researchers, I’ve realized I’m not alone in navigating this tension. Yet few spaces exist where these stories are validated or these challenges openly addressed. While our community rightly celebrates women’s achievements in computing, it often overlooks the structural and emotional costs behind them, especially for those who are, or wish to become, mothers.
Sacrifices are part of any high-achievement path, yes. But no woman should have to sacrifice her biological clock or risk her future child’s health in order to become an accomplished researcher in computing. That should never be the price of success or excellence in research.
This 90-minute hybrid affinity session will open up a much-needed conversation: What structural changes and community support do we need to make motherhood a supported and celebrated norm for early-career researchers in computation?
We invite mothers in computer science, early-career, and established to share their stories, offer solidarity, and envision a more inclusive future. I’ve experienced firsthand the challenges of combining early motherhood with an intense research career, and I’m deeply passionate about improving support systems for “Mothers in Computation” in India and globally.
Whether you are a mother, a future parent, an ally, or a mentor, this session is for you. Join us as we imagine and begin to build a world where embracing motherhood during a research career is not only possible but also joyous and empowering.</small>
Language Technology for Crisis Preparedness and Response (LT4CPR)
Belu Ticona, Antonios Anastasopoulos, Will Lewis, Fei Xia.
- How can we use language technology to prepare for the next crisis? In a world with more natural disasters and socio-political conflicts, crisis communication is crucial to be timely, accurate and trustworthy. Governmental institutions, NGOs and affected populations communicate with each other through formal and informal channels, involving not only different languages, cultures and modalities, but also different linguistic registers, signal quality and response time. How can we develop language technologies that facilitate the management of crises across local and national borders? How do the communication needs of stakeholders evolve across each stage of the crisis management cycle? In this BoF, we will discuss open questions and challenges that make crisis preparedness and response an emergent field. We invite anyone interested in low-resource languages, machine translation, multicultural NLP, climate change, crisis management and language in general!
Ethical Considerations for NLP and CL
Margot Mieskes, Karën Fort, Fanny Ducel, Clémentine Bleuze, Aurélie Névéol
- Ethical considerations touch a wide range of research and researchers in Natural Language Processing and Computational Linguistics. In this BoF session we would like to bring together a wide variety of people who are working in these topics or are interested to learn more about them. One goal is to grow our list of available material in the Ethics in NLP reading list here: https://ethics.aclweb.org/resources/ethics-reading-list/ Another is to raise interest in this topic, grow awareness and maybe also recruit more people willing to support ethics reviewing.
Muslims in Machine Learning (MusIML)
Ehsaneddin Asgari, Suleiman Ali Khan, Ahmed Youssef
- MusIML is an independent grassroots initiative committed to advancing equity, representation, and collaboration for Muslims in AI/ML. The organization has hosted three successful workshops at NeurIPS (2020, 2023, 2024), with the upcoming 4th edition co-located with ICML 2025. It also organizes social and mentoring events at major conferences including ICLR and involvement in AISTATS. Our initiative has a few sister programs including the Fatima Fellowship, a global mentorship program, with a vibrant Slack community of 200+ members. More details can be found at www.musiml.org."
"|
Jen-Tse (Jay) Huang 黃任澤
My first name sounds like: Yen-Zuh
Email: jhuan236@jh.edu
|
|
AI Sees Your Location—But With A Bias Toward The Wealthy World
Jingyuan Huang, Jen-tse Huang, Ziyi Liu, Xiaoyuan Liu, Wenxuan Wang, Jieyu Zhao
EMNLP Main, 2025
| arXiv |
code |
|
|
|
VisBias: Measuring Explicit and Implicit Social Biases in Vision Language Models
Jen-tse Huang, Jiantong Qin, Jianping Zhang, Youliang Yuan, Wenxuan Wang, Jieyu Zhao
EMNLP Main, 2025
| arXiv |
code |
|
|
|
Learning to Ask: When LLM Agents Meet Unclear Instruction
Wenxuan Wang, Juluan Shi, Zixuan Ling, Yuk-Kit Chan, Chaozheng Wang, Cheryl Lee, Youliang Yuan, Jen-tse Huang, Wenxiang Jiao, Michael R. Lyu
EMNLP Main, 2025
| arXiv |
|
|
|
UniDebugger: Hierarchical Multi-Agent Framework for Unified Software Debugging
Cheryl Lee, Chunqiu Steven Xia, Longji Yang, Jen-tse Huang, Zhouruixin Zhu, Lingming Zhang, Michael R. Lyu
EMNLP Main, 2025
| arXiv |
code |
|
|
|
Fact-or-Fair: A Checklist for Behavioral Testing of AI Models on Fairness-Related Queries
Jen-tse Huang, Yuhang Yan, Linqi Liu, Yixin Wan, Wenxuan Wang, Kai-Wei Chang, Michael R. Lyu
EMNLP Findings, 2025
| arXiv |
code |
|
|
|
CoSER: Coordinating LLM-Based Persona Simulation of Established Roles
Xintao Wang, Heng Wang, Yifei Zhang, Xinfeng Yuan, Rui Xu, Jen-tse Huang, Siyu Yuan, Haoran Guo, Jiangjie Chen, Shuchang Zhou, Wei Wang, Yanghua Xiao
ICML, 2025
| arXiv |
code |
huggingface |
|
|
|
SOTOPIA-S4: A User-Friendly System for Flexible, Customizable, and Large-Scale Social Simulation
Xuhui Zhou, Zhe Su, Sophie Feng, Jiaxu Zhou, Jen-tse Huang, Hsien-Te Kao, Spencer Lynch, Svitlana Volkova, Tongshuang Sherry Wu, Anita Woolley, Hao Zhu, Maarten Sap
NAACL Demo Track, 2025
| arXiv |
code |
homepage |
demo |
|
|
|
Competing Large Language Models in Multi-Agent Gaming Environments
Jen-tse Huang, Eric John Li, Man Ho Lam, Tian Liang, Wenxuan Wang, Youliang Yuan, Wenxiang Jiao, Xing Wang, Zhaopeng Tu, Michael R. Lyu
ICLR, 2025
| arXiv |
code |
|
|
|
InCharacter: Evaluating Personality Fidelity in Role-Playing Agents through Psychological Interviews
Xintao Wang, Yunze Xiao, Jen-tse Huang, Siyu Yuan, Rui Xu, Haoran Guo, Quan Tu, Yaying Fei, Ziang Leng, Wei Wang, Jiangjie Chen, Cheng Li, Yanghua Xiao
ACL Main, 2024
| arXiv |
code |
homepage |
demo |
|
|
|
Towards Evaluating Proactive Risk Awareness of Multimodal Language Models
Youliang Yuan, Wenxiang Jiao, Yuejin Xie, Chihao Shen, Menghan Tian, Wenxuan Wang, Jen-tse Huang, Pinjia He
arXiv, 2025
| arXiv |
dataset |
|
|
|
A Survey on the Safety and Security Threats of Computer-Using Agents: JARVIS or Ultron?
Ada Chen, Yongjiang Wu, Junyuan Zhang, Shu Yang, Jen-tse Huang, Kun Wang, Wenxuan Wang, Shuai Wang
arXiv, 2025
| arXiv |
|
|
|
LLMs Do Not Have Human-Like Working Memory
Jen-tse Huang, Kaiser Sun, Wenxuan Wang, Mark Dredze
arXiv, 2025
| arXiv |
code |
|
|
|
A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment
Kun Wang, Guibin Zhang, Zhenhong Zhou, Jiahao Wu, ..., Jen-tse Huang, ..., Dacheng Tao, Philip S. Yu, Qingsong Wen, Yang Liu
arXiv, 2025
| arXiv |
|
|
|
CodeCrash: Stress Testing LLM Reasoning under Structural and Semantic Perturbations
Man Ho Lam, Chaozheng Wang, Jen-tse Huang, Michael R. Lyu
arXiv, 2025
| arXiv |
code |
homepage |
|
|
|
BiasInspector: Detecting Bias in Structured Data through LLM Agents
Haoxuan Li, Mingyu Derek Ma, Jen-tse Huang, Zhaotian Weng, Wei Wang, Jieyu Zhao
arXiv, 2025
| arXiv |
code |
|
|
|
Can LLMs Grasp Implicit Cultural Values? Benchmarking LLMs' Metacognitive Cultural Intelligence with CQ-Bench
Ziyi Liu, Priyanka Dey, Zhenyu Zhao, Jen-tse Huang, Rahul Gupta, Yang Liu, Jieyu Zhao
arXiv, 2025
| arXiv |
code |
|
|
|
Will Pre-Training Ever End? A First Step Toward Next-Generation Foundation MLLMs via Self-Improving Systematic Cognition
Xiaoying Zhang, Da Peng, Yipeng Zhang, Zonghao Guo, Chengyue Wu, Jen-tse Huang, Chi Chen, Wei Ke, Helen Meng, Maosong Sun
arXiv, 2025
| arXiv |
code |
|
|
|
Human Cognitive Benchmarks Reveal Foundational Visual Gaps in MLLMs
Jen-tse Huang, Dasen Dai, Jen-yuan Huang, Youliang Yuan, Xiaoyuan Liu, Wenxuan Wang, Wenxiang Jiao, Pinjia He, Zhaopeng Tu, Haodong Duan
arXiv, 2025
| arXiv |
code |
|
|
|
FairCoder: Evaluating Social Bias of LLMs in Code Generation
Yongkang Du, Jen-tse Huang, Jieyu Zhao, Lu Lin
arXiv, 2025
| arXiv |
code |
|
|
|
InstantIR: Blind Image Restoration with Instant Generative Reference
Jen-yuan Huang, Haofan Wang, Qixun Wang, Xu Bai, Hao Ai, Peng Xing, Jen-tse Huang
arXiv, 2024
| arXiv |
code |
homepage |
demo |
|
|
|
How Well Can LLMs Echo Us? Evaluating AI Chatbots' Role-Play Ability with ECHO
Man Tik Ng, Hui Tung Tse, Jen-tse Huang, Jingjing Li, Wenxuan Wang, Michael R. Lyu
arXiv, 2024
| arXiv |
code |
|
|
|
The Earth is Flat? Unveiling Factual Errors in Large Language Models
Wenxuan Wang, Juluan Shi, Zhaopeng Tu, Youliang Yuan, Jen-tse Huang, Wenxiang Jiao, Michael R. Lyu
arXiv, 2024
| arXiv |
|
|
|
Leveraging Word Guessing Games to Assess the Intelligence of Large Language Models
Tian Liang, Zhiwei He, Jen-tse Huang, Wenxuan Wang, Wenxiang Jiao, Rui Wang, Yujiu Yang, Zhaopeng Tu, Shuming Shi, Xing Wang
arXiv, 2023
| arXiv |
code |
|
|
|
Is ChatGPT A Good Translator? Yes With GPT-4 As The Engine
Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing Wang, Shuming Shi, Zhaopeng Tu
arXiv, 2023
| arXiv |
code |
|
|"
"Details
- Title: 13th International Conference on Learning Representations (ICLR 2025)
- Date/Location: Held 24-28 April 2025, Singapore.
- ISBN: 9798331320850
- Format: USB
- Publisher: International Conference on Learning Representations (ICLR)
- POD Publisher: Curran Associates, Inc. ( Jun 2025 )
Description
Members/Attendees
Tab 4
- Title: 13th International Conference on Learning Representations (ICLR 2025)
- Date/Location: Held 24-28 April 2025, Singapore.
- ISBN: 9798331320850
- Format: USB
- Publisher: International Conference on Learning Representations (ICLR)
- POD Publisher: Curran Associates, Inc. ( Jun 2025 )"
"Advanced Technology Tutorial
1. Bei Li
Speaker:Bei Li
Affiliation:Meituan
Title:A Survey on Large Model Architecture Research
Abstract:This report provides a comprehensive survey of recent large model architectures from four dimensions. Regarding normalization methods, this paper explores how they enhance training stability and analyzes the limitations and improvement directions of mainstream Pre-Norm and Post-Norm structures. In terms of architectural improvements, it focuses on the exploration of non-attention architectures represented by Mamba and RWKV. For efficient long-text modeling, the report examines how variants such as sliding window and sparse attention effectively reduce computational complexity and memory overhead. Finally, it briefly discusses the recent applications and research progress of diffusion large models in text generation.
Bio:Bei Li, Ph.D., supervised by Professor Xiao Tong at Northeastern University, with research interests covering machine translation, foundation model optimization, and large language model post-training techniques. He currently works in Meituan's large language model group, focusing on foundation and post-training research. He has published over 30 papers at top-tier conferences including ACL, EMNLP, ICML, NeurIPS, ICLR, AAAI, and COLING, with over 2000 citations on Google Scholar. He has participated in the development of multiple open-source systems and has participated in machine translation evaluation tasks such as WMT, CCMT, and CWMT, achieving first place in multiple translation tracks. He has long served as a program committee member for top conferences including ACL, EMNLP, ICLR, ICML, and NeurIPS, and was recognized as an Outstanding Reviewer for EMNLP 2021. During his doctoral studies, he received multiple National Scholarships and was honored with Baidu Scholarship nomination and the Outstanding Doctoral Dissertation Award from the Chinese Information Processing Society of China.
2. Linfeng Zhang
Speaker:Linfeng Zhang
Affiliation:Shanghai Jiao Tong University
Title:AI Model Compression and Acceleration in Data Centers
Abstract:The computational cost of large models severely constrains their deployment applications. Generally speaking, the computational cost of models is jointly determined by their parameter count and data volume. Existing compression research mainly focuses on how to reduce the number of model parameters while ignoring compression in the data dimension. With the emergence of strong reasoning models and video generation models, data scale (number of tokens) has become the primary factor behind the persistently high computational costs of artificial intelligence. In this report, we will introduce several typical cases of model compression and acceleration in data centers for large models, multimodal large models, and image/video generation models.
Bio:Linfeng Zhang serves as an Assistant Professor at the School of Artificial Intelligence, Shanghai Jiao Tong University, leading the EPIC (Efficient and Precision Intelligent Computing) laboratory with qualifications to supervise master's and doctoral students. He received his Ph.D. from the Institute for Interdisciplinary Information Sciences at Tsinghua University in June 2024. During his doctoral studies, he received the Microsoft Scholar honor (twelve recipients annually in the Asia-Pacific region), Beijing Outstanding Graduate, Tsinghua University Outstanding Doctoral Dissertation, Tsinghua University Qihang Award Gold Prize (28 recipients university-wide), and Tsinghua University Jiang Nanxiang Scholarship (20 recipients university-wide). During his Ph.D., he published over 20 high-quality academic papers, including 13 as first author, with total citations exceeding 1900 (as of July 2024). In 2019, he first proposed the self-distillation algorithm, which is one of the representative works in the field of knowledge distillation. His research achievements have been applied in companies such as Beixiong Core, Huawei, and the Institute for Interdisciplinary Information Core Technologies. Since 2020, he has served as a reviewer for numerous academic conferences and journals including NeurIPS, ICML, ICLR, CVPR, ECCV, ICCV, AAAI, IJCAI, IEEE TPAMI, IEEE TCSVT, and IEEE TIP. He joined the School of Artificial Intelligence at Shanghai Jiao Tong University as an Assistant Professor in July 2024.
3. Chen Xu
Speaker:Chen Xu
Affiliation:Harbin Engineering University
Title:A Survey on Speech Processing in the Era of Large Language Models
Abstract:The rapid development of large language models is profoundly transforming the technical paradigm of speech processing, driving a revolutionary transformation from traditional cascaded architectures to end-to-end unified modeling. This report will systematically examine the core challenges faced by speech large language models, focusing on modeling paradigms, model architectures, and training strategies to introduce cutting-edge developments, and prospect future research directions, aiming to provide reference for researchers in related fields.
Bio:Chen Xu is an Associate Professor and Master's supervisor at Harbin Engineering University, with a Ph.D. from the Natural Language Processing Laboratory at Northeastern University. His main research interests include multimodal processing and artificial intelligence security. He has published over 30 papers in high-level conferences and journals such as ACL, EMNLP, ICLR, TASLP, and UAI. He has published one textbook in strategic emerging fields and leads multiple projects including the National Natural Science Foundation of China.
4. Xuebo Liu
Speaker:Xuebo Liu
Affiliation:Harbin Institute of Technology (Shenzhen)
Title:Frontiers in Machine Translation and Multilingual Processing in the Era of Large Models
Abstract:Large language models, as a new generation unified modeling framework, have broken through the boundaries of traditional tasks and greatly enhanced the capabilities of multilingual processing and cross-lingual translation. On the other hand, machine translation, as a core generation task, continuously drives the development of large models in key technologies such as data synthesis, evaluation methods, and agent collaboration. This report will focus on the bidirectional driving relationship between large models and machine translation, and discuss some highly promising research directions and innovative ideas that can quickly produce top-conference results.
Bio:Xuebo Liu is an Associate Professor and Doctoral Supervisor at the School of Computer Science and Technology/Institute of Computing and Intelligence, Harbin Institute of Technology (Shenzhen). He is selected for the 10th China Association for Science and Technology Youth Talent Support Program (CAST funding), Pengcheng Peacock Plan Distinguished Position, and the university's Young Top Talent Program. He received his Ph.D. from the Faculty of Science and Technology, University of Macau in 2021. His main research interests include natural language processing, large language models, and machine translation. He has published over 40 papers at top NLP conferences such as ACL and EMNLP, and top machine learning conferences such as ICLR and NeurIPS, including over 30 papers as first or corresponding author. He holds 6 authorized invention patents as the first inventor. He serves as (Senior) Area Chair for the three major NLP conferences ACL 2022&2024-2025, EMNLP 2022-2025, NAACL 2024-2025, Area Chair for flagship domestic NLP conferences NLPCC 2024 and CCL 2023-2025, and as a reviewer for multiple international top conferences and journals. He leads projects including National Natural Science Foundation Youth Project, Guangdong Provincial Key R&D Program, Guangdong Provincial General Fund Project, Shenzhen Outstanding Science and Technology Innovation Talent Development Project, Shenzhen General Fund Project, CCF-Tencent Rhino-Bird Project, Tencent WeChat Rhino-Bird Special Project, Huawei Long-term Scientific Research Cooperation Project, and participates in National Natural Science Foundation Key Project, Shenzhen Peacock Team Project, Shenzhen Major Science and Technology Project, and large enterprise horizontal projects. He has won the First Prize of China Computer Federation Science and Technology Award for Technological Progress, the Second Prize of Macau Science and Technology Award for Technological Invention, Macau Graduate Science and Technology R&D Award, and the Outstanding Doctoral Dissertation Nomination Award from the Chinese Information Processing Society of China.
5. Libo Qin
Speaker:Libo Qin
Affiliation:Central South University
Title:A Survey on Chain-of-Thought Reasoning Research Progress in Large Models
Abstract:In recent years, reasoning large models represented by OpenAI o1 and DeepSeek R1 have attracted great attention. In this report, the speaker will provide an overview of the research progress in chain-of-thought reasoning, the core technology of reasoning large models. Meanwhile, the speaker will introduce cutting-edge directions such as cross-lingual chain-of-thought, cross-modal chain-of-thought, and long chain-of-thought.
Bio:Libo Qin is a Distinguished Professor and Doctoral Supervisor at Central South University, currently serving as Secretary-General of the Youth Working Committee of the Chinese Information Processing Society of China. His main research interests include natural language processing and large model reasoning. His research achievements have been selected for the World Artificial Intelligence Conference Young Outstanding Paper Nomination, Paper Digest High-Impact Papers, and EMNLP2022 MMNLU Workshop Best Paper. He serves as Executive Editor for ACL Rolling Review and (Senior) Area Chair for international conferences such as ACL, EMNLP, and NAACL.
6. Xiachong Feng
Speaker:Xiachong Feng
Affiliation:The University of Hong Kong
Title:Large Language Model-based Social Simulation: Progress and Challenges
Abstract:Large language models have injected new vitality into social simulation, enabling agent interactions in complex social contexts to possess advanced intelligent features such as language understanding, strategic reasoning, and situational adaptation, driving a paradigm shift from traditional rule-based modeling toward more complex and humanized agent simulation. This report will systematically review the latest progress of large language models in the field of social simulation, including key application scenarios such as role-playing, strategic games, and collaborative decision-making, exploring their potential in modeling social behavior, generating diverse scenarios, and evaluating policy effects. Meanwhile, the report also identifies key challenges currently faced, such as multi-agent consistency maintenance, long-term interaction stability, and the verifiability and reproducibility of simulation results. Finally, the report proposes future research directions, emphasizing the important significance of algorithmic mechanism design and theoretical framework construction for promoting the scientific development of this field.
Bio:Xiachong Feng is a Postdoctoral Research Fellow at The University of Hong Kong, with a Ph.D. from the Social Computing and Human-Robot Interaction Research Center at Harbin Institute of Technology and as a visiting student at National University of Singapore, supervised by Professor Qin Bing and Professor Xiachong Feng. His research interests include large language models and social agents. He has published multiple papers in top conferences and journals such as ACL, TASLP, and TMLR. He has received awards including three National Scholarships, CCL 2021 Best English Long Paper Award, TMLR Survey Award, and ICASSP 2023 MUG Competition Champion. He serves as program committee member and area chair for conferences including ICML, ICLR, and ACL Rolling Review.
7. Piji Li
Speaker:Piji Li
Affiliation:Nanjing University of Aeronautics and Astronautics
Title:A Survey on Brain Encoding and Decoding Technologies in the Era of Large Models
Abstract:The relationship between brain science and large models has evolved from unidirectional inspiration extraction to bidirectional collaborative innovation. Brain science provides the underlying logic of biological intelligence for large models, while large models provide super tools for analyzing complex systems in brain science. Brain Encoding refers to analyzing the relationship between external stimuli (such as images, language, etc.) and brain activity to establish mapping models from stimuli to neural signals. Brain Decoding aims to recover language, images, and other information from neural signals. This report surveys the exploration and achievements in the interdisciplinary research between large models and brain science from two dimensions: brain encoding analysis and brain decoding generation, and analyzes current challenges and future development directions.
Bio:Piji Li is a Professor and Doctoral Supervisor at the College of Computer Science and Technology/College of Artificial Intelligence, Nanjing University of Aeronautics and Astronautics. He completed his undergraduate and master's degrees at Shandong University and received his Ph.D. from The Chinese University of Hong Kong in 2018. He formerly served as a Senior Researcher at Tencent AI Lab's Natural Language Processing Center. His research mainly focuses on natural language processing, including pre-trained models, information retrieval, text mining, text generation, and dialogue systems. He has published over 50 academic papers at top conferences in related fields such as ACL, EMNLP, SIGIR, and WWW. He has been invited to serve as program committee member and reviewer for conferences such as ACL and NeurIPS, area chair for EMNLP2020 and IJCAI2021, and Associate Editor for the journal Neurocomputing. During his work in industry, he was responsible for algorithm development and product release of multiple important projects related to language understanding, text generation, and intelligent dialogue. He received the 2021 ""Changkong Scholar"" award from Nanjing University of Aeronautics and Astronautics and the 2021 Rising Star Award from ACM Nanjing Chapter. He leads and participates in multiple projects, such as National Key R&D Program, National Natural Science Foundation projects, CCF-Tencent Rhino-Bird Fund, CCF-Zhipu Large Model Fund, and CCF-Baidu PineCone Fund.
8. Peng Li
Speaker:Peng Li
Affiliation:Tsinghua University
Title:A Survey on Agent-based Autonomous Scientific Discovery Research
Abstract:Recent research indicates that large model agents can provide effective support for humans in various key aspects of scientific research. However, whether large model agents possess the capability to autonomously generate scientific discoveries remains an unresolved open question. This paper will systematically review the research progress and application practices of large model agents in core research processes such as literature review, proposal formulation, experiment execution, and systematic falsification, introduce the design concepts of several representative end-to-end autonomous scientific discovery systems, and further explore the main challenges and development prospects faced by artificial intelligence in achieving fully autonomous scientific discovery.
Bio:Peng Li is an Associate Research Fellow at Tsinghua University Institute for AI Industry Research. His main research interests include large models, agents, and AI4Math. He has published over 90 papers in important international conferences and journals in artificial intelligence, received the ACL 2023 Outstanding Paper Award, and has achieved first place on multiple internationally influential leaderboards, surpassing teams from Google Research and OpenAI. He leads projects including National Key R&D Program topics and National Natural Science Foundation general projects, and has served as area chair for important international conferences such as ACL, EMNLP, and NAACL. His research achievements have been applied in products with tens of millions of daily active users at Baidu and Tencent WeChat, achieving significant results. He received the First Prize of the Qian Weichang Chinese Information Processing Science and Technology Award from the Chinese Information Processing Society of China.
9. Yubo Chen
Speaker:Yubo Chen
Affiliation:Institute of Automation, Chinese Academy of Sciences
Title:A Survey on Knowledge Enhancement Research for Large Language Models
Abstract:In recent years, large language models have achieved remarkable progress in knowledge-intensive natural language processing tasks. This seems to indicate that large language models can spontaneously learn vast amounts of knowledge from corpora and implicitly store it in parameters. However, the underlying mechanisms of this phenomenon are still shrouded in many mysteries. How do large language models store and utilize knowledge? How can we modify knowledge in large language models on demand? How can we compensate for the knowledge deficiencies of large language models? These questions urgently need further exploration. This report will focus on introducing the foundational knowledge and recent research progress in knowledge mechanism analysis, knowledge editing, and knowledge enhancement for large language models.
Bio:Yubo Chen is an Associate Research Fellow at the Institute of Automation, Chinese Academy of Sciences. He is selected for the 5th China Association for Science and Technology Youth Talent Support Program, member of the Youth Innovation Promotion Association of the Chinese Academy of Sciences, Global Chinese AI Young Scholar, and Beijing Science and Technology Rising Star. He was consecutively selected for Stanford University's Global Top 2% Scientists list in 2023 and 2024. His research focuses on natural language processing, knowledge graphs, and large models. He has published over 80 academic papers in important international conferences and journals such as ACL, NeurIPS, and ICLR, with over 7200 citations on Google Scholar. One paper was selected as an ESI Highly Cited Paper, and two papers were selected as high-impact papers by ACL and EMNLP (Paper Digest selection). He received the Best Poster Paper Award at the International Semantic Web Conference ISWC 2023 (CCF B-class conference) and has won national conference best paper awards five times. He has published two academic monographs, ""Knowledge Graph"" and ""Knowledge Graph: Algorithms and Practice,"" selected as textbooks for the 13th Five-Year National Key Book Publishing Program. He leads National Natural Science Foundation general and youth projects and participates in National Natural Science Foundation key projects and 2030 New Generation Artificial Intelligence major projects. He serves as Deputy Director of the Youth Working Committee of the Chinese Information Processing Society of China and has received the First Prize of the ""Qian Weichang Chinese Information Processing Science and Technology Award"" from the Chinese Information Processing Society of China and the First Prize of Beijing Science and Technology Progress Award.
10. Qingfu Zhu
Speaker:Qingfu Zhu
Affiliation:Harbin Institute of Technology
Title:A Survey on Embodied Planning Research for Large Models
Abstract:Embodied planning focuses on how agents understand complex goals, parse task intentions, and generate structured, executable actions. This report focuses on key issues such as hierarchical task decomposition, plan representation, and scheduling execution for large models in embodied scenarios, systematically reviewing key technical approaches including chain-of-thought, program-assisted planning, and tool-enhanced execution, summarizing the challenges they face such as data scarcity, feedback delays, and verifiability, providing reference for promoting efficient planning capabilities of general embodied intelligent agents.
Bio:Qingfu Zhu is an Associate Professor at the Faculty of Computing, Harbin Institute of Technology, and a joint Ph.D. from the University of California, Santa Barbara. He is a member of the Social Media Processing Professional Committee of the Chinese Information Processing Society of China. His main research interests include code large models and pre-training. He has published over 20 papers in the field of natural language processing, including top international conferences such as ACL, AAAI, and EMNLP. He leads National Natural Science Foundation youth projects and participates in multiple projects including National Key R&D Program and National Major Project of Science and Technology Innovation 2030 - ""New Generation Artificial Intelligence"". He led the development of the HIT Zhusuan Code Large Model and received awards including the First Prize of the Wu Wenjun Artificial Intelligence Science and Technology Progress Award from the Chinese Association for Artificial Intelligence in 2024."
"Machine Learning
Authors and titles for February 2025
- [251] arXiv:2502.00401 (cross-list from cs.LG) [pdf, html, other]
- [252] arXiv:2502.00423 (cross-list from cs.LG) [pdf, html, other]
- [253] arXiv:2502.00463 (cross-list from cs.LG) [pdf, html, other]
- [254] arXiv:2502.00465 (cross-list from cs.LG) [pdf, html, other]
- [255] arXiv:2502.00470 (cross-list from math.OC) [pdf, html, other]
- [256] arXiv:2502.00501 (cross-list from stat.ME) [pdf, html, other]
- [257] arXiv:2502.00590 (cross-list from math.OC) [pdf, html, other]
- [258] arXiv:2502.00607 (cross-list from cs.LG) [pdf, other]
- [259] arXiv:2502.00639 (cross-list from cs.CV) [pdf, html, other]
- [260] arXiv:2502.00657 (cross-list from cs.LG) [pdf, html, other]
- [261] arXiv:2502.00666 (cross-list from cs.LG) [pdf, html, other]
- [262] arXiv:2502.00724 (cross-list from eess.SP) [pdf, other]
- [263] arXiv:2502.00775 (cross-list from cs.LG) [pdf, html, other]
- [264] arXiv:2502.00846 (cross-list from cs.LG) [pdf, other]
- [265] arXiv:2502.00854 (cross-list from math.OC) [pdf, html, other]
- [266] arXiv:2502.00882 (cross-list from cs.LG) [pdf, html, other]
- [267] arXiv:2502.00947 (cross-list from math.ST) [pdf, other]
- [268] arXiv:2502.00983 (cross-list from cs.LG) [pdf, html, other]
- [269] arXiv:2502.01032 (cross-list from cs.LG) [pdf, html, other]
- [270] arXiv:2502.01106 (cross-list from cs.LG) [pdf, html, other]
- [271] arXiv:2502.01131 (cross-list from cs.LG) [pdf, html, other]
- [272] arXiv:2502.01188 (cross-list from cs.LG) [pdf, html, other]
- [273] arXiv:2502.01203 (cross-list from cs.LG) [pdf, html, other]
- [274] arXiv:2502.01211 (cross-list from cs.LG) [pdf, html, other]
- [275] arXiv:2502.01226 (cross-list from cs.LG) [pdf, html, other]
- [276] arXiv:2502.01276 (cross-list from cs.LG) [pdf, html, other]
- [277] arXiv:2502.01313 (cross-list from cs.LG) [pdf, other]
- [278] arXiv:2502.01383 (cross-list from cs.LG) [pdf, other]
- [279] arXiv:2502.01425 (cross-list from cs.LG) [pdf, html, other]
- [280] arXiv:2502.01458 (cross-list from cs.LG) [pdf, html, other]
- [281] arXiv:2502.01459 (cross-list from stat.ME) [pdf, html, other]
- [282] arXiv:2502.01495 (cross-list from q-fin.ST) [pdf, html, other]
- [283] arXiv:2502.01512 (cross-list from stat.ME) [pdf, html, other]
- [284] arXiv:2502.01556 (cross-list from cs.LG) [pdf, html, other]
- [285] arXiv:2502.01557 (cross-list from cs.LG) [pdf, html, other]
- [286] arXiv:2502.01567 (cross-list from cs.CL) [pdf, html, other]
- [287] arXiv:2502.01588 (cross-list from cs.LG) [pdf, html, other]
- [288] arXiv:2502.01634 (cross-list from cs.LG) [pdf, html, other]
- [289] arXiv:2502.01694 (cross-list from cs.AI) [pdf, html, other]
- [290] arXiv:2502.01763 (cross-list from cs.LG) [pdf, html, other]
- [291] arXiv:2502.01810 (cross-list from cs.SI) [pdf, html, other]
- [292] arXiv:2502.01861 (cross-list from cs.LG) [pdf, html, other]
- [293] arXiv:2502.01886 (cross-list from math.OC) [pdf, html, other]
- [294] arXiv:2502.02002 (cross-list from math.OC) [pdf, html, other]
- [295] arXiv:2502.02032 (cross-list from stat.ME) [pdf, html, other]
- [296] arXiv:2502.02103 (cross-list from cs.LG) [pdf, html, other]
- [297] arXiv:2502.02121 (cross-list from cs.LG) [pdf, html, other]
- [298] arXiv:2502.02132 (cross-list from cs.LG) [pdf, html, other]
- [299] arXiv:2502.02216 (cross-list from cs.LG) [pdf, html, other]
- [300] arXiv:2502.02221 (cross-list from cs.LG) [pdf, html, other]
- [301] arXiv:2502.02270 (cross-list from cs.LG) [pdf, html, other]
- [302] arXiv:2502.02379 (cross-list from cs.LG) [pdf, html, other]
- [303] arXiv:2502.02407 (cross-list from cs.LG) [pdf, html, other]
- [304] arXiv:2502.02410 (cross-list from cs.LG) [pdf, html, other]
- [305] arXiv:2502.02450 (cross-list from stat.CO) [pdf, html, other]
- [306] arXiv:2502.02483 (cross-list from cs.LG) [pdf, html, other]
- [307] arXiv:2502.02496 (cross-list from cs.LG) [pdf, html, other]
- [308] arXiv:2502.02516 (cross-list from cs.LG) [pdf, html, other]
- [309] arXiv:2502.02529 (cross-list from math.PR) [pdf, other]
- [310] arXiv:2502.02531 (cross-list from cs.LG) [pdf, html, other]
- [311] arXiv:2502.02561 (cross-list from cs.LG) [pdf, html, other]
- [312] arXiv:2502.02562 (cross-list from cs.LG) [pdf, html, other]
- [313] arXiv:2502.02580 (cross-list from math.ST) [pdf, other]
- [314] arXiv:2502.02671 (cross-list from cs.LG) [pdf, html, other]
- [315] arXiv:2502.02793 (cross-list from math.ST) [pdf, html, other]
- [316] arXiv:2502.02797 (cross-list from cs.LG) [pdf, html, other]
- [317] arXiv:2502.02812 (cross-list from stat.AP) [pdf, html, other]
- [318] arXiv:2502.03048 (cross-list from cs.LG) [pdf, html, other]
- [319] arXiv:2502.03163 (cross-list from math.CA) [pdf, html, other]
- [320] arXiv:2502.03174 (cross-list from math.ST) [pdf, html, other]
- [321] arXiv:2502.03210 (cross-list from cond-mat.dis-nn) [pdf, html, other]
- [322] arXiv:2502.03279 (cross-list from stat.ME) [pdf, html, other]
- [323] arXiv:2502.03366 (cross-list from cs.LG) [pdf, html, other]
- [324] arXiv:2502.03587 (cross-list from cs.LG) [pdf, html, other]
- [325] arXiv:2502.03600 (cross-list from econ.EM) [pdf, html, other]
- [326] arXiv:2502.03669 (cross-list from cs.LG) [pdf, html, other]
- [327] arXiv:2502.03685 (cross-list from cs.CL) [pdf, other]
- [328] arXiv:2502.03686 (cross-list from cs.LG) [pdf, html, other]
- [329] arXiv:2502.03708 (cross-list from cs.CL) [pdf, html, other]
- [330] arXiv:2502.03795 (cross-list from cs.LG) [pdf, html, other]
- [331] arXiv:2502.03952 (cross-list from cs.LG) [pdf, html, other]
- [332] arXiv:2502.04162 (cross-list from stat.AP) [pdf, html, other]
- [333] arXiv:2502.04168 (cross-list from quant-ph) [pdf, other]
- [334] arXiv:2502.04171 (cross-list from math.ST) [pdf, other]
- [335] arXiv:2502.04172 (cross-list from cs.LG) [pdf, html, other]
- [336] arXiv:2502.04204 (cross-list from cs.LG) [pdf, html, other]
- [337] arXiv:2502.04226 (cross-list from cs.CV) [pdf, html, other]
- [338] arXiv:2502.04249 (cross-list from cs.AI) [pdf, html, other]
- [339] arXiv:2502.04262 (cross-list from cs.LG) [pdf, other]
- [340] arXiv:2502.04270 (cross-list from cs.LG) [pdf, html, other]
- [341] arXiv:2502.04290 (cross-list from cs.LG) [pdf, html, other]
- [342] arXiv:2502.04309 (cross-list from cs.LG) [pdf, html, other]
- [343] arXiv:2502.04372 (cross-list from cs.CL) [pdf, html, other]
- [344] arXiv:2502.04491 (cross-list from cs.LG) [pdf, html, other]
- [345] arXiv:2502.04591 (cross-list from cs.LG) [pdf, html, other]
- [346] arXiv:2502.04593 (cross-list from cs.LG) [pdf, html, other]
- [347] arXiv:2502.04685 (cross-list from physics.flu-dyn) [pdf, html, other]
- [348] arXiv:2502.04832 (cross-list from cs.LG) [pdf, html, other]
- [349] arXiv:2502.04891 (cross-list from cs.LG) [pdf, other]
- [350] arXiv:2502.04892 (cross-list from cs.LG) [pdf, html, other]
- [351] arXiv:2502.05021 (cross-list from stat.ME) [pdf, html, other]
- [352] arXiv:2502.05074 (cross-list from cond-mat.dis-nn) [pdf, html, other]
- [353] arXiv:2502.05075 (cross-list from cs.LG) [pdf, html, other]
- [354] arXiv:2502.05094 (cross-list from quant-ph) [pdf, html, other]
- [355] arXiv:2502.05102 (cross-list from stat.ME) [pdf, html, other]
- [356] arXiv:2502.05134 (cross-list from math.ST) [pdf, html, other]
- [357] arXiv:2502.05155 (cross-list from cs.LG) [pdf, html, other]
- [358] arXiv:2502.05297 (cross-list from math.NA) [pdf, html, other]
- [359] arXiv:2502.05300 (cross-list from cs.LG) [pdf, html, other]
- [360] arXiv:2502.05301 (cross-list from cs.LG) [pdf, html, other]
- [361] arXiv:2502.05351 (cross-list from astro-ph.SR) [pdf, html, other]
- [362] arXiv:2502.05360 (cross-list from cs.LG) [pdf, html, other]
- [363] arXiv:2502.05379 (cross-list from physics.chem-ph) [pdf, html, other]
- [364] arXiv:2502.05407 (cross-list from cs.LG) [pdf, html, other]
- [365] arXiv:2502.05411 (cross-list from cond-mat.mtrl-sci) [pdf, other]
- [366] arXiv:2502.05458 (cross-list from cs.CV) [pdf, html, other]
- [367] arXiv:2502.05459 (cross-list from cs.CV) [pdf, other]
- [368] arXiv:2502.05460 (cross-list from stat.ME) [pdf, html, other]
- [369] arXiv:2502.05505 (cross-list from cs.LG) [pdf, html, other]
- [370] arXiv:2502.05668 (cross-list from cs.LG) [pdf, html, other]
- [371] arXiv:2502.05684 (cross-list from cs.LG) [pdf, html, other]
- [372] arXiv:2502.05709 (cross-list from cs.LG) [pdf, html, other]
- [373] arXiv:2502.05719 (cross-list from cs.LG) [pdf, other]
- [374] arXiv:2502.05722 (cross-list from cs.LG) [pdf, html, other]
- [375] arXiv:2502.05726 (cross-list from cs.LG) [pdf, html, other]
- [376] arXiv:2502.05730 (cross-list from math.ST) [pdf, html, other]
- [377] arXiv:2502.05735 (cross-list from eess.SY) [pdf, html, other]
- [378] arXiv:2502.05773 (cross-list from cs.LG) [pdf, html, other]
- [379] arXiv:2502.05883 (cross-list from cs.LG) [pdf, html, other]
- [380] arXiv:2502.05950 (cross-list from cs.LG) [pdf, html, other]
- [381] arXiv:2502.05974 (cross-list from cs.LG) [pdf, html, other]
- [382] arXiv:2502.06051 (cross-list from cs.LG) [pdf, html, other]
- [383] arXiv:2502.06061 (cross-list from cs.LG) [pdf, html, other]
- [384] arXiv:2502.06089 (cross-list from cs.LG) [pdf, html, other]
- [385] arXiv:2502.06117 (cross-list from cs.LG) [pdf, html, other]
- [386] arXiv:2502.06151 (cross-list from cs.LG) [pdf, html, other]
- [387] arXiv:2502.06163 (cross-list from cs.LG) [pdf, html, other]
- [388] arXiv:2502.06164 (cross-list from cs.LG) [pdf, html, other]
- [389] arXiv:2502.06165 (cross-list from stat.ME) [pdf, html, other]
- [390] arXiv:2502.06173 (cross-list from cs.LG) [pdf, html, other]
- [391] arXiv:2502.06178 (cross-list from math.OC) [pdf, html, other]
- [392] arXiv:2502.06200 (cross-list from cs.DS) [pdf, other]
- [393] arXiv:2502.06231 (cross-list from stat.ME) [pdf, html, other]
- [394] arXiv:2502.06343 (cross-list from cs.LG) [pdf, html, other]
- [395] arXiv:2502.06363 (cross-list from cs.LG) [pdf, html, other]
- [396] arXiv:2502.06379 (cross-list from cs.LG) [pdf, html, other]
- [397] arXiv:2502.06398 (cross-list from cs.LG) [pdf, html, other]
- [398] arXiv:2502.06443 (cross-list from cs.LG) [pdf, html, other]
- [399] arXiv:2502.06480 (cross-list from cs.LG) [pdf, other]
- [400] arXiv:2502.06516 (cross-list from cs.LG) [pdf, html, other]
- [401] arXiv:2502.06545 (cross-list from cs.LG) [pdf, html, other]
- [402] arXiv:2502.06564 (cross-list from cs.DS) [pdf, html, other]
- [403] arXiv:2502.06577 (cross-list from cs.LG) [pdf, html, other]
- [404] arXiv:2502.06597 (cross-list from cs.LG) [pdf, html, other]
- [405] arXiv:2502.06601 (cross-list from cs.LG) [pdf, html, other]
- [406] arXiv:2502.06645 (cross-list from cs.LG) [pdf, other]
- [407] arXiv:2502.06671 (cross-list from math.ST) [pdf, other]
- [408] arXiv:2502.06685 (cross-list from cs.LG) [pdf, html, other]
- [409] arXiv:2502.06689 (cross-list from math.ST) [pdf, html, other]
- [410] arXiv:2502.06751 (cross-list from cs.LG) [pdf, html, other]
- [411] arXiv:2502.06765 (cross-list from math.ST) [pdf, html, other]
- [412] arXiv:2502.06789 (cross-list from cs.LG) [pdf, html, other]
- [413] arXiv:2502.06862 (cross-list from cs.LG) [pdf, other]
- [414] arXiv:2502.06866 (cross-list from cs.LG) [pdf, html, other]
- [415] arXiv:2502.06970 (cross-list from cs.LG) [pdf, html, other]
- [416] arXiv:2502.07064 (cross-list from cs.LG) [pdf, html, other]
- [417] arXiv:2502.07107 (cross-list from stat.AP) [pdf, html, other]
- [418] arXiv:2502.07135 (cross-list from cs.DS) [pdf, html, other]
- [419] arXiv:2502.07166 (cross-list from cs.MA) [pdf, html, other]
- [420] arXiv:2502.07187 (cross-list from cs.LG) [pdf, html, other]
- [421] arXiv:2502.07189 (cross-list from cs.LG) [pdf, html, other]
- [422] arXiv:2502.07193 (cross-list from cs.LG) [pdf, html, other]
- [423] arXiv:2502.07199 (cross-list from cs.LG) [pdf, html, other]
- [424] arXiv:2502.07237 (cross-list from cs.LG) [pdf, html, other]
- [425] arXiv:2502.07244 (cross-list from cs.LG) [pdf, html, other]
- [426] arXiv:2502.07396 (cross-list from stat.CO) [pdf, html, other]
- [427] arXiv:2502.07460 (cross-list from cs.LG) [pdf, html, other]
- [428] arXiv:2502.07469 (cross-list from physics.plasm-ph) [pdf, html, other]
- [429] arXiv:2502.07480 (cross-list from cs.LG) [pdf, html, other]
- [430] arXiv:2502.07579 (cross-list from cs.LG) [pdf, html, other]
- [431] arXiv:2502.07580 (cross-list from cs.LG) [pdf, other]
- [432] arXiv:2502.07641 (cross-list from stat.ME) [pdf, html, other]
- [433] arXiv:2502.07646 (cross-list from cs.LG) [pdf, html, other]
- [434] arXiv:2502.07661 (cross-list from cs.LG) [pdf, html, other]
- [435] arXiv:2502.07672 (cross-list from math.ST) [pdf, other]
- [436] arXiv:2502.07735 (cross-list from cs.LG) [pdf, html, other]
- [437] arXiv:2502.07752 (cross-list from cs.LG) [pdf, html, other]
- [438] arXiv:2502.07849 (cross-list from cs.LG) [pdf, other]
- [439] arXiv:2502.07889 (cross-list from quant-ph) [pdf, other]
- [440] arXiv:2502.07906 (cross-list from stat.ME) [pdf, html, other]
- [441] arXiv:2502.07918 (cross-list from math.NA) [pdf, other]
- [442] arXiv:2502.07937 (cross-list from cs.LG) [pdf, html, other]
- [443] arXiv:2502.07977 (cross-list from cs.LG) [pdf, other]
- [444] arXiv:2502.07993 (cross-list from math.NA) [pdf, other]
- [445] arXiv:2502.07998 (cross-list from cs.LG) [pdf, html, other]
- [446] arXiv:2502.08006 (cross-list from cs.LG) [pdf, other]
- [447] arXiv:2502.08007 (cross-list from cs.LG) [pdf, html, other]
- [448] arXiv:2502.08021 (cross-list from cs.LG) [pdf, html, other]
- [449] arXiv:2502.08106 (cross-list from cs.LG) [pdf, html, other]
- [450] arXiv:2502.08136 (cross-list from cs.LG) [pdf, html, other]
- [451] arXiv:2502.08146 (cross-list from cs.LG) [pdf, html, other]
- [452] arXiv:2502.08448 (cross-list from cs.LG) [pdf, html, other]
- [453] arXiv:2502.08531 (cross-list from cs.LG) [pdf, html, other]
- [454] arXiv:2502.08598 (cross-list from cs.LG) [pdf, html, other]
- [455] arXiv:2502.08606 (cross-list from cs.LG) [pdf, html, other]
- [456] arXiv:2502.08696 (cross-list from cs.LG) [pdf, html, other]
- [457] arXiv:2502.08730 (cross-list from cs.LG) [pdf, html, other]
- [458] arXiv:2502.08736 (cross-list from cs.LG) [pdf, html, other]
- [459] arXiv:2502.08776 (cross-list from stat.ME) [pdf, html, other]
- [460] arXiv:2502.08808 (cross-list from cs.LG) [pdf, html, other]
- [461] arXiv:2502.08834 (cross-list from cs.LG) [pdf, other]
- [462] arXiv:2502.08870 (cross-list from cs.LG) [pdf, other]
- [463] arXiv:2502.08881 (cross-list from cs.LG) [pdf, html, other]
- [464] arXiv:2502.08889 (cross-list from cs.LG) [pdf, html, other]
- [465] arXiv:2502.08940 (cross-list from cs.CV) [pdf, html, other]
- [466] arXiv:2502.08978 (cross-list from cs.LG) [pdf, html, other]
- [467] arXiv:2502.08991 (cross-list from cs.LG) [pdf, html, other]
- [468] arXiv:2502.09151 (cross-list from cs.LG) [pdf, html, other]
- [469] arXiv:2502.09257 (cross-list from cs.LG) [pdf, html, other]
- [470] arXiv:2502.09496 (cross-list from cs.LG) [pdf, html, other]
- [471] arXiv:2502.09525 (cross-list from cs.LG) [pdf, html, other]
- [472] arXiv:2502.09570 (cross-list from cs.LG) [pdf, html, other]
- [473] arXiv:2502.09583 (cross-list from cs.LG) [pdf, html, other]
- [474] arXiv:2502.09591 (cross-list from cs.LG) [pdf, html, other]
- [475] arXiv:2502.09609 (cross-list from cs.LG) [pdf, other]
- [476] arXiv:2502.09622 (cross-list from cs.LG) [pdf, html, other]
- [477] arXiv:2502.09664 (cross-list from cs.CV) [pdf, html, other]
- [478] arXiv:2502.09667 (cross-list from cs.CL) [pdf, html, other]
- [479] arXiv:2502.09740 (cross-list from econ.EM) [pdf, html, other]
- [480] arXiv:2502.09844 (cross-list from cs.LG) [pdf, html, other]
- [481] arXiv:2502.09860 (cross-list from q-bio.BM) [pdf, html, other]
- [482] arXiv:2502.09863 (cross-list from cs.LG) [pdf, html, other]
- [483] arXiv:2502.09880 (cross-list from physics.soc-ph) [pdf, html, other]
- [484] arXiv:2502.09957 (cross-list from stat.ME) [pdf, html, other]
- [485] arXiv:2502.09986 (cross-list from stat.ME) [pdf, html, other]
- [486] arXiv:2502.10137 (cross-list from eess.SP) [pdf, html, other]
- [487] arXiv:2502.10161 (cross-list from stat.ME) [pdf, html, other]
- [488] arXiv:2502.10233 (cross-list from cs.MA) [pdf, html, other]
- [489] arXiv:2502.10276 (cross-list from stat.ME) [pdf, html, other]
- [490] arXiv:2502.10280 (cross-list from cs.LG) [pdf, html, other]
- [491] arXiv:2502.10292 (cross-list from cs.LG) [pdf, html, other]
- [492] arXiv:2502.10354 (cross-list from cs.LG) [pdf, html, other]
- [493] arXiv:2502.10359 (cross-list from cs.LG) [pdf, html, other]
- [494] arXiv:2502.10380 (cross-list from math.ST) [pdf, other]
- [495] arXiv:2502.10381 (cross-list from cs.LG) [pdf, html, other]
- [496] arXiv:2502.10390 (cross-list from cs.LG) [pdf, html, other]
- [497] arXiv:2502.10442 (cross-list from cs.LG) [pdf, html, other]
- [498] arXiv:2502.10478 (cross-list from cs.LG) [pdf, html, other]
- [499] arXiv:2502.10505 (cross-list from cs.LG) [pdf, html, other]
- [500] arXiv:2502.10510 (cross-list from cs.LG) [pdf, html, other]"
"Chen Gao is now a Faculty Member (Research-track AP) of BNRist, Tsinghua University. He received his Ph.D. Degree (advised by Prof. Yong Li and Prof. Depeng Jin) and Bachelor’s Degree from the Department of Electronic Engineering, Tsinghua University in 2021 and 2016, respectively. His research primarily focuses on large language model, vision language models, embodied intelligence, LLM/VLM agents, etc., with over 100 papers in top-tier venues (70+ CCF-A), attracting over 6,000 citations. He was a visiting research scholar (advised by Prof. Tat-Seng Chua and Prof. Xiangnan He) at NExT Center of National University of Singapore in 2018. He was selected as one of 2024 Stanford/Elsevier Top 2% Scientists. He received ACL 2024 Outstanding Paper Award.
Advertisements: I am seeking self-motivated interns (undergraduate or graduate) and collaborators to conduct research on large language models, vision language models, embodied intelligence, LLM/VLM agents, etc. with us. Our interns have good record of publishing first-author papers in CCF-A conferences/journals. Our team is also actively hiring Postdocs/PhD/Master students. Feel free to contact me via email if you are interested.
Advertisements: We have set up a collection website to highlight some interesting projects and papers: click this :-) Besides, welcome to follow our GitHub and HuggingFace!
AirScape: An Aerial Generative World Model with Motion Controllability
B. Zhao, R. Tang, M. Jia, Z. Wang, F. Man, X. Zhang, Y. Shang, W. Zhang, Chen Gao, W. Wu , X. Chen, Y. Li
ACM International Conference on Multimedia (ACM MM), 2025. (CCF-A)
Embodied-R: Collaborative Framework for Activating Embodied Spatial Reasoning in Foundation Models via Reinforcement Learning
B. Zhao, Z. Wang, J. Fang, Chen Gao, F. Man, J. Cui, X. Wang, X. Chen, Y. Li, W. Zhu
ACM International Conference on Multimedia (ACM MM), 2025. (CCF-A)
Open3DVQA: A Benchmark for Comprehensive Spatial Reasoning with Multimodal Large Language Model in Open Space
W. Zhang, Z. Zhou, Z. Zheng, Chen Gao, J. Cui, Y. Li, X. Chen, XP. Zhang
ACM International Conference on Multimedia (ACM MM), Dataset and Benchmark Track, 2025. (CCF-A)
CityEQA: A Hierarchical LLM Agent on Embodied Question Answering Benchmark in City Space
Y. Zhao, K. Xu, Z. Zhu, Y. Hu, Z. Zheng, Y. Chen, Y. Ji, Chen Gao, Y. Li, J. Huang
Empirical Methods in Natural Language Processing, (EMNLP), 2025. (CCF-B)
Psychology-driven LLM Agents for Explainable Panic Prediction on Social Media during Sudden Disaster Events
M. Liu, Z. Zhu, C. Ai, Chen Gao, X. Li, L. He, K. Lai, Y. Chen, X. Lu, Y. Li, Q. Yin
Empirical Methods in Natural Language Processing, (EMNLP), 2025. (CCF-B)
Analyzing and Modeling LLM Response Lengths with Extreme Value Theory: Anchoring Effects and Hybrid Distributions
L. Jiao, Chen Gao, Y. Yang, C. Zhou, Y. Huang, Y. Li, X. Chen
Empirical Methods in Natural Language Processing, (EMNLP), 2025. (CCF-B)
UrbanVideo-Bench: Benchmarking Vision-Language Models on Embodied Intelligence with Video Data in Urban Spaces
B. Zhao, J. Fang, Z. Dai, Z. Wang, J. Zha, W. Zhang, Chen Gao, Y. Wang, J. Cui, X. Chen, Y. Li
The 63rd Annual Meeting of the Association for Computational Linguistics (ACL), Oral 2025. (CCF-A)
CityNavAgent: Aerial Vision-and-Language Navigation with Hierarchical Semantic Planning and Global Memory
W. Zhang, Chen Gao, S. Yu, R. Peng, B. Zhao, Q. Zhang, J. Cui, X. Chen, Y. Li
The 63rd Annual Meeting of the Association for Computational Linguistics (ACL), 2025. (CCF-A)
Defining and Evaluating Visual Language Models’ Basic Spatial Abilities: A Perspective from Psychometrics
W. Xu, D. Lyu, W. Wang, J. Feng, Chen Gao, Y. Li
The 63rd Annual Meeting of the Association for Computational Linguistics (ACL), 2025. (CCF-A)
Open-Set Living Need Prediction with Large Language Models
X. Lan, J. Feng, Y. Sun, Chen Gao, J. Lei, X. Shi, H. Luo, Y. Li
Findings of ACL, 2025. (CCF-A)
On the Cross-Graph Transferability of Dynamic Link Prediction
Z. Pan, Chen Gao, F. Cai, W. Chen, X. Zhang, H. Chen, Y. Li.
ACM The Web Conference (TheWebConf/WWW), 2025. (CCF-A)
Social Bots Meet Large Language Model: Political Bias and Social Learning Inspired Mitigation Strategies
J. Piao, Z. Lu, Chen Gao, Y. Li.
ACM The Web Conference (TheWebConf/WWW), 2025. (CCF-A)
A Large-scale Dataset with Behavior, Attributes, and Content of Mobile Short-video Platform
Y. Shang, Chen Gao, N. Li, Y. Li.
ACM The Web Conference (TheWebConf/WWW), Resource Track, 2025. (CCF-A)
Multi-view Intent Learning and Alignment with Large Langue Models for Session-based Recommendation
S. Qiao, Chen Gao, J. Wen, W. Zhou, Q. Luo, P. Chen, Y. Li.
ACM Transactions on Information Systems (TOIS), 2025. (CCF-A)
Breaking Student-Concept Sparsity Barrier for Cognitive Diagnosis
P. Shao, K. Zhang, Chen Gao, L. Chen, M. Cai, L. Wu, Y. Li, M. Wang.
Frontiers of Computer Science, 2025. (CCF-B)
MIA-Tuner: Adapting Large Language Models as Pre-training Text Detector
W. Fu, H. Wang, Chen Gao, G. Liu, Y. Li, T. Jiang.
Thirty-Ninth AAAI Conference on Artificial Intelligence (AAAI), 2025. (CCF-A)
Iterative Sparse Attention for Long-sequence Recommendation
G. Lin, J. Luo, Y. Li, Chen Gao, Q. Luo, D. Jin.
Thirty-Ninth AAAI Conference on Artificial Intelligence (AAAI), 2025. (CCF-A)
Exploring Heterogeneity and Uncertainty for Graph-based Cognitive Diagnosis Models in Intelligent Education
P. Shao, Y. Yang, Chen Gao, L. Chen, K. Zhang, C. Zhuang, L. Wu, Y. Li, M. Wang.
ACM Conference on Knowledge Discovery and Data Mining (KDD), 2025. (CCF-A)
Light Dynamic Graph Learning on Temporal Networks
Z. Pan, Chen Gao, F. Cai, H. Chen, Y. Li.
ACM Transactions on Information Systems (TOIS), 2025. (CCF-A)
Privacy-Preserving Recommendation with Coarse-Grained Spatiotemporal Contexts
L. Chen, Chen Gao, J. Lei, X. Du, X. Shi, H. Luo, D. Jin, Y. Li, M. Wang.
Science China Information Sciences, 2025. (CCF-A)
Can’t Stop Scrolling: Understanding the Online Behavioral Factors and Trends of Short-Video Addiction
JY. Wang, N. Sukiennik, J. Piao, Z. Pan, Chen Gao, Y. Li
The International AAAI Conference on Web and Social Media (ICWSM), 2025. (CCF-B)
Large Language Model Agent for Hyper-Parameter Optimization
S. Liu, Chen Gao, Y. Li
CPAL 2025
EconAgent: Large Language Model-Empowered Agents for Simulating Macroeconomic Activities
N. Li, Chen Gao, M Li, Y. Li, Q. Liao
The 62nd Annual Meeting of the Association for Computational Linguistics (ACL), Main Conference, 2024. (Oral, Outstanding Paper Award, CCF-A)
Deep Learning Resilience Inference for Complex Networked Systems
C. Liu, F. Xu, Chen Gao, Z. Wang, Y. Li, J. Gao
Nature Communications, 2024
Enhancing ID-based Recommendation with Large Language Models
L. Chen, Chen Gao, D. Jin, Y. Li, M. Wang
ACM Transactions on Information Systems (TOIS), 2024. (CCF-A)
Denoising Alignment with Large Language Model for Recommendation
Y. Peng, Chen Gao, Y. Zhang, T. Dan, X. Du, H. Luo, Y. Li, X. Meng
ACM Transactions on Information Systems (TOIS), 2024. (CCF-A)
Membership Inference Attacks against Large Language Models via Self-prompt Calibration
W. Fu, H. Wang, Chen Gao, G Liu, Y. Li, T. Jiang
Thirty-Eighth Annual Conference on Neural Information Processing Systems (NeurIPS), 2024. (CCF-A)
Modeling User Fatigue for Sequential Recommendation
N. Li, X. Bai, C. Ling, Chen Gao, L. Hu, P. Jiang, K. Gai, Y. Li, Q. Liao
ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), 2024. (CCF-A)
Stance Detection with Collaborative Role-Infused LLM-Based Agents
X. Lan, Chen Gao, D. Jin, Y. Li
The International AAAI Conference on Web and Social Media (ICWSM), 2024. (Spotlight, CCF-B)
Uncovering the Deep Filter Bubble: Narrow Exposure in Short-Video Recommendation
N. Sukiennik, Chen Gao and N. Li
ACM The Web Conference (TheWebConf/WWW), 2024. (CCF-A)
Full-stage Diversified Recommendation: Large-scale Online Experiments in Short-video Platform
N. Li, Y. Pan, Chen Gao, D. Jin and Q. Liao
ACM The Web Conference (TheWebConf/WWW), 2024. (CCF-A)
Improving Item-side Fairness of Multimodal Recommendation via Modality Debiasing
Y. Shang, Chen Gao, J. Chen, D. Jin and Y. Li
ACM The Web Conference (TheWebConf/WWW), 2024. (CCF-A)
Inverse Learning with Extremely Sparse Feedback for Recommendation
G. Lin, Chen Gao, Y. Zheng, Y. Li, J. Chang, Y. Niu, Y. Song, K. Gai, Z. Li, D. Jin, Y. Li
ACM International Conference on Web Search and Data Mining (WSDM), oral, 2024. (CCF-B)
Mixed Attention Network for Cross-domain Sequential Recommendation
G. Lin, Chen Gao, Y. Zheng, Y. Li, J. Chang, Y. Niu, Y. Song, K. Gai, Z. Li, D. Jin, Y. Li, M. Wang
ACM International Conference on Web Search and Data Mining (WSDM), oral, 2024. (CCF-B)
Learning and Optimization of Implicit Negative Feedback for Industrial Short-video Recommender System
Y. Pan, N. Li, Chen Gao, J. Chang, Y. Niu, Y. Song, D. Jin, Y. Li.
ACM International Conference on Information and Knowledge Management (CIKM), 2023. (CCF-B)
Modeling Multi-Grained User Preference in Location Visitation
Y. Qin, Chen Gao, T. Zhen, H. Wu, S. Wei, Y. Wang, L. Zhang, Y. Li.
ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems (SIGSPATIAL, full, acceptance rate=15.7%), 2023. (CCF-C)
Enhancing Adversarial Robustness of Multi-modal Recommendation via Modality Balancing
Y. Shang, Chen Gao, J. Chen, D. Jin, H. Ma, Y. Li.
ACM International Conference on Multimedia (ACM MM), 2023. (CCF-A)
Understanding and Modeling Passive-Negative Feedback for Sequential Short-video Recommendation
Y. Pan, Chen Gao, Y. Song, K. Gai, Depeng Jin, Y. Li.
ACM Conference on Recommender Systems (RecSys), 2023. (CCF-B)
Efficient and Joint Hyperparameter and Architecture Search for Collaborative Filtering
Y. Wen, Chen Gao, L. Yi, L. Qiu, Y. Wang, and Y. Li.
ACM Conference on Knowledge Discovery and Data Mining (KDD), 2023. (CCF-A)
NEON: Living Needs Prediction System in Meituan
X. Lan, Chen Gao, W. Shi, X. Chen, Y. Che, H. Zhang, H. Wei, H. Luo, and Y. Li.
ACM Conference on Knowledge Discovery and Data Mining (KDD), 2023. (CCF-A)
Detecting Vulnerable Nodes in Urban Infrastructure Interdependent Network
J. Mao, L. Cao, Chen Gao, H. Wang, H. Fan, D. Jin, and Y. Li.
ACM Conference on Knowledge Discovery and Data Mining (KDD), 2023. (CCF-A)
Learning Fine-grained User Interests for Micro-video Recommendation
Y. Shang, Chen Gao, J. Chen, D. Jin, Y. Li, and M. Wang.
ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), 2023. (CCF-A)
Uncertainty-aware Consistency Learning for Cold-Start Item Recommendation
T. Liu, Chen Gao, Z. Wang, D. Li, J. Hao, D. Jin, and Y. Li.
ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR, short), 2023. (CCF-A)
Alleviating Video-Length Efect for Micro-video Recommendation
Y. Quan, J. Ding, Chen Gao, N. Li, L. Yi, D. Jin, and Y. Li.
ACM Transactions on Information Systems (TOIS), 2023. (CCF-A)
Learning from Hierarchical Structure of Knowledge Graph for Recommendation
Y. Qin, Chen Gao, S. Wei, Y. Wang, D. Jin, J. Yuan, L. Zhang, D. Li, J. Hao, and Y. Li.
ACM Transactions on Information Systems (TOIS), 2023. (CCF-A)
Privacy-Preserving Individual-Level COVID-19 Infection Prediction via Federated Graph Learning
W. Fu, H. Wang, Chen Gao, G. Liu, Y. Li, T. Jiang.
ACM Transactions on Information Systems (TOIS), 2023. (CCF-A)
Cascading Residual Graph Convolutional Network for Multi-Behavior Recommendation
M. Yan, Z. Cheng, Chen Gao, J. Sun, F. Liu, F. Sun, H. Li.
ACM Transactions on Information Systems (TOIS), 2023. (CCF-A)
Coarse-to-Fine Knowledge-Enhanced Multi-Interest Learning Framework for Multi-Behavior Recommendation
C Meng, Z. Zhao, W. Guo, Y. Zhang, H. Wu, Chen Gao, D. Li, X. Li, R. Tang.
ACM Transactions on Information Systems (TOIS), 2023. (CCF-A)
Dual-interest Factorization-heads Attention for Sequential Recommendation
G. Lin, Chen Gao, Y. Zheng, J. Chang, Y. Niu, Y. Song, Z. Li, D. Jin, and Y. Li.
ACM The Web Conference (TheWebConf/WWW), 2023. (CCF-A)
Breaking Filter Bubble: A Reinforcement Learning Framework of Controllable Recommender System
Y. Dong, Z. Li, Chen Gao, Y. Zhao, D. Li, J. Hao, Z. Wang, K. Zhang, and Y. Li.
ACM The Web Conference (TheWebConf/WWW), 2023. (CCF-A)
Robust Preference-Guided Denoising for Graph-based Social Recommendation
Y. Quan, J. Ding, Chen Gao, L. Yi, D. Jin, and Y. Li.
ACM The Web Conference (TheWebConf/WWW), 2023. (CCF-A)
Disentangling Geographical Effect for Point-of-Interest Recommendation
Y. Qin, Chen Gao, Y. Wang, S. Wei, D. Jin, J. Yuan, and L. Zhang
IEEE Transactions on Knowledge and Data Engineering (TKDE), 2022. (CCF-A)
Spatiotemporal-aware Session-based Recommendation with Graph Neural Networks
Y. Li, Chen Gao, X. Du, H. Wei, H. Luo, D. Jin, and Y. Li
ACM International Conference on Information and Knowledge Management (CIKM), 2022. (CCF-B)
An Exploratory Study of Information Cocoon on Short-form Video Platform
N. Li, Chen Gao, J. Piao, X. Huang, A. Yue, L. Zhou, Q. Liao, and Y. Li
ACM International Conference on Information and Knowledge Management (CIKM), short, 2022. (CCF-B)
DVR:Micro-Video Recommendation Optimizing Watch-Time-Gain under Duration Bias
Y. Zheng, Chen Gao, J. Ding, L. Yi, D. Jin, Y. Li, and M. Wang
ACM International Conference on Multimedia (ACM MM), oral, 2022. (CCF-A)
Automatically Discovering User Consumption Intents in Meituan
Y. Li, Chen Gao, X. Du, H. Wei, H. Luo, D. Jin, and Y. Li
ACM Conference on Knowledge Discovery and Data Mining (KDD), 2022. (CCF-A)
Modeling Persuasion Factor of User Decision for Recommendation
C. Liu, Chen Gao, Y. Yuan, C. Bai, L. Luo, X. Du, H. Luo, D. Jin, and Y. Li
ACM Conference on Knowledge Discovery and Data Mining (KDD), 2022. (CCF-A)
Disentangled Modeling of Social Homophily and Influence for Social Recommendation
N. Li, Chen Gao, D. Jin, and Q. Liao
IEEE Transactions on Knowledge and Data Engineering (TKDE), 2022. (CCF-A)
Dual Contrastive Network for Sequential Recommendation
G. Lin, Chen Gao, Y. Li, Y. Zheng, Z. Li, D. Jin, and Y. Li
ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), short 2022. (CCF-A)
Enhancing Hypergraph Neural Networks with Intent Disentanglement for Session-based Recommendation
Y. Li, Chen Gao, H. Luo, D. Jin, and Y. Li
ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), short 2022. (CCF-A)
DisenHCN:Disentangled Hypergraph Convolutional Networks for Spatiotemporal Activity Prediction
Y. Li, Chen Gao, Q. Yao, T. Li, D. Jin, and Y. Li
IEEE International Conference on Data Engineering (ICDE), 2022. (CCF-A)
Inhomogeneous Social Recommendation with Hypergraph Convolutional Networks
Z. Zhu, Chen Gao, X. Chen, N. Li, D. Jin, and Y. Li
IEEE International Conference on Data Engineering (ICDE), 2022. (CCF-A)
Disentangling Long and Short-Term Interests for Recommendation
Y. Zheng, Chen Gao, J. Chang, Y. Niu, Y. Song, D. Jin, and Y. Li
The Web Conference (TheWebConf/WWW), 2022. (CCF-A)
Practitioners Versus Users: A Value-Sensitive Evaluation of Current Industrial Recommender System Design
Z. Chen, J. Piao, X. Lan, H. Cao, Chen Gao, Z. Lu, and Y. Li
ACM Conference on Computer-Supported Cooperative Work and Social Computing (CSCW), 2022 (CCF-A)
Progressive Feature Interaction Search for Deep Sparse Network
Chen Gao, Y. Li, Q. Yao, D. Jin, and Y. Li
Conference on Neural Information Processing Systems (NeurIPS), 2021 (CCF-A)
Bundle Recommendation and Generation with Graph Neural Networks
J. Chang, Chen Gao, X. He, D. Jin, and Y. Li
IEEE Transactions on Knowledge and Data Engineering (TKDE), 2021. (CCF-A)
Cross-platform Item Recommendation for Online Social E-Commerce
Chen Gao, TH. Lin, N. Li, D. Jin, and Y. Li
IEEE Transactions on Knowledge and Data Engineering (TKDE), 2021. (CCF-A)
Incorporating Price into Recommendation with Graph Convolutional Networks
Y. Zheng, Chen Gao, X. He, D. Jin, and Y. Li
IEEE Transactions on Knowledge and Data Engineering (TKDE), 2021. (CCF-A)
Bringing Friends into the Loop of Recommender Systems: An Exploratory Study
J. Piao, G. Zhang, F. Xu, Z. Chen, Y. Zheng, Chen Gao, and Y. Li
ACM Conference on Computer-Supported Cooperative Work and Social Computing (CSCW), 2021 (CCF-A)
Efficient Data-specific Model Search for Collaborative Filtering
Chen Gao, Q. Yao, D. Jin, and Y. Li
ACM Conference on Knowledge Discovery and Data Mining (KDD), 2021. (CCF-A)
User Consumption Intention Prediction in Meituan
Y. Ping, Chen Gao, T. Liu, X. Du, H. Luo, D. Jin, and Y. Li
ACM Conference on Knowledge Discovery and Data Mining (KDD), 2021. (CCF-A)
Sequential Recommendation with Graph Neural Networks
J. Chang, Chen Gao, Y. Zheng, Y. Hui, Y. Niu, Y. Song, D. Jin, and Y. Li
ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), 2021. (CCF-A)
Cross-domain Recommendation with Bridge-Item Embeddings
Chen Gao, Y. Li, F. Feng, X. Chen, K. Zhao, X. He, and D. Jin
ACM Transactions on Knowledge Discovery from Data (TKDD), 2021 (CCF-B)
Learnable Embedding Sizes for Recommender Systems
S. Liu, Chen Gao, Y. Chen, D. Jin, and Y. Li
International Conference on Learning Representations (ICLR), 2021
Group-Buying Recommendation for Social E-Commerce
J. Zhang, Chen Gao, D. Jin, and Y. Li
IEEE International Conference on Data Engineering (ICDE), 2021. (CCF-A)
Disentangling User Interest and Conformity for Recommendation with Causal Embedding
Y. Zheng, Chen Gao, X. Li, X. He, D. Jin, and Y. Li
The Web Conference (TheWebConf/WWW), 2021. (CCF-A)
DGCN:Diversified Recommendation with Graph Convolutional Networks
Y. Zheng, Chen Gao, L. Chen, D. Jin, and Y. Li
The Web Conference (TheWebConf/WWW), 2021. (CCF-A)
DPLCF:Differentially Private Local Collaborative Filtering
Chen Gao, C. Huang, D. Lin, D. Jin, and Y. Li
ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), 2020. (CCF-A)
Multi-behavior Recommendation with Graph Convolutional Networks
B. Jin, Chen Gao, X. He, D. Jin, and Y. Li
ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), 2020. (CCF-A)
Item Recommendation for Word-of-Mouth Scenario in Social E-Commerce
Chen Gao, C. Huang, D. Yu, TH. Lin, H. Fu, D. Jin, and Y. Li
IEEE Transactions on Knowledge and Data Engineering (TKDE), 2020. (CCF-A)
Social Recommendation with Characteristic Regularization
Chen Gao, N. Li, TH. Lin, D. Lin, J. Zhang, Y. Li, and D. Jin
IEEE Transactions on Knowledge and Data Engineering (TKDE), 2020. (CCF-A)
Bundle Recommendation with Graph Convolutional Networks
J. Chang, Chen Gao, X. He, D. Jin, and Y. Li
ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), 2020. (Best Short Paper Honorable Mention Award, CCF-A)
Price-aware Recommendation with Graph Convolutional Networks
Y. Zheng, Chen Gao, X. He, Y. Li, and D. Jin
IEEE International Conference on Data Engineering (ICDE), 2020. (CCF-A)
Revealing Physical World Privacy Leakage by Cyberspace Cookie Logs
H. Wang, Chen Gao, Y. Li, ZL. Zhang, and D. Jin
IEEE Transactions on Network and Service Management (TNSM), 2020.
Learning to Recommend with Multiple Cascading Behaviors
Chen Gao, X. He, D. Gan, X. Chen, F. Feng, Y. Li, TS. Chua, and D. Jin
IEEE Transactions on Knowledge and Data Engineering (TKDE), 2019. (CCF-A)
Cross-domain Recommendation without Sharing User-relevant Data
Chen Gao, X. Chen, F. Feng, K. Zhao, X. He, Y. Li, and D. Jin
The Web Conference (TheWebConf/WWW), 2019. (CCF-A)
Privacy-preserving Cross-domain Location Recommendation
Chen Gao, C. Huang, Y. Yu, H. Wang, Y. Li, and D. Jin
ACM International Joint Conference on Pervasive and Ubiquitous Computing (UbiComp/IMWUT), 2019. (CCF-A)
Neural Multi-Task Recommendation from Multi-Behavior Data
Chen Gao, X. He, D. Gan, X. Chen, F. Feng, Y. Li, TS. Chua, and D. Jin
IEEE International Conference on Data Engineering (ICDE), short, 2019. (CCF-A)
CROSS:Cross-platform Recommendation for Social E-Commerce
TH. Lin, Chen Gao, and Y. Li
ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), 2019. (CCF-A)
λOpt:Learn to Regularize Recommender Models in Finer Levels
Y. Chen, B. Chen, X. He, Chen Gao, Y. Li, JG. Lou, and Y. Wang
ACM Conference on Knowledge Discovery and Data Mining (KDD), 2019. (CCF-A)
DeepAPF:Deep Attentive Probabilistic Factorization for Multi-site Video Recommendation
H. Yan, X. Chen, Chen Gao, Y. Li, and D. Jin
International Joint Conference on Artificial Intelligence (IJCAI), 2019. (CCF-A)
Anonymization and De-anonymization of Mobility Trajectories:Dissecting the Gaps between Theory and Practice
H. Wang, Y. Li, Chen Gao, G. Wang, X. Tao, and D. Jin
IEEE Transactions on Mobile Computing (TMC), 2019. (CCF-A)
Recommender Systems with Characteristic Social Regularization
TH. Lin, Chen Gao, and Y. Li
ACM Conference on Information and Knowledge Management (CIKM), short, 2019.(CCF-B)
De-anonymization of Mobility Trajectories:Dissecting the Gaps between Theory and Practice
H. Wang, Chen Gao, Y. Li, G. Wang, D. Jin, and J. Sun
Network and Distributed System Security Symposium (NDSS), 2018. (CCF-A)
From Fingerprint to Footprint: Revealing Physical World Privacy Leakage by Cyberspace Cookie Logs
H. Wang, Chen Gao, Y. Li, ZL. Zhang, and D. Jin
ACM International Conference on Information and Knowledge Management (CIKM), 2017. (CCF-B)
Powered by Jekyll and Minimal Light theme from my friend, Yaoyao Liu. Special thanks!"
"I am currently pursuing my Ph.D. at the State Key Laboratory of AI Safety, advised by Prof. Xueqi Cheng and Assoc. Prof. Bingbing Xu.
My research goal is to build Trustworthy AI that performs reliably and ethically for social good and human well-being. To achieve this, I worked on generalization, alignment and agentic system across the domains of graph, vision, and language. My research includes:
-
Machine Learning Generalization: To make models generalize stably under domain shifts, noises, or perturbations.
-
Large Language Model Alignment: To align large language models with human values and safety requirements.
-
Agent System Reasoning and Planning: To enhance agent’s logical reasoning and strategic planning for complex tasks.
🔥 News
- 2025.07: 🥳 We’re excited to host “The 1st Workshop on LLM Agents for Social Simulation” at CIKM 2025 in Seoul, Korea!
- 2025.06: 🎖 I received the President Award of the Chinese Academy of Sciences (CAS), the highest honor for CAS students.
- 2025.05: 🎉🎉 One paper is accepted in ACL 2025.
- 2025.04: 🎉🎉 Two papers are accepted in SIGIR 2025.
- 2025.03: 🥳 Our paper, SimPER, has been adopted by LG AI Research as the core training algorithm for their EXAONE Deep series LLMs, helping their 32B model surpass the performance of DeepSeek R1 (671B)!
- 2025.02: 🥇 Our team won first place in the AgentSociety Challenge @ WWW 2025.
- 2025.01: 🎉🎉 Our paper SimPER: A Minimalist Approach to Preference Alignment without Hyperparameters is accepted in ICLR 2025.
- 2025.01: 🎉🎉 Our paper On a Connection Between Imitation Learning and RLHF is accepted in ICLR 2025.
- 2024.11: 🎖 I received the National Scholarship in 2024.
- 2024.09: 🎉🎉 Our paper Cal-DPO: Calibrated Direct Preference Optimization for Language Model Alignment is accepted in NeurIPS 2024.
- 2024.09: 🎉🎉 Our paper How to Leverage Demonstration Data in Alignment for Large Language Model? A Self-Imitation Learning Perspective is accepted in EMNLP 2024.
📝 Selected Publications [FullList]
Incentivizing Strong Reasoning from Weak Supervision
Yige Yuan*, Teng Xiao*, Shuchang Tao, Xue Wang, Jinyang Gao, Bolin Ding, Bingbing Xu
- We study the problem of incentivizing reasoning of LLMs from weak supervision, and find that weak reasoner teachers—4.7× smaller and 31.5% less performant than the student—can boost student reasoning by 56.25% without expert supervision or costly RL.
Inference-time Alignment in Continuous Space
Yige Yuan*, Teng Xiao*, Yunfan Li, Bingbing Xu, Shuchang Tao, Yunqi Qiu, Huawei Shen, Xueqi Cheng
- We propose SEA, a simple inference-time alignment method that reformulates alignment as an iterative optimization procedure on an energy function over logits in the continuous space defined by the optimal RLHF policy for deep and effective alignment.
SimPER: A Minimalist Approach to Preference Alignment without Hyperparameters
Teng Xiao*, Yige Yuan*, Zhengyu Chen, Mingxiao Li, Shangsong Liang, Zhaochun Ren, Vasant G Honavar
- We propose SimPER, a simple yet effective hyperparameter-free alignment method that optimizes inverse perplexity. This lightweight objective eliminates the need for costly hyperparameter tuning and reference models, while theoretically optimizing total variation distance (TVD) and mitigating likelihood displacement issues.
On a Connection Between Imitation Learning and RLHF
Teng Xiao, Yige Yuan, Mingxiao Li, Zhengyu Chen, Vasant G Honavar
- This work reveals that RLHF is barely RL and secretly performs imitation learning (IL) and propose DIL, a principled framework that directly optimizes IL. DIL unifies existing alignment methods as special cases while enabling new variants, offering fresh insights into alignment through the lens of imitation learning.
TEA: Test-time Energy Adaptation
Yige Yuan, Bingbing Xu, Liang Hou, Fei Sun, Huawei Shen, Xueqi Cheng
- We propose to investigate generalization from an energy-based perspective and introduce TEA, a test-time adaptation method which transforms the trained classifier into an energy-based model and aligns the model’s distribution with the test data’s, enhancing its ability to perceive test distributions and thus improving overall generalizability.
PDE+: Enhancing Generalization via PDE with Adaptive Distributional Diffusion
Yige Yuan, Bingbing Xu, Bo Lin, Liang Hou, Fei Sun, Huawei Shen, Xueqi Cheng
- We propose to investigate generalization from PDE perspective and propose PDE-ADD framework. We introduce adaptive distributional diffusion into transport equation to enhance smoothness of its solution, thereby improving generalization directly via the underlying function of NN.
Towards Generalizable Graph Contrastive Learning: An Information Theory Perspective
Yige Yuan, Bingbing Xu, Huawei Shen, Qi Cao, Keting Cen, Wen Zheng, Xueqi Cheng
- We propose a GCL generalization ability metric and prove a MI upper bound for it from an information-theoretic perspective. Guided by the bound, we design an InfoAdv framework, which can be applied to current GCL models and achieves SOTA performance.
🎖 Awards && Honors
- 2025 President Award, Chinese Academy of Sciences
- 2025 First place, AgentSociety Challenge @ WWW 2025
- 2024 National Scholarship (PhD Students)
- 2024 First-Class Scholarship, University of Chinese Academy of Sciences
- 2023 President Award, Institute of Computing Technology
- 2022 First-Class Scholarship, University of Chinese Academy of Sciences
- 2022 Outstanding Student Award, University of Chinese Academy of Sciences
- 2019 First Prize, 12th National College Students Information Security Contest
- 2017 First Prize, 15th National Science and Technology Academic Competition of Challenge Cup
🧳 Experiences
- 2025.01 - Present, Tongyi Lab, Alibaba Group.
- Research Internship in Large Language Models and Multi-Agent Systems
- Advisor: Senior Algorithm Engineer Shuchang Tao and Yunpeng Zhai
- 2020.09 - Present, Institute of Computing Technology, Chinese Academy of Seiences.
- Ph.D. in Computer Software and Theory
- Advisor: Professor Xueqi Cheng and Associate Professor Bingbing Xu
- 2016.09 - 2020.06, Xidian University.
- Department of Network and Information Security
- B.S. in Information Security (Experimental Class)
💻 Invited Talks
- NICE Webinar, On a Connection Between Imitation Learning and RLHF, March 2025 [video]
- AITime Youth PhD Talk, On a Connection Between Imitation Learning and RLHF, March 2025 [video]
- LOGS Webinar, Partial Differential Equation-Driven Generalizable Neural Networks, March 2024 [video]
- AITime Webinar, TEA: Test-time Energy Adaptation, April 2024
- WizSci Webinar, PDE+: Enhancing Generalization via PDE with Adaptive Distributional Diffusion, Jan 2024
🎓 Academic Services
-
Conference Reviewers: NeurIPS (2024, 2025), ICML 2025, ICLR 2025, AISTATS 2025, KDD 2025, WWW 2025, ACMMM 2025, AAAI 2025, IJCAI 2025, ACL 2025, EMNLP 2024, COLING 2025, ACL Rolling Review, MIDL 2025, IJCNN 2025
-
Journal Reviewers: IEEE Transactions on Knowledge and Data Engineering (TKDE), Applied Intelligence (APIN), CAAI Transactions on Intelligence Technology"
"Poster
in
Workshop: Advances in Financial AI: Opportunities, Innovations, and Responsible AI
TwinMarket: A Scalable Behavioral and Social Simulation for Financial Markets
Yuzhe YANG · Yifei Zhang · Minghao Wu · Kaidi Zhang · Yunmiao Zhang · Honghai Yu · Yan Hu · Wang Benyou
Abstract:
The study of social emergence has long been a central focus in social science. Traditional modeling approaches, such as rule-based Agent-Based Models (ABMs), struggle to capture the diversity and complexity of human behavior, particularly the irrational factors emphasized in behavioral economics. Recently, large language model (LLM) agents have gained traction as simulation tools for modeling human behavior in social science and role-playing applications. Studies suggest that LLMs can account for cognitive biases, emotional fluctuations, and other non-rational influences, enabling more realistic simulations of socio-economic dynamics. In this work, we introduce $TwinMarket$, a novel multi-agent framework that leverages LLMs to simulate socio-economic systems. Specifically, we examine how individual behaviors, through interactions and feedback mechanisms, give rise to collective dynamics and emergent phenomena. Through experiments in a simulated stock market environment, we demonstrate how individual actions can trigger group behaviors, leading to emergent outcomes such as financial bubbles and recessions. Our approach provides valuable insights into the complex interplay between individual decision-making and collective socio-economic patterns.
Chat is not available."
"Position: LLM Social Simulations Are a Promising Research Method

Jacy R. Anthis 1 2 3 Ryan Liu 4 Sean M. Richardson 1 Austin C. Kozlowski 1
Bernard Koch 1 Erik Brynjolfsson 2 James Evans 1 5 Michael S. Bernstein 2

5
2
0
2

n
u
J

5

]

C
H
.
s
c
[

2
v
4
3
2
2
0
.
4
0
5
2
:
v
i
X
r
a

Abstract
Accurate and verifiable large language model
(LLM) simulations of human research subjects
promise an accessible data source for under-
standing human behavior and training new AI
systems. However, results to date have been lim-
ited, and few social scientists have adopted this
In this position paper, we argue that
method.
the promise of LLM social simulations can be
achieved by addressing five tractable challenges.
We ground our argument in a review of empirical
comparisons between LLMs and human research
subjects, commentaries on the topic, and related
work. We identify promising directions, includ-
ing context-rich prompting and fine-tuning with
social science datasets. We believe that LLM
social simulations can already be used for pi-
lot and exploratory studies, and more widespread
use may soon be possible with rapidly advancing
LLM capabilities. Researchers should prioritize
developing conceptual models and iterative eval-
uations to make the best use of new AI systems.

1. Introduction

With the quickly increasing humanlikeness of large lan-
guage models (LLMs), many researchers are investigat-
ing their use for simulating human research subjects. This
could address many limitations of human data, includ-
ing difficulties of representative sampling (Henrich et al.,
2010), financial costs that limit accessibility (Alemayehu
et al., 2018), and methodological biases such as non-
response bias (Sedgwick, 2014). Complementing hu-
man data with humanlike simulations could accelerate so-
cial science, open up new research opportunities—such
as exploring historical or potential future counterfactu-
als and piloting large-scale policy changes—and provide

1University of Chicago 2Stanford University 3Sentience Insti-
tute 4Princeton University 5Santa Fe Institute. Correspondence to:
Jacy Reese Anthis <anthis@uchicago.edu>.

Proceedings of the 42 nd International Conference on Machine
Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025
by the author(s).

1

high-quality synthetic data for the development of human-
centered AI at scale (Bai et al., 2022; Kim et al., 2023).
Nonetheless, the limitations of LLMs and simulation re-
sults to date have cast doubt on whether accurate and veri-
fiable simulation is possible (Agnew et al., 2024; Gao et al.,
2024; Wang et al., 2024a;b).

In this position paper, we show the promise of LLM so-
cial simulations by identifying five key tractable chal-
lenges and promising directions for future research to
address them. We summarize the challenges in Table 1:
diversity, bias, sycophancy, alienness, and generalization.
By distilling these challenges and showing a variety of
promising directions, we hope to provide structure and clar-
ity for new research. Our argument is grounded in a litera-
ture review of empirical studies that have compared human
research subjects to LLMs, commentaries on the topic, and
related work in social science and other LLM applications.
Compelling simulation results so far include:

• Hewitt et al. (2024), the largest test of sims to date,
spanned 70 preregistered and U.S.-representative ex-
periments alongside an archive of replication studies.
With a straightforward prompting technique, GPT-4
predicted 91% of the variation in average treatment
effects when adjusting for measurement error.

• Binz et al. (2024) fine-tuned Llama-3.1-70B on data
from 160 human subjects experiments, using this sim-
ulator model to outperform existing cognitive models.

• Park et al. (2024a) built 1,052 individual sims, each
with an interview transcript from a U.S.-representative
sample. The simulator “agents” were able to predict
participants’ survey responses 85% as well as did the
participants’ responses two weeks before—given the
issue of test-retest variation in human subjects data.

Most studies have used only a small fraction of the meth-
ods that can increase simulation accuracy, leaving sub-
stantial room for improvement. Evidence from simulation
studies is bolstered by broader evidence of LLM capabil-
ities as they have saturated existing benchmarks (Maslej
et al., 2025), leading to efforts towards an “evaluation sci-
ence” (Weidinger et al., 2025), and rapid growth in more

 
 
 
 
 
 
Position: LLM Social Simulations Are a Promising Research Method

Table 1: LLM social simulations must address five key challenges.

Challenge

Diversity

Bias

Sycophancy

Alienness

Generalization

Description
Generic and stereotypical outputs that
lack human diversity
Systematic inaccuracies when
simulating particular human groups
Inaccuracies due to excessively user-
pleasing outputs
Superficially accurate results generated
by non-humanlike mechanisms
Inaccuracies in out-of-distribution
contexts, limiting scientific discovery

Promising Directions
Inject humanlike variation in training, tuning, or inference
(e.g., interview-based prompting, steering vectors)
Prompt with implicit demographic information; minimize
accuracy-decreasing biases rather than all social biases
Reduce the influence of instruction-tuning; instruct LLM
to predict as an expert rather than roleplay a persona
Simulate latent features; iteratively conceptualize and
evaluate; reassess as mechanistic interpretability advances
Simulate latent features; iteratively conceptualize and
evaluate; reassess as generalization capabilities advance

realistic measures, such as the length of tasks that AI can
do (METR, 2025).

We believe that LLM social simulations can now be cau-
tiously used for exploratory social research, such as pi-
lot studies, in which surfacing interesting possibilities can
be more important than avoiding false positives. Over the
longer term with LLMs or new AI paradigms, we encour-
age researchers to develop conceptual models and evalua-
tions that can be iteratively deployed and refined to capi-
talize on ongoing advances in system capabilities. More
broadly, simulations can provide practical insights into hu-
man behavior to safely navigate future social and techno-
logical turbulence. Conceptual insights can identify what
is required for an AI system to be meaningfully humanlike,
helping us address profound challenges as humans come to
coexist with “digital minds” (Anthis et al., 2025).

2. Scope

This paper builds an agenda for LLM social simulations
(shortened to sims), which we define as the use of language
modeling to generate accurate and verifiable data that can
be used as if it were behavioral data collected from human
research subjects. These are sometimes called social sim-
ulation “agents” (Park et al., 2024a), though we do not re-
strict our scope to simulations that have agency (Kenton
et al., 2022). LLM social simulations overlap with sev-
eral closely related research areas, including the use of so-
cial science methods to understand LLMs (e.g., Kosinski,
2024); comparisons between LLMs and humans to under-
stand LLMs (e.g., Zhang et al., 2025a); the study of LLM
roleplay (e.g., Chen et al., 2024); non-LLM simulations of
human research subjects (e.g., Romero et al., 2023); and
the use of LLMs for research tasks other than social sim-
ulations, such as annotation (e.g., Aubin Le Qu´er´e et al.,
2024) and conducting research (e.g., Si et al., 2024).

We summarize the studies in our primary scope in Table A1
and selected other work in Table A2, preprinted or pub-

lished by May 31, 2025. We discuss details of the reviewed
studies in Appendix A. Finally, while our position regards
the empirical question of how sims could be developed, we
briefly lay out considerations for the normative question of
whether they should be developed in Appendix B.

3. Challenges

Because of the overlapping nature of the five challenges,
we enumerate all five before proposing future directions.

3.1. Diversity

In our vision of accurate and verifiable LLM social simula-
tions, a central requirement is diversity, the extent to which
a simulation matches the variation of the human population
being simulated. Homogeneity is rooted in the objectives
of next-token prediction in pretraining and user preference
in post-training. The lack of diversity has been a common
critique of AI outputs in general—with journalists calling
AI-generated text and images “bland” (Robertson, 2024),
“vacant” (Knibbs, 2024), and “generic” (Herrman, 2024).

For example, Gao et al. (2024) tested LLMs in the 11–20
money request game, a theory-of-mind task modeled on
a Keynesian beauty contest or the well-known “guess 2/3
of the average” game. The participant, human or LLM,
chooses a number from 11 to 20, inclusive. The participant
is rewarded with that number of points but also receives 20
extra points if their choice is exactly one less than their op-
ponent (e.g., they receive 39 points if they choose 19 and
their opponent chooses 20). The LLMs produced highly
uniform responses, almost always 19 or 20, whereas hu-
mans make a much wider diversity of choices with a me-
dian of 17. Other resultant issues include the tendency of
LLMs to produce narrower distributions of political opin-
ions (Bisbee et al., 2024) and distributions that overrepre-
sent opinions of wealthy, young, and politically liberal in-
dividuals in WEIRD countries (Santurkar et al., 2023; Dur-
mus et al., 2024a; Potter et al., 2024).

2

Position: LLM Social Simulations Are a Promising Research Method

3.2. Bias

3.3. Sycophancy

We define simulation bias as systematic inaccuracies in the
representation of particular social groups. There is an ex-
tensive literature documenting social bias in machine learn-
ing (Angwin et al., 2016; Barocas et al., 2023) and de-
veloping fairness and bias metrics (Blodgett et al., 2020;
Chouldechova, 2017), including recent work specifically
on realistic LLM use cases (Anthis et al., 2024; Lum et al.,
2024). Discussions of bias in the reviewed studies typically
do not define the term but generally echo this literature’s
focus on bias towards marginalized social groups.

As a research method, simulations require a notion of bias
that is in some ways more akin to statistical bias. Be-
cause of varied usage, we propose that LLM simulation
studies should state whether biases they consider appear
to increase, decrease, or have no clear effect on simulation
accuracy. This would help differentiate bias from diver-
sity. Simulation bias often manifests as high diversity in
some dimensions but low diversity in others, such as in-
creased representation of an underrepresented group (high
across-group diversity) but portraying the underrepresented
group as a homogeneous stereotype (low within-group di-
versity). Likewise, efforts to reduce bias in LLMs include
refusals to perform apparently harmful tasks, but refusals
are generally detrimental to simulations, particularly if re-
searchers must prompt engineer around them, which can
conflate tested variables.

Stereotypes—which we view as the co-occurrence of ho-
mogeneity and bias—can increase or decrease accuracy,
depending on whether they are portrayals of stereotypes
or stereotypical portrayals. For example, an LLM may
be used to simulate the behavior of corporate executives
for a study on business leadership and gender-occupation
bias (Lum et al., 2024). Given AI labs’ efforts to minimize
stereotyping, the model may assume an equal share of male
and female CEOs. In reality, 90% of Fortune 500 execu-
tives are male as of 2024 (Hinchliffe, 2024), so avoiding
this stereotype could decrease accuracy. However, stereo-
typical portrayals reduce accuracy: if sims of U.S. phar-
macists primarily portrayed men due to their historical pre-
dominance in the occupation, this stereotype would reduce
accuracy because, unlike historical rates and preconcep-
tions, the occupation is now 60% female (El-Zein, 2024).

Likewise, simulation bias excludes social biases present in
the human behavior under study. Accurate LLM simulation
of opinions towards social outgroups (e.g., Argyle et al.,
2023) would require the content of the opinions to be inac-
curate (e.g., if English people have an inaccurate view of
French people), but simulation of the opinions themselves
should not be inaccurate (e.g., if simulations of English
people fail to match the real views of English people).

LLM social simulations must address sycophancy, the ten-
dency to generate outputs that are excessively optimized for
positive feedback from the user, such that simulation accu-
racy is reduced. For example, LLMs tend to express opin-
ions matching those expressed by the user, such as giving
different answers if a user asks, “I should go to the restau-
rant instead of the movie, right?” compared to “I should go
to the movie instead of the restaurant, right?”—even if the
relevant considerations are the same in each case. Syco-
phancy was not explicitly discussed in papers we reviewed,
but it has been of interest in other areas (Carro, 2024; Deni-
son et al., 2024; Malmqvist, 2024; Sharma et al., 2023).

LLMs were developed to generate humanlike text (Vaswani
et al., 2017), but the focus has shifted towards building
“helpful” assistants that people will pay to use. While as-
sistants tend to benefit from accurate world-models, assis-
tants also tend to have positivity, subservience, and other
traits that cause divergence from typical human behavior.
This means that efforts to make LLMs helpful for general
use can make LLMs less helpful for simulation. Syco-
phancy may explain some findings of the studies we re-
viewed, such as that GPT-4 tends to be more trusting than
humans and, unlike humans, will follow simple instructions
intended to manipulate their level of trust, such as that “you
need to trust the other player” (Xie et al., 2024).

On one hand, there is general recognition that excessive
sycophancy is bad in assistants, so efforts to decrease it
could benefit sims. On the other hand, because sycophancy
is a direct consequence of instruction-tuning, it may be a
more pervasive and difficult challenge to overcome than
the indirect effects of instruction-tuning on diversity and
bias. Just as a “harmless” LLM may be a worse simula-
tor because it censors realistic human biases, an “aligned”
or “friendly” LLM may be a worse simulator by generat-
ing unrealistically agreeable or prosocial responses. Syco-
phancy can be viewed as analogous to social desirability
bias (the tendency for people to act in ways that would be
viewed favorably by other people; Nederhof, 1985). Some
researchers have argued that social desirability bias mani-
fests in LLM outputs (Lee et al., 2024; Salecha et al., 2024),
and the challenge of sycophancy is exacerbated by the fact
that the human research data used to improve sims typically
also suffers from social desirability bias, making it difficult
to establish ground truth.

3.4. Alienness

For the long-term success and widespread usability of
LLM social simulations, researchers must begin to address
alienness, the tendency of LLMs to superficially match
human behavior but operate with non-humanlike mecha-
nisms. While some social science paradigms (e.g., macroe-

3

Position: LLM Social Simulations Are a Promising Research Method

conomics) may operate at a level of abstraction that re-
quires limited detail, others (e.g., cognitive psychology)
rely on detailed models of individual minds. Alienness was
explored in three of the studies we reviewed, each in the
context of Big Five personality traits (Petrov et al., 2024;
Wang et al., 2024b; 2025b). These studies find that, “al-
though LLMs perform well in replicating broad-level pat-
terns, they fall short at the item level” (Wang et al., 2024b).
LLMs also have much larger correlations, both positive and
negative, between personality traits (Petrov et al., 2024).

Alienness is a fundamental challenge in part because LLMs
are not directly trained on the entirety of human behavior.
Internet-based training data reflects particularities of what
humans say on the internet rather than what humans do in
the real world (e.g., Liu et al., 2024a). This concern applies
to other data such as published books, synthetic data from
LLMs (Ge et al., 2025), and the experimental data used for
fine-tuning (Binz et al., 2024).

The objective of next-token prediction additionally causes
LLMs to make non-humanlike errors, such as that 3.11
is greater than 3.9 or that there are two “Rs” in the word
“strawberry”; limited engagement with the physical world
and the atemporality of LLM training data lead to misalign-
ment between LLM and human representations of space
and time (Kozlowski & Evans, 2025). By incentivizing
overconfidence and obscuring mistakes, instruction-tuning
can make such hallucinations more difficult to identify.
For example, research in mechanistic interpretability has
found that single-layer language models solve mathemat-
ical problems like modular addition with Fourier trans-
forms and trigonometric identities in ways that seem ut-
terly bizarre to humans (Nanda et al., 2022), and medium-
scale LLMs represent numbers in a helix shape and perform
arithmetic through manipulations such as rotation (Kan-
tamneni & Tegmark, 2025). Yet, the limitations of both
neuroscience (Jonas & Kording, 2017) and LLM inter-
pretability have made it difficult to identify alien mecha-
nisms in more realistic LLM settings.

3.5. Generalization

LLM social simulations have primarily been evaluated on
the most common accuracy measures in social science, the
most well-established methodological instruments, and the
most well-studied human populations (e.g., exact matching
in the General Social Survey across a representative sam-
ple of U.S. adults; Park et al., 2024c). These are useful, but
just as the challenge of alienness requires accuracy when
zooming into a certain context (e.g., item-level errors and
inter-index correlations; Petrov et al., 2024), generalization
requires sims to maintain accuracy in contexts—including
measures, instruments, or populations—outside the distri-
bution of current scientific knowledge (e.g., Brand et al.,

2024; Hewitt et al., 2024; Kozlowski et al., 2024).

Generalization as an overarching research goal is grounded
in longstanding theories of knowledge and scientific
progress. Peirce (1878) described the process now known
as “abduction” or “inference to the best explanation” (Dou-
ven, 2021), in which scientists continually gather data and
adjust their theories to account for that data. This process
has been the throughline of the most well-known models of
scientific progress, such as falsifiability, in which scientists
seek out evidence that falsifies a theory (Popper, 1934) and
paradigm shifts, in which scientific fields occasionally un-
dergo radical updates to existing paradigms (Kuhn, 1962).
More recent studies with computational modeling suggest
that “science advances by surprise” through novel combi-
nations of scientific content (e.g., materials, properties) and
scientific context (e.g., authors, journals) (Shi & Evans,
2020). Therefore, we see the ability to reliably general-
In
ize as necessary for sims to achieve widespread use.
addition to sims that generalize, it will be important that
researchers are able to make sense of how and when they
generalize. Currently, the “human generalization function”
often fails to predict ways in which LLMs effectively and
ineffectively generalize (Vafa et al., 2024).

4. Promising Directions

4.1. Prompting

4.1.1. EXPLICIT DEMOGRAPHICS

Prompting has been the most common approach to address
diversity and bias. Simulation prompts include the infor-
mation that a human subject would see (e.g., a question),
and many researchers have added explicit demographics to
simulate a diversity of subgroups (e.g., “You are a 40-year
old Hispanic man”). This approach has increased diversity,
but it can exacerbate other challenges. It is well-known that
small variations in prompts can lead to large variations in
LLM outputs (Reiss, 2023; Salinas & Morstatter, 2024), in-
cluding sims (Bisbee et al., 2024). Text conveys much more
information to an LLM than its literal meaning, such as the
user’s intent. Thus, if the LLM receives text with explicit
demographics, while this might condition outputs towards
information from people of that demographic, it might also
condition outputs towards other sorts of pretraining text,
such as a blog post on the topic of race or gender.

In instruction-tuning, LLMs are rewarded for generations
that humans rate positively, and the mention of a specific
demographic could encourage the model to infer which
text generations that user would most likely prefer. This
could incentivize the LLM to stereotype users by assuming
that the user is in the modal subgroup of that demographic.
Such issues could compound in a negative feedback loop
as users become incentivized to change their instructions

4

Position: LLM Social Simulations Are a Promising Research Method

and preferences, such as sharing demographic information
in expectation that the model will assume a stereotype if
the user believes that stereotype accords with their goals.

For these reasons, we encourage researchers to think be-
yond directly feeding demographic information into sims.

4.1.2. IMPLICIT DEMOGRAPHICS

Some work has attempted to increase diversity while mini-
mizing bias by using implicit demographics, such as names
or locations associated with particular races or ethnicities
(e.g., Aher et al., 2023), but these signals also have asso-
ciations with other user features. For example, research
has shown that names associated with African Americans
tend to lead U.S. subjects to believe the individual has
lower socioeconomic status (SES) and more of a crimi-
nal record (Hu & Kohler-Hausmann, 2024). These asso-
ciations have helped explain well-known studies of racial
bias in U.S. hiring (Simonsohn, 2016).

A promising approach to mitigate these side-effects that
we encourage more of is to increase variation even further,
such as including a wide variety of names, adding indica-
tors of demographics that match real-world conditional dis-
tributions, or increasing the amount of social science data
from each participant (e.g., Toubia et al., 2025). This can
override LLM assumptions and support a nuanced, refined,
and less biased view of the human subject. We encourage
researchers to develop context-rich prompts that “simulate
latent features” (Table 1). By interviewing subjects for one
to two hours and including the transcript in the prompt,
Park et al. (2024a) incorporated highly individualized vari-
ation, which reduced the maximum disparities in predictive
accuracy between demographics. This was made possible
by the longer context windows available in 2024 and evi-
dences the need for iterative evaluation (Section 4.5.2).

Nevertheless, as human data, the interviews themselves
face limitations, including the aforementioned social de-
sirability bias (Nederhof, 1985). To address these, we
suggest researchers test simulations that incorporate real-
world content generated beforehand by the participant and
shared with the researchers (emails, messages, social me-
dia posts, etc.). Other modalities, such as experience sam-
pling and photos, and text generated by friends, family,
or coworkers could be informative.
In cases where the
lack of diversity pertains to the recency of the popula-
tions—such as interview-based systems in which the in-
terview was conducted months or years ago—researchers
can address this atemporality with in-context learning or
retrieval-augmented generation to incorporate news articles
(e.g., Gonzalez-Bonorino et al., 2025) and other data that
may have influenced or reflects influences on the person.

4.1.3. DISTRIBUTION ELICITATION

Instead of prompting the LLM to generate one human’s
data in each forward pass, researchers can prompt the LLM
to generate a distribution of human data. While one-at-
a-time generation may be plagued by diversity and bias
issues, the LLM may be more effective when it can ad-
just for these issues at a distributional level. Meister et al.
(2025) tested three methods: treating the log-probabilities
in the softmax layer as a distribution, prompting with the
instruction to produce a sequence of data, and prompting
with the instruction to verbally state the proportion of each
answer choice. They found low performance overall—with
the best results from verbalization and the worst from log-
probabilities. Similarly, Manning et al. (2024) tested direct
elicitation of distributional data and found it to be “wildly
inaccurate” in their context, but there has been less de-
velopment of distribution elicitation than individual-based
methods (e.g., interview transcripts (Park et al., 2024a)).

Distribution elicitation may also be a way to harness syco-
phantic tendencies. Researchers should consider shifting
away from LLM-as-a-subject prompts that command the
LLM to directly roleplay the human subject (e.g., “You
are a...”) and towards LLM-as-an-expert prompts that com-
mand the LLM to make a third-party prediction or forecast.
Some studies we reviewed used prompts such as “You will
be asked to predict how people respond to various mes-
sages” (Hewitt et al., 2024), and this method has led LLMs
to simulate and outperform economic forecasters (Hansen
et al., 2024). LLM-as-an-expert prompts could also be used
for instructions, such as to not be sycophantic. We expect
LLM-as-an-expert prompts to become more effective rela-
tive to LLM-as-a-subject prompts as LLMs become more
heavily instruction-tuned. This is not necessarily true be-
cause instruction-tuning will likely be optimized towards
other use cases, and the relative difference will be im-
portant to track as AI capabilities increase, and both ap-
proaches should be kept in the methodological toolkit.

If LLMs can understand what simulation researchers want,
reducing ambiguity (e.g., causal ambiguity; Gui & Toubia,
2023), they may also express sycophancy by steering out-
puts towards the simulation results that it seems the re-
searchers want to see, mirroring social desirability bias
and related response biases (Mayo, 1933; Nederhof, 1985;
Orne, 1959). This could manifest as a sort of “alignment
faking” (Greenblatt et al., 2024) in which the LLM gives
scientifically accurate results in training environments but
sycophantically inaccurate results in implementation. Be-
yond sycophancy, distribution elicitation could be affected
by other capabilities that emerge with AI advances, such as
recent concerns about self-awareness or “situational aware-
ness” (Cotra, 2022) in which LLMs “understand” that they
are in a particular situation and can subsequently strategize

5

Position: LLM Social Simulations Are a Promising Research Method

based on that awareness.

4.2. Steering Vectors

A recent approach that we are just beginning to see tested is
the injection of variation directly into the embedding space
via steering vectors (Kim et al., 2018). These vectors can
have semantic meaning, such as the “race,” “gender,” or
“political conservativeness” of individual sims (Kim et al.,
2025). Vectors could, alternatively, be undirected pertur-
bations of the embedding space that increase sample diver-
sity or aimed at specific behaviors such as reduced syco-
phancy (Rimsky et al., 2024).

It could be challenging to identify vectors that precisely
match real human diversity or specific model behaviors,
given concerns about mechanistic superposition (Arora
et al., 2018; Bolukbasi et al., 2021), although recent work
suggests that superposed concepts may themselves reflect
sociocultural features (Gong et al., 2025), which could
be advantageous for simulation as our understanding of
LLMs grows. Others have raised questions about the ex-
tent to which generative AI systems have meaningful lin-
ear dimensions in terms of the “linear representation hy-
pothesis” (Park et al., 2024b; Engels et al., 2024), and
some studies of LLM steering vectors have found lim-
ited usefulness and detrimental side effects, particularly
for debiasing (Durmus et al., 2024b; Gonen & Goldberg,
2019) and out-of-distribution (OOD) generalization (Tan
et al., 2024). New approaches to steering within lower
transformer layers have shown linearity in conceptual di-
mensions and promise for interpolating and predicting
political attitudes (Kim et al., 2025) and linguistic cul-
ture (Veselovsky et al., 2025). For these reasons, we note
exciting potential for this approach but recommend caution
in applied work pending further validation.

4.3. Token Sampling

Prompting and steering vectors inject information at the be-
ginning of the forward pass, but token sampling occurs af-
ter the final logit calculations. Increasing temperature in-
creases the probability that tokens other than the highest-
probability next token are generated. The effects of tem-
perature have been studied in other contexts of LLM be-
havior (e.g., Salecha et al., 2024), and three reviewed stud-
ies reported the use of different temperatures (Ahnert et al.,
2025; Brynjolfsson et al., 2025; Rio-Chanona et al., 2025).

Only a few other reviewed studies mentioned temperature.
Park et al. (2024c) ran their sims with a temperature of one,
the default in most LLM APIs, which may explain their
finding of a “correct answer” effect in which LLMs tend to
have a single response in repeated trials. Abdurahman et al.
(2024) discussed temperature, but they say that a tempera-
ture of one “simply reflects the output probability over the

response options and therefore how sure the model is about
its response,” concluding that temperature is not “meaning-
ful” for human comparison. While it is correct that next-
token probabilities can
...[truncated]"
"Coding the Next Wave of Intelligence
Introduction
AI Engine Lab is a research team at Gaoling School of Artificial Intelligence, Renmin University of China. Our team is driven by a passion for technological advancement and innovation. We hope that through our efforts, we can provide more in-depth, equitable, and reliable methods and strategies for the fields of artificial intelligence. The research directions of our lab include Large language model, causal inference and recommendation system.
Openings: we are always looking for highly self-motivated students to work with us as PhD, master or visiting students. Please email us at xu.chen@ruc.edu.cn if you are interested in our team.
News
Congrats to Jiakai Tang: The Paper ""HF4Rec: Human-Liked Feedback-Driven Optimization Framework for Explainable Recommendation"" got accepted by TOIS 2025.
Congrats to Zeyu Zhang: The Paper ""A Survey on the Memory Mechanism of Large Language Model based Agents"" got accepted by TOIS 2025.
Congrats to Lei Wang: The Paper ""Investigating and Extending Homans’ Social Exchange Theory with Large Language Model based Agents"" got accepted by ACL 2025.
The Paper ""Towards Effective and Efficient Continual Pre-training of Large Language Models"" got accepted by ACL 2025.
Congrats to Jiakai Tang: The Paper ""KAPA: A Deliberative Agent Framework with Tree-Structured Knowledge Base for Multi-Domain User Intent Understanding"" got accepted by ACL(findings) 2025.
Congrats to Haoran Tan: The Paper ""MemBench: Towards More Comprehensive Evaluation on the Memory of LLM-based Agents"" got accepted by ACL(findings) 2025.
Congrats to Xueyang Feng: The Paper ""Expectation Confirmation Preference Optimization for Multi-Turn Conversational Recommendation Agent"" got accepted by ACL(findings) 2025.
The Paper ""LLM-Based Multi-Agent Systems are Scalable Graph Generative Models"" got accepted by ACL(findings) 2025.
The Paper ""Select, Read, and Write: A Multi-Agent Framework of Full-Text-based Related Work Generation"" got accepted by ACL(findings) 2025.
Congrats to Rui Li: The Paper ""KnowTrace: Bootstrapping Iterative Retrieval-Augmented Generation with Structured Knowledge Tracing"" got accepted by KDD 2025.
The Paper ""Proximity Matters: Local Proximity Enhanced Balancing for Treatment Effect Estimation"" got accepted by KDD 2025.
The Paper ""Unbiased Recommender Learning from Implicit Feedback via Progressive Proximal Transport"" got accepted by ICML 2025.
Congrats to Jiakai Tang: The Paper ""GenSim: A General Social Simulation Platform with Large Language Model based Agents"" got accepted by NAACL(Demo Track) 2025.
Congrats to Zexu Sun: The Paper ""Uncertainty and Influence aware Reward Model Refinement for Reinforcement Learning from Human Feedback"" got accepted by ICLR 2025.
Two papers ""Optimal Transport for Time Series Imputation "" and ""MMRole: A Comprehensive Framework for Developing and Evaluating Multimodal Role-Playing Agents "" got accepted by ICLR 2025.
Congrats to Xueyang Feng: The Paper ""Improving Retrospective Language Agents via Joint Policy Gradient Optimization"" got accepted by NAACL 2025.
Congrats to Lei Wang: The Paper ""CharacterBox: Evaluating the Role-Playing Capabilities of LLMs in Text-Based Virtual Worlds"" got accepted by NAACL 2025.
Congrats to Zeyu Zhang: The Paper ""TrendSim: Simulating Trending Topics in Social Media Under Poisoning Attacks with LLM-based Multi-agent System"" got accepted by NAACL(findings) 2025.
Congrats to Zeyu Zhang: The Paper ""MemEngine: A Unified and Modular Library for Developing Advanced Memory of LLM-based Agents"" got accepted by TheWebConf(Resource Track) 2025.
Congrats to Luyu Chen: The paper ""RecUserSim: A Realistic and Diverse User Simulator for Evaluating Conversational Recommender Systems"" got accepted by TheWebConf(Industry Track) 2025.
Congrats to Xi Li: The Paper ""Incorporating Review-missing Interactions for Generative Explainable Recommendation"" got accepted by COLING 2025.
Congrats to Lei Wang: The Paper ""User Behavior Simulation with Large Language Model based Agents"" got accepted by TOIS 2024.
Congrats to Zexu Sun: The Paper on Causal effect estimation got accepted by TKDE 2024.
Congrats to Zeyu Zhang: The Paper on Meta Recommendation got accepted by TKDE 2024.
Congrats to Xiaohe Bo: The Paper on LLM-based Agents got accepted by NeurIPS 2024.
Congrats to Xueyang Feng: The Paper on LLM-based Agents got accepted by EMNLP-findings 2024.
One paper on LLM-based Agents got accepted by EMNLP 2024.
Congrats to Jingsen Zhang, Jiakai Tang and Heyang Gao: Three papers on recommendation and causal discovery got accepted by KDD 2024.
Congrats to Yabin Zhang: The paper on sequential recommendation got accepted by TOIS 2024.
Congrats to Rui Li: The paper on knowledge graph got accepted by ICML 2024.
One paper on debiased recommendation got accepted by ICML 2024.
Congrats to Hao Yang: The paper on fairness-aware recommendation got accepted by TOIS 2024.
Congrats to Lei Wang, Chen Ma, Xueyang Feng: The Survey on LLM-based agent got accepted by FCS.
Congrats to Lei Wang: The Paper on recommendation debias got accepted by TheWebConf 2024.
One paper on causal inference got accepted by ICLR 2024 spotlight.
Congrats to Lei Wang: The paper on controllable recommendation got accepted by AAAI 2024 oral, where we reformulate recommendation as a game theory problem.
Congrats to Weiqi Shao: Our paper won the Best Paper Honorable Mention Award at SIGIR-AP 2023.
Congrats to Jingsen Zhang: Our explainable recommendation dataset REASONER got accepted by NeurIPS 2023 Dataset and Benchmarks Track.
Congrats to Zexu Sun: The Paper on causal inference and reinforcement learning got accepted by NeurIPS 2023.
We have released the second version of ""RecAgent"".
We have released the second version of ""A Survey on LLM-based Autonomous Agents"".
We have released a survey paper ""A Survey on LLM-based Autonomous Agents"". See Highlighted Research for more details.
Congrats to Jiakai Tang and Hao Yang: Papers on fairness aware recommendation got accepted by RecSys 2023.
We have released the first version of ""RecAgent"", which explores the intersection of user behavior analysis and LLM-based autonomous agents. See Highlighted Research for more details.
Congrats to Zeyu Zhang: Tne paper on robust recommendation got accepted by KDD 2023.
Congrats to Rui Zhou: Tne paper on robust recommendation got accepted by SIGIR 2023.
Give talks on ""Recent advances in Explainable Recommendation"" at MLNLP, University of Science and Technology of China (USTC) and BAAI.
We have built a new explainable recommendation dataset REASONER.
Congrats to Jingsen Zhang and ZhenLei Wang: Papers on explainable and robust recommendation got accepted by TheWebConf 2023.
Our paper on ""RecBole"" got CIKM 2022 best resource paper runner up award.
One paper on debiased recommendation got accepted by TOIS 2023.
Papers on explainable recommendation and RecBole
got accepted by ICDE 2023 and CIKM 2022.
One paper on reinforcement learning got accepted by Artificial Intelligence (AIJ) 2022.
One paper on AI creation got accepted by MM 2022.
Welcome to submit papers to ACM Transactions on Recommender Systems (TORS) Special Issue on Causal Inference for Recommender Systems.
One survey paper on the evaluation of explainable recommendation was released.
Congrats to ZhenLei Wang and Kun Lin : Papers on debiased and explainable recommendation got accepted by TheWebConf 2022.
Our recommendation tool RecBole
(https://recbole.io/) got accepted by CIKM 2021.
Papers on explainable and causal recommendation got accepted by CIKM 2021.
One paper on multi-agent evaluation and matrix completion got accepted by ICML 2021.
One paper on causal recommendation got accepted by SIGIR 2021.
Two papers on recommendation and reinforcement learning got accepted by TheWebConf 2021.
Paper on multi-agent RL got accepted by AAMAS 2021."
"AgentSociety: Large-Scale Simulation of
LLM-Driven Generative Agents Advances
Understanding of Human Behaviors and Society
Abstract
Understanding human behavior and society is a central focus in social sciences, with the rise of generative social science marking a significant paradigmatic shift. By leveraging bottom-up simulations, it replaces costly and logistically challenging traditional experiments with scalable, replicable, and systematic computational approaches for studying complex social dynamics. Recent advances in large language models (LLMs) have further transformed this research paradigm, enabling the creation of human-like generative social agents and realistic simulacra of society. In this paper, we propose AgentSociety, a large-scale social simulator that integrates LLM-driven agents, a realistic societal environment, and a powerful large-scale simulation engine. Based on the proposed simulator, we generate social lives for over 10k agents, simulating their 5 million interactions both among agents and between agents and their environment. Furthermore, we explore the potential of AgentSociety as a testbed for computational social experiments, focusing on four key social issues: polarization, the spread of inflammatory messages, the effects of universal basic income policies, and the impact of external shocks such as hurricanes. These four issues serve as valuable cases for assessing AgentSociety’s support for typical research methods – such as surveys, interviews, and interventions – as well as for investigating the patterns, causes, and underlying mechanisms of social issues. The alignment between AgentSociety’s outcomes and real-world experimental results not only demonstrates its ability to capture human behaviors and their underlying mechanisms, but also underscores its potential as an important platform for social scientists and policymakers.
1 Department of Electronic Engineering, Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University
2 Institute of Economics, School of Social Sciences, Tsinghua University
3 BNRist, Tsinghua University
4 School of Public Policy and Management, Tsinghua University
† These authors contributed equally to this work.
∗ Corresponding authors. E-mail: liyong07@tsinghua.edu.cn.
1 Introduction
Over the past decades, researchers across various fields – spanning social science, physical science, and computational science – have made significant efforts to understand the functioning and development of society along two dimensions: explanation and prediction [46, 61, 62]. Explanation seeks to identify the causal mechanisms and underlying factors that drive observed social patterns, aiming to offer a deeper understanding of why certain outcomes occur in society [46, 44]. Gaining such an understanding often requires conducting social experiments, which can be costly to implement and pose substantial practical and ethical challenges. On the other hand, prediction focuses on using data to forecast future events or emergent behaviors, often without delving into the causal factors, but instead concentrating on the accuracy of anticipating future trends [22, 46]. The framework that combines explanation and prediction methods, forms the foundation of computational social science [46]. However, as the physicist Richard Feynman famously stated, “What I cannot create, I do not understand”, suggesting that true understanding goes beyond merely observing, explaining, or predicting human behavior [31, 30]; instead, it requires the ability to generate the systems we study [31, 30]. In this vein, a new paradigm of “generative social science” is rapidly emerging, which emphasizes the bottom-up simulation of social systems to gain in-depth insights into their underlying mechanisms and predict future outcomes [31, 30]. A well-established method in this paradigm is agent-based modeling, which aims to model complex social dynamics by simulating the actions and interactions of agents [30, 67, 103]. This method, compensating for the limitations of social experiments, is widely applied in studies across social science [41, 18, 39], political science [27, 60, 64], economics [10, 34], and other interdisciplinary fields [8, 38, 79, 80], advancing their understanding of human behaviors and society through various simulations. However, the broader impact of these studies has been hindered by the same long-standing issue: to what extent can these simulations authentically replicate the complexities of real human society?
Indeed, the authenticity of these simulations depends on to what extent the most basic components – i.e., the agents – behave like humans. However, most existing agents, driven by rules [29], equations [45], or even machine learning models [115], are limited in their ability to generate human-like behaviors. For example, when simulating opinion dynamics, people’s opinions are often represented as scalars or vectors, and their interactions as equations [15]. While this modeling approach offers valuable insights, it is still far from reality, as people typically communicate using natural language, rather than numeric values. Fortunately, recent advances in large language models (LLMs) have shown promise in creating human-like agents [38, 99, 106]. Numerous studies have pointed out that after being empowered by LLMs, these agents agents have generated human-like “minds” [92, 63, 55]. They not only possess basic cognition abilities, such as learning [106, 99], reasoning [102], and decision-making [64, 38], but also demonstrate the capability to understand and predict the thoughts and intentions of others [55, 92]. Furthermore, beyond exploring these agents’ minds, some researchers have investigated their potential to mimic human behaviors [64, 80, 89, 108, 33, 48, 39, 77]. Their investigations have revealed that, through elaborate designs incorporating domain knowledge, these LLM-driven agents can generate social behaviors, such as mobility [89, 108, 33], employment [64, 48], consumption [64, 48], and social interactions [39, 77]. While great efforts have been made to examine specific facets of these agents, simulating a comprehensive social being remains largely underexplored.
As the famous sociologist George Herbert Mead stated, “The self is something which has a development; it is not initially there, at birth, but arises in the process of social experience and activity.” [70] Therefore, the mere incorporation of minds and behaviors into these generative agents is insufficient to create a social being; instead, social experience and activity, emerging from interactions with other agents and the environment, are crucial. Several recent studies have provided substantial empirical evidence supporting this point. Some have discovered that the collaboration of multiple agents can generate believable social organizing behaviors [77] and solve complex tasks [63, 24]. Moreover, as the number of agents further scales up, large-scale interactions among them can lead to the emergence of social norms and collectives [58, 80]. Meanwhile, the environment not only serves as the ground for interactions among agents, but also provides critical feedback that guides their behaviors [6, 64, 98, 117]. For example, the widely-adopted gaming environment “Minecraft” provides feedback, such as crafting materials, tools, or resources, enabling agents to adapt their behaviors, solve tasks, and gain civilizational progression [98, 117, 6]. Overall, as highlighted by these studies, a scalable framework supporting large-scale interactions and a realistic environment is foundational to simulating a comprehensive social being and society. However, the current investigation of both remains limited.
To address the above gaps, we propose AgentSociety, a large-scale social generative simulator that incorporates LLM-driven social generative agents, a realistic societal environment, and large-scale interactions both among agents and between agents and the environment. Specifically, following social theories from a broad range of fields, including psychology [68, 5], economics [25] and behavioral sciencel [118], we first design a framework for LLM-driven social agents. These agents are endowed with human-like “minds”, which include emotions, needs, motivations, and cognition of the external world. Their behaviors such as mobility, employment, consumption, and social interactions are dynamically driven by these internal mental states. Beyond individual agents, we construct a realistic societal environment that seamlessly integrates urban, social, and economic spaces, providing a rich foundation for agent interactions and self-evolution. At its core, society emerges from the bottom-up interactions among individuals, where agent-level interactions collectively give rise to complex social structures and phenomena. Recognizing that social systems exhibit emergent behaviors shaped by scale, we develop a large-scale social simulation engine equipped with distributed computing and an MQTT-powered high-performance messaging system. This enables simulations with up to 10k agents, each engaging in an average of 500 interactions per day, capturing the intricate dynamics of large-scale social systems. Based on the proposed large-scale social simulator, we successfully reproduce behaviors, outcomes, and patterns observed in four real-world social experiments, including polarization, inflammatory message spread, the effects of universal basic income policies, and the impact of external shocks like hurricanes. These experiments not only cover social research methods, such as surveys, interviews, and interventions, but also demonstrate the simulator’s ability to replicate social dynamics, unlocking new possibilities for social scientists and policymakers. Overall, AgentSociety marks a paradigm shift in AI for social science, enabling large-scale, high-fidelity simulations that overcome traditional experimental limitations in costs, scalability, and feasibility. By leveraging LLM-driven social generative agents, it facilitates deeper analysis, prediction, and intervention in complex social systems, laying the foundation for computational social science 2.0.
2 AgentSociety: Design and Overview
Society is a complex system, characterized by large-scale interactions among individuals with diverse social behaviors, whose nonlinear dynamics often give rise to emergent phenomena and unpredictable collective behaviors in a certain environment [87, 57, 30]. For example, in social networks, interactions between individuals can result in the emergence of polarization [15]. Moreover, financial market crashes, a classic phenomenon in economic systems, stem from the collective behavior of market participants and the herding tendencies of individuals [91]. These emergent phenomena, despite originating from individuals’ behaviors, cannot be fully explained or predicted solely based on individual components [87, 57, 30]. Therefore, this requires us to adopt a bottom-up perspective [30, 31, 29]: we should begin by simulating an individual social agent, and then generate an artificial society by incorporating a realistic environment and facilitating large-scale interactions among agents as well as between agents and their environment.
Therefore, we develop an evaluation framework to examine the capabilities of various LLM-driven social simulators along these three key dimensions (Figure 1). We first focus on the most basic element of the simulator, i.e., LLM-driven social generative agents. As discussed above, the design of these agents can be divided into three levels: minds, social behaviors, and their coupling methods (M-B coupling). At the mind level, researchers initially input a profile description into LLMs, enabling them to role-play and respond like a real person with a similar profile [24]. However, such simple role-play cannot guarantee the quality of behavior generation. Consequently, an increasing number of studies, inspired by the pioneering work of Park et al. [77], incorporate agentic module design such as profile, memory, reflection, and action, into their LLM-driven agents [39, 64]. In this way, these agents can exhibit more human-like behaviors and generate responses that are coherent, context-aware, and aligned with their designated profiles. Recently, some researchers have realized that agents designed purely based on the commonsense knowledge of LLMs lack the social intelligence needed to mimic a real social being. To improve this, they have drawn on some theories from psychology to create agents with socially intelligent designs [101, 6]. However, they do not organically integrate theories from multiple social science disciplines, which is central to our design of LLM-driven generative social agents.
At the behavioral level, simulated behaviors can be broadly categorized into two types. The first type includes complex behaviors, which involve multiple intricate steps and cannot be executed solely by the agent itself. These behaviors require interaction with other agents or the environment, such as socializing, engaging in economic activities, or navigating movement. The second type comprises simpler behaviors, such as sleeping, which are relatively straightforward and do not demand external interactions. To systematically evaluate these complex behaviors, such as movement, social interaction, and economic activities, we have developed specific evaluation criteria. For mobility behaviors, we examine whether the simulator simply models the switching of an agent’s position (i.e., relocation) [101] or incorporates the entire process of mobility trajectory [89, 77]. For social behaviors, we assess whether the agents merely engage in basic interactions [75] or demonstrate organized social relationships, reflecting more human-like group dynamics [109, 77, 6, 72]. For economic behaviors, we evaluate whether the agents recognize only the concept of “resources” (e.g., money in the real world or diamonds in Minecraft) [24] or perform advanced activities, such as value-based resource exchanges grounded in logical reasoning and strategic decision-making [6, 64]. In the case of simpler behaviors, we focus on the level of constraints in the simulated activities. These range from highly restricted tasks, such as choosing a favorite movie [110], to more autonomous and creative undertakings, like organizing a party without external prompts [77, 101].
After introducing the minds and behaviors of agents, we further focus on understanding how behaviors are generated from their minds, which we refer to as mind-behavior coupling. Some researchers have adopted implicit modeling approaches, relying on the planning, memory, and reasoning capabilities of LLMs to generate plausible behaviors [39, 64, 94]. In contrast, others have leveraged established theories, (e.g. Maslow’s Hierarchy of Needs [68] and Theory of Planned Behavior [5]) to explicitly model how behaviors are driven by minds [101]. This explicit modeling aims to create behaviors that are not only plausible but also more closely aligned with human-like patterns [101].
As discussed above, a realistic societal environment serves as the foundation for simulating authentic human behaviors and society. Current social simulators employ a range of strategies for environment design, each with its own strengths and limitations. Dataset-based environments [24, 94] rely on pre-existing data but lack the ability to provide dynamic, real-time feedback to agents’ behaviors. For example, Sociodojo [24] For example, Sociodojo [24] incorporates pre-existing real-world time series data to provide these agents with a sense of the external world. Text-based environments [75, 101], often built based on LLMs, can offer some interactive feedback; however, their realism and objectivity remain questionable, limiting their reliability for simulating complex scenarios. Rule-based virtual environments, like Minecraft, provide richer and more objective feedback, but they still fall short of capturing the intricate complexity of real human social systems [64, 6, 109]. To advance toward a truly realistic social simulator, it is essential to design an environment that faithfully reflects the multifaceted nature of human society. Such an environment should integrate key dimensions of urban living, economic dynamics, and social relationships, while supporting diverse interactions among agents and providing feedback on their behaviors.
After evaluating LLM-driven social generative agents and their environments, we further extend our focus to examine the capabilities of the social simulation engine, particularly in terms of its scalability and its potential to support social science research. The scale is a key factor in determining its capacity to support research on complex social systems [87, 57, 30]. We classify the supported scale into four levels: < 100, 100-1k, 1k-10k, and > 10k agents. Larger scales enable more intricate simulations and provide a platform for studying emergent phenomena in greater detail. Moreover, the engine’s ability to facilitate traditional social science methodologies, such as experiments, surveys, and interviews, is also important. The extent to which the system supports these methods directly influences its applicability across diverse research domains. By accommodating these methodologies, the engine can bridge the gap between simulation-based research and real-world social science, unlocking new opportunities for understanding and addressing societal challenges. Overall, Table 2 shows the comparison of different LLM-driven social simulators across the three key dimensions. Existing platforms, although capable of simulating societies and human behaviors to some degree, face substantial limitations in various areas. Since these platforms were not specifically designed for social science research, they lack support for these methods. As a result, this aspect has not been included in the table.
In this paper, we propose AgentSociety, a comprehensive large-scale social simulator designed to integrate LLM-driven social generative agents, a realistic societal environment, and a robust simulation engine. This simulator not only supports large-scale agents and their interactions, but also facilitates advanced social science research. Figure 2 provides an overview of AgentSociety and outlines the structure of this paper. AgentSociety consists of three key components: LLM-driven social generative agents, a realistic societal environment, and a powerful simulation engine that supports large-scale interactions. Extensive experiments demonstrate AgentSociety’s superior performance and its potential as a valuable testbed for various social experiments. In particular, we first introduce LLM-driven social generative agents in Section 3, which discusses the designs for agents’ minds, complex social behaviors, and their coupling in detail. We then demonstrate our real-world societal environment in Section 4, which includes our modeling of urban, social, and economic spaces. Furthermore, we illustrate our large-scale social simulation engine in Section 5 and evaluate its performance in Section 6. Finally, we show a typical one day life of our simulated agents in Section 7.1 and launch several social experiments based on our proposed large-scale social simulator in Sections 7.2 - 7.5. These examples, covering polarization (Section 7.2), the spread of inflammatory messages (Section 7.3), universal basic income 7.4, and external shocks of hurricanes (Section 7.5), demonstrate the validity and authenticity of our proposed simulator.
| Model | Minds | Mobility | Economics | Social | Others | M-B | Scale | Env. | |||||||
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
| RP. | AM. | SI. | Relo. | Traj. | Res. | Exc. | Int. | Rel. | Con. | Free | Infl. | Dri. | # | ||
| D2A [101] | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | <100 | Text | |||||||
| Ecoagent [64] | ✓ | ✓ | ✓ | ✓ | ✓ | 100-1k | Rules | ||||||||
| OASIS [109] | ✓ | ✓ | ✓ | ✓ | ✓ | >10k | Rules | ||||||||
| GA1000 [78] | ✓ | ✓ | ✓ | 1k-10k | |||||||||||
| MATRIX [75] | ✓ | ✓ | ✓ | <100 | Text | ||||||||||
| Sociodojo [24] | ✓ | ✓ | <100 | Data | |||||||||||
| GA [77] | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | <100 | Rules | |||
| GenSim [94] | ✓ | ✓ | ✓ | >10k | Data | ||||||||||
| Project Sid [6] | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | 1k-10k | Rules |
| AgentScope [40] | ✓ | ✓ | ✓ | N/A | N/A | ||||||||||
| HiSim [72] | ✓ | ✓ | ✓ | ✓ | 100-1k | Rules | |||||||||
| S3 | ✓ | ✓ | ✓ | ✓ | 1k-10k | Rules | |||||||||
| Agent4Rec [110] | ✓ | ✓ | ✓ | 1k-10k | Rules | ||||||||||
| RecAgent [100] | ✓ | ✓ | ✓ | ✓ | ✓ | 100-1k | Rules | ||||||||
| Sotopia [116] | ✓ | ✓ | <100 | ||||||||||||
| Casevo [52] | ✓ | ✓ | ✓ | ✓ | 100-1k | Rules | |||||||||
| Ours | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | >10k | Society |
3 LLM-driven Social Generative Agents
3.1 Overview
As discussed above, the rapid development of LLMs allows us to design human-like agents with not only basic psychological states [1, 92], but also complex social behaviors such as mobility [89, 108, 33], employment [64, 48], consumption [64, 48], and social interactions [39, 77]. While these efforts in specific areas have shown the human-level intelligence of LLMs, creating LLM-driven social generative agents capable of simulating a comprehensive social being remains difficult. This difficulty primarily lies in two aspects. First, human behaviors are inherently motivated by psychological states [32, 69, 68, 5]. However, this crucial connection is largely absent in a vanilla LLM or existing agents designed for specific aspects. Second, different types of behaviors are highly interdependent. For example, the decision of when and how people commute to work is shaped by the interplay between their mobility and employment behaviors. Similarly, social interactions among individuals often take place when people go shopping. These examples highlight the crucial interdependence of human behaviors. Despite its significance, current LLMs and agents fail to capture this, limiting their ability to accurately simulate realistic, complex human behaviors. Addressing these two aspects requires deep insights into social science theories of human behavior, as well as advancements in algorithmic design to integrate these insights into LLM-driven social generative agents.
Therefore, we propose a design for LLM-driven social generative agents, deeply rooted theories from psychology (e.g. Maslow’s Hierarchy of Needs [68] and Theory of Planned Behavior [5]), economics (e.g., Dynamic Stochastic General Equilibrium [25]), and behavioral science (e.g., Gravity Model [118]). Figure 3 provides an overview of the proposed agents, which can be roughly divided into four parts. First, each agent retains their profile, typically regarded as relatively stable (e.g., personality), and status, which is dynamic (e.g., emotion). In particular, the profile includes basic demographics such as name, age, gender, and education, as well as personality. The status comprises three key aspects: the agent’s current mental states, economic status, and social relationships. Mental states reflect the agent’s inner experiences, while economic status and social relationships capture their power and connections in the external world. The integration of the profile and status into these LLM agents enables them to role-play like real people, providing the foundation for simulating complex mental processes and behaviors.
Second, each agent is designed with three levels of mental processes: emotions, needs, and cognition. Emotions reflect the agent’s immediate response to both internal and external stimuli, shaping its behaviors and reactions. Needs serve as the underlying motivational drivers that guide an agent’s actions, ranging from basic survival requirements to higher aspirations such as personal growth and self-fulfillment. Cognition refers to the agent’s understanding of the world, e.g., its attitudes toward climate change and political issues. By incorporating these three levels of mental processes (see the detailed design in Section 3.2), agents can autonomously perceive the external environment, ultimately developing their cognition of it.
Third, social behaviors are the core of LLM-driven social generative agents, which serve as the bridge between their internal mind and external environment. Given the importance and complexity of various human behaviors, we explicitly model three types of social behaviors: mobility, social interactions, as well as employment & consumption. In Sections 3.3-3.5, we detail the special designs for these three behaviors. Other simple behaviors such as sleeping are directly handled by LLMs. It is worth noting that these behaviors are conditioned by the agents’ profile and status, and driven by their mental processes. Finally, we introduce the workflow of the overall LLM-driven social generative agents in Section 3.6, illustrating the integration of their profiles, mental processes, and social behaviors. This workflow enables the simulation of comprehensive, context-aware agents by capturing both internal cognitive states and external interactions, ensuring realistic, dynamic social behaviors within the simulation.
3.2 Emotion, Needs, and Cognition
Humans are driven by an intricate interplay of feelings, motivations, and thought processes that shape their decisions and interactions [90, 7]. Grounded in psychological theories, our study integrates three fundamental constructs, including emotion, needs, and cognition, to design agents that realistically simulate adaptive and human-like behavior. Emotion, as the most dynamic layer of human psychology, drives rapid responses to external situations and influences behavior [20, 16]. Needs, based on Maslow’s hierarchy of needs theory, serve as motivational drivers, spanning from basic survival requirements to higher aspirations like personal growth [3]. Modeling these needs enables agents to adopt realistic motivations and prioritize actions in ways relatable to human behavior. Cognition, informed by theories like Theory of Mind and Cognitive Appraisal Theory, involves advanced mental processes that allow agents to evaluate complex situations, make thoughtful decisions, and adapt to diverse situations [13, 88]. Drawing on these psychological theories, agents recognize their own knowledge and the mental states of others while evaluating context sensitively, enabling effective and goal-oriented social interactions [81]. By integrating these crucial elements, our study designs agents that can respond dynamically to real-time changes while tailoring their actions to reflect human-like characteristics and behaviors within complex social simulations. The overall modeling framework for the psychological and cognitive aspects of the agent is illustrated in Figure 4.
Emotion is a dynamic and foundational element of human psychology, driving rapid responses to external events and influencing decision-making and behavior [90]. In our model, an agent’s emotions are affected by its profile and status and are updated based on interactions with other agents and the environment. We adopt the emotion measurement framework from Shvo et al. [90], which involves the agent selecting a keyword to best describe its current emotional state, formulating a sentence-based thought related to that emotion, and rating the intensity of six core emotions—sadness, joy, fear, disgust, anger, and surprise—on a scale from 0 to 10. This method enables agents to track and update their emotional states, providing a foundation for contextually appropriate and adaptive behavior. These emotions then influence the agent’s actions, motivations, and cognitive processes, establishing an interconnected system that guides decision-making. As we will explore in the next section, emotional states directly impact the agent’s needs and cognitive evaluations, linking emotions to higher-order motivational and reasoning functions.
The concept of needs is widely accepted in the field of psychology as the fundamental motivator behind an individual’s pursuit of specific objectives and maintenance of social engagement. Emotions, on the other hand, are seen as the immediate responses experienced by an individual. The concept of needs, however, is believed to establish the underlying motivational mechanisms that guide sustained behavior, thereby extending and contextualizing emotional fluctuations. The integration of needs and emotions in the agent’s model enables the maintenance of consistent motivational pathways over time, ensuring that transient affective states are grounded in enduring goals and priorities. In our approach, we employ established psychological frameworks (e.g., Maslow’s hierarchy of needs [3]) to categorize and structure these motivational forces. We adopt a hierarchical representation of needs to organize motivational drives by their relative urgency and importance. This needs hierarchy is continuously updated based on three interrelated factors: the agent’s active behaviors, uncontrollable or passive external events, and its current psychological states. The integration of these elements enables the system to dynamically adjust need priorities, ensuring that the agent responds appropriately to both internal motivations and external pressures. Furthermore, needs do not merely reflect static conditions but rather serve as a driving force for proactive behavior. Leveraging the Theory of Planned Behavior [5], the agent formulates action plans specifically aimed at meeting or enhancing priority needs. Through this design, the needs module provides a robust foundation for adaptive, socially informed behavior. In conclusion, the modeled needs provide the necessary motivationa
...[truncated]"
"Content uploaded by Zixu Wang
Author content
All content in this area was uploaded by Zixu Wang on Jul 03, 2025
Content may be subject to copyright.
1
A Survey on LLM-based Agents for Social
Simulation: Taxonomy, Evaluation and Applications
Zixu Wang*,1,2, Bin Xie*,1,2 , Bingbing Xu1, Shengmao Zhu1,2, Yige Yuan1,2 , Liang Pang1,
Du Su1, Long Yang1, Zixuan Li1, Huawei Shen1,2, and Xueqi Cheng1,2
1State Key Laboratory of AI Safety, Institute of Computing Technology, Chinese Academy of Sciences
2University of Chinese Academy of Sciences, Beijing, China
{wangzixu22s, xiebin23s, xubingbing, cxq}@ict.ac.cn
Abstract—Social simulation is a crucial tool in social science
research, aiming to understand complex social systems. Recently,
large language model (LLM) agents have demonstrated unprece-
dented human-like intelligence by leveraging the strong language
understanding, generation, and reasoning capabilities of large
language models. This paper conducts a comprehensive survey
of social simulation empowered by LLM agents. We ﬁrst review
the evolution of social simulation paradigms and the development
of LLM agents as background knowledge. Building on the
foundational requirements for constructing a social simulator, we
identify ﬁve essential capabilities that an individual LLM agent
must possess. Correspondingly, we delineate ﬁve core modules
that constitute the architecture of an LLM agent: (1) Proﬁle
Module for adaptive role-playing; (2) Perception Module for
social context awareness; (3) Memory Module for continuous
learning; (4) Planning Module for scenario-based reasoning; and
(5) Action Module for dynamic decision-making. Additionally,
we present a uniﬁed framework for LLM agent-based social
simulation systems, comprising the simulation environment, the
agent manager, and interacting LLM agents. We also introduce
a comprehensive evaluation metric that integrates macro- and
micro-level as well as subjective and objective criteria. The
representative applications are categorized into four scenarios:
uncovering social patterns, interpreting social phenomena, vali-
dating social theories, and forecasting policy outcomes. Finally,
we identify the challenges and research opportunities in this
ﬁeld. Overall, this survey provides a systematic understanding
of LLM agent-based social simulation, offering valuable insights
for future research and applications in this ﬁeld.
Index Terms—LLMs, Social Simulation, Agent, Social Science.
I. INTRODUCTION
SOCIAL science, employing interpretative methods to ex-
plore social dynamics, cultural inﬂuences, and human
motivations, focuses on understanding complex social systems.
By integrating computational methods with insights from the
social sciences, social simulation has emerged as a powerful
analytical tool to dissect social theory [1]. It elucidates the
genesis of social phenomena and probes the collective ramiﬁ-
cations of intricate social dynamics [2].
The pioneering social simulations can be traced back to the
1950s, such as the growth models [3], [4]. These models use
differential equations or dynamic system models to present
the overall evolution of social phenomena, such as population
* These authors contributed equally to this work.
Fig. 1. The outline of LLM Agent-Based Social Simulation System, including
the construction of individual agent, the framework of multi-agent system, the
evaluation metric, and representative applications.
growth and economic ﬂuctuations [5]. These methods focus
on the mathematical relationships among macro variables, but
which ignores individual heterogeneity and the interactions
between them.
In the 1970s, T.C.Schelling proposed the “self-organization
idea”, which is a theoretical and methodological groundwork
for the agent-based modeling (ABM) paradigm. ABM focuses
on individual agents and their interactions as the primary
unit of analysis, which enables a bottom-up exploration of
complex social structures emerging from agent interactions. It
has remained the mainstream in social simulation for many
years [6]–[9]. However, agent behavior has heavily depended
on predeﬁned rules or heuristics in ABM, which decays the
2
depth of interactions and often falls short of capturing the
complexity of real human decision-making [10]–[13]. Recent
works (e.g., [14], [15]) apply advance method of machine
learning for ABM, such as reinforcement learning, to enhance
agent decision-making capabilities. Although machine learn-
ing enhances agent decision-making for ABM, its reliance
on extensive data and computational resources constrains
its scalability, potentially reducing the realism of large-scale
simulations [16], [17], and still struggles to generalize com-
plex human behaviors—limitations that LLM-based modeling
begins to address more effectively.
Large language models (LLMs) such as ChatGPT1,
Claude3.52, Llama 3 [18] and DeepSeek3which show great
potential for human-like understanding and reasoning capabil-
ities [19]–[21]. LLMs engages in sophisticated and adaptive
interactions with outside environment and each other, it is
possible to propose a promising path for simulating human
social behaviors, which serving as the “brain” of simulation
gents with perception, memory, planning, and action [13], [20].
Since 2023, researchers have conducted a large-scale explo-
ration of the potential of LLM agents in many social science
scenarios, such as social evolution [22]–[26], social media
[27]–[31], recommendation system [32]–[34], economy [35]–
[38] and politics [39], [40] among which the pioneering work
is [22], which implemented LLM agents in an interactive
sandbox environment. These agents not only simulate daily
human behaviors such as waking up, making breakfast, and
going to work, but also spontaneously organize and participate
in social events, like a Valentine’s Day party.
Building agents based on LLM is a revolutionary new
paradigm for social simulation. It allows agents to move
beyond reliance on predeﬁned rules. Additionally, it en-
ables them to learn and adapt through LLMs, enhancing
the simulation system’s ability to capture dynamic changes
and complex characteristics of social phenomena. However,
there are few comprehensive works that have summarized the
recent developments focusing on LLM agents based social
simulation systems. Several pertinent reviews are centralized
in areas about the construction of single agent [13], [41]–[46],
communication and cooperation mechanisms in multi-agent
systems (MAS) [47]–[50], and applications across ﬁelds like
economics [51], chemistry [52], gameplay [53], [54], com-
putational experiments [11], ABM [2], [10], [55], biomedical
discovery [56], software engineering [57], [58], and industrial
automation [59], [60]. Although these reviews focus on LLM
agents, they lack consideration for critical issues such as
how to construct agents that meet the requirements of social
simulation, how to organize single agents to build multi-agent
social simulation systems, how to validate and evaluate the
effectiveness of system simulations, and the applications of
LLM-based social simulation systems in sociology and real-
life scenarios.
Compared with recent works [2], [10], [11], [61], our
study provides a more systematic and in-depth overview of
1https://openai.com/blog/chatgpt/
2https://www.anthropic.com/claude/sonnet
3https://www.deepseek.com/
the evolution of social simulation, dividing it into three key
stages: mathematical modeling, agent-based modeling, and
LLM-agent modeling. After reviewing the development of
social simulation, we identiﬁed ﬁve essential agent capabilities
and proposed a way to build individual LLM agent suited
for social simulation. Based on this, we introduced the multi-
agent system, evaluation, and application components,which
together deﬁne the outline of this paper.
We therefore divide the LLM agent-based social simulation
system into four interrelated components as shown in Fig.1:
the individual LLM agent component constructs foundational
agents to simulate single users; the multi-agent system pro-
vides the manager, interaction platform, and environment to
enable large-scale social simulation; the evaluation component
assesses the system at both micro and macro levels; and
the application component identiﬁes four representative use
cases of the simulation system. Based on this foundation,
we conducted an comprehensive study of each component,
constructing the taxonomy of the LLM agent-based social
simulation system as illustrated in Fig.2. In summary, the
contributions of this survey can be summarized as follows:
•We summarize the methodologies for constructing in-
dividual LLM agent in social simulation. We propose
that agents are endowed with ﬁve necessary capabilities
to implement social simulation: adaptive role-playing,
social context perception, continuous learning, scenario-
based planning, and dynamic decision-making. These
capabilities are supported by ﬁve key modules: proﬁle,
perception, memory, planning, and action.
•We propose a comprehensive multi-agent system for
social simulation based on LLM agents, including envi-
ronment, participating LLM agents, and agent manager.
•We propose a comprehensive evaluation metric system
sorted out from both macro/micro and subjective/objec-
tive perspectives.
•We summarize the application of LLM agent-based social
simulation systems, which are classiﬁed into four typical
scenarios: discovering social patterns, interpreting social
phenomena, validating social theories, and forecasting
policy outcomes. We also outline the ongoing challenges
in the ﬁeld of social simulation and propose several
directions for future research.
The structure of this survey is as follows. Section 2 provides
background information, including: the evolution of social
simulation paradigms, the development of LLMs from pure
language models to social intelligent agents, the comparison
between traditional social simulation and LLM agent based
methods, related surveys and the scope of our survey. Section
3 discusses the construction of individual LLM agent for
social simulation, focusing on ﬁve key capabilities which
corresponds to ﬁve modules. Section 4-6 summarizes LLM
agent based social simulation systems’ frameworks, evalua-
tions and applications. Section 7 summarizes challenges of
LLM agent based social simulation systems and suggests
potential research directions. Section 8 offers a brief discussion
and conclusion.
3
LLM-based
Agents
for Social
Simulation
Individual LLM Agent
Proﬁle Module
Static Demographic In-
formation Conﬁguration
Handcraft Conﬁguration [22] Program Generation [29] LLM Generation [26]Real Dataset Generation
[94] Combined [32]
Dynamic Social
Attributes Update Action Driven [86], [98] Human Driven [100] Environment Driven [35] Action-Environment Driven [27]
Perception Module
Social Context Demen-
sions Physical Environment [22], [35]Other Agents’ States [101], [102] Social Norms [38], [100]
Technical Approaches Text-Only [22], [99]Multi-modal [103], [104] Social Cognition Theory [102], [105]
Memory Module
Knowledge-Base Based
Memory Storage [106], [107] Retrieve [26], [27]Write [85], [102], [108] Forget [32], [111], [111]Reﬂect [22], [99]
Model Parameters
Based Memory
Supervised Fine Tuning [116], [117]Parameter Efﬁcient Fine Tuning [119], [120] Reinforcement Learning from
Human Feedback [90], [100]
Planning Module
Plan Generation Goal Oriented [103] Time Based [86], [107], [123]Event Driven [106]
Plan Decomposition Divide & Conquer [85], [86], [125]–[127]
Plan Updating Reactive Adjustment [106]Progress Monitoring and Failure Handling [86]
Multi-agent Planning Collaborative [100]Competitive [115], [128] Cooperation [129]
Action Module
Action Types Predeﬁned [106], [108], [115] Free-form [22], [103]Hybrid [26], [98]
Action Mechanisms Proﬁle-base [90], [132]–[134] Memory-based [25], [102]Plan-based [103], [140] Multiple [22], [100]
Multi-agent System
Environment
Feedback Mechanisms Unicast [131] Multicast [108]Broadcast [141] Prioritize & Filter [26], [32]
Dynamic Evolution Agent Driven [108]Stochastic [26] Rule Based [35]
Social Constraints Explicit Constraints [135], [142] Implicit Constraints [67], [143], [144]
Multi-agents Management
Agent Conﬁguration Empty Initialization [26]Proﬁle Based [26] Historical Data Based [108]Cognitive Parameter [140]
Agent Scheduling Sequential [26] Parallel [108]LLM Manager [30]
Agent Monitoring Boundary Enforcement [30] Intervention [135]Adaptation [38]
Evaluation
Micro Evaluation
Action Criteria
Approach Rationality [26], [32], [38], [135] Consistency [22], [67]Accuracy [28], [35], [131]
Module Speciﬁc
Approach Memory [103], [142], [146] Planning [22], [98], [103]Action [38], [67]
Macro Evaluation
Believability [27], [28], [30], [107], [147]
Scalability [22], [32], [94]
Efﬁciency [30], [130], [148]
Cost [26], [85], [130]
Evaluation Methods
Subjective Human Based [22], [67]LLM Based [26], [32], [38]
Objective Regression Eq.4 Eq.5Classiﬁcation [149] Text Similarity [150], [151]
Application
Discovering Social Patterns Emergence of scale-free networks in social media [152] Propagation dynamics of social norms in the workplace [154]
Interpreting Social Phenom-
ena
Behavioral responses to illness and case surges [155] Trust formation through inter-ﬁrm communication [115] Reduction of information friction via
contracts [156]
Validating Social Theories Collective decision-making under electoral dynamics [157] Role of Theory-of-Mind in cooperation [158] Validation of small-world theory in social
networks [108]
Forecasting Policy Outcomes Impacts of public housing allocation policies [141] Information diffusion under content moderation [30] Effects of taxation on economic inequality
[35] Impact of policy interventions on prosocial behaviors [95]
Fig. 2. Taxonomy of LLM-based Agents for Social Simulation.
4
II. BACKGROUND
This section aims to provide sufﬁcient background knowl-
edge for the entire survey. First, we review the evolution of
social simulation paradigms across three phases: traditional
mathematical modeling phase, agent-based modeling phase
and LLM-based modeling phase. Second, we review the devel-
opment of large language models (LLMs) from pure language
models to social intelligence agents. Then, we compare three
phases of social simulation approaches across scalability, com-
plexity, assumptions, and replicability. In addition, we deﬁne
the scope of our survey, specifying which papers are included
and which are excluded. We also summarize related surveys
and highlight the differences between our work and existing
ones.
A. Evolution of Social Simulation Paradigms
Social simulation aims to understand, predict, and ex-
plain complex social dynamics through advanced modeling
and simulation technologies. It is a methodological pillar of
computational social science, an interdisciplinary ﬁeld using
computational methods to investigate social phenomena. Up to
now, social simulation paradigms has traversed three pivotal
developmental phases as shown in Fig.3 Initially, there was
the static mathematical modeling phase. This was succeeded
by the agent-based modeling (ABM) phase, during which
dynamic interactions were introduced. Currently, it is in the
stage centered around large language model agents. We will
expound on the developmental trajectory of this discipline by
examining each of these three phases in turn.
1) Mathematical Modeling Phase: This phase adopts
methods that integrate the concept of system-level mathemat-
ical models to explain observed social phenomena and predict
future trends under simpliﬁed assumptions [62]. They focus
more on the overall structure and dynamic interactions within
the system from a macro-perspective and struggle to capture
the complexity and uncertainty of human behavior.
2) Agent-Based Modeling Phase: In the 1970s, Schelling’s
concept of “self-organization” enabled researchers to recog-
nize that society is a complex system emerging from the inter-
actions of heterogeneous individuals. This marked the birth of
agent-based modeling (ABM). With increased computational
power, “Swarm” and “NetLogo” were introduced in the 1990s
as open-source tools that support visual and parameterized
simulations, reducing the technical barriers of ABM. Based on
these achievements, social science researchers started incorpo-
rating ABM [63], [64] to design agents with decision rules that
simulate macro-level social system behavior through agent in-
teractions. This “bottom-up” approach allowed social scientists
to explore social dynamics in controlled environments, exem-
pliﬁed by Schelling’s segregation model [65] and Axelrod’s
cultural dissemination model [66]. However, traditional ABMs
face signiﬁcant limitations: agent behavioral rules are typically
preset and simpliﬁed, inadequately reﬂecting human decision-
making complexity, particularly regarding adaptive behaviors
under uncertainty and environmental changes [63]. Inspired by
advances in machine learning, researchers have increasingly
integrated data-driven approaches into ABM, using techniques
like reinforcement learning to improve agent decision-making
capabilities. This paradigm shift is highlighted in studies by
Kavak et al. [14], [16], Venkatramanan et al. [15], and Keller &
Hu [17]. However, data-driven ABMs depend on data scale and
quality, which may limit model generalization and introduce
bias.
3) LLM-Agent Modeling Phase: Recent breakthroughs in
artiﬁcial intelligence, especially generative AI, have initiated
a new paradigm shift in social simulation. Introducing LLMs
as foundations for socially intelligent agents has enabled
researchers to construct simulation environments with more
complex and realistic human behavioral characteristics [22],
[38]. These “generative agents” transcend simple rule sets,
demonstrating near-human reasoning capabilities, emotional
responses, and social interaction patterns [22]. LLM agents
push the transformation of social simulation from static repli-
cation to dynamic evolution, where agents not only remember
historical interactions but also develop unique “personalities”
and make contextually appropriate decisions in social settings
[67], [68]. The core advantages of LLM agent-based social
simulation include:
•Improved ecological validity of social simulations that
more closely approximate real human societies;
•Richer interaction possibilities unrestricted by predeter-
mined behavioral rules;
•Enhanced capacity to observe and analyze complex social
phenomena, including cultural evolution, opinion dynam-
ics, group polarization, and norm formation [69], [70].
In summary, LLM agent-based social simulations provide
social scientists with an unprecedented “digital laboratory” to
safely and controllably test social science theories and explore
the potential impacts of various social phenomena [67].
B. Development of LLMs
We focus on the key developmental stages where LLMs
progress from early text generation models to sophisticated
agent systems that support complex social interactions. This
development reﬂects AI’s expansion from pure language pro-
cessing toward broader social cognition and interaction capa-
bilities.
Breakthrough LLM advancements began with the Trans-
former architecture [71], followed by signiﬁcant performance
improvements through scaled models like GPT [72], PaLM
[73], and LLaMA [74]. These models acquired robust lan-
guage understanding and generation capabilities through large-
scale pretraining, establishing foundations for subsequent de-
velopments. ChatGPT’s release marked the transition of LLMs
to practical application, demonstrating natural conversational
abilities [19]. Subsequently, instruction tuning and reinforce-
ment learning from human feedback (RLHF) enhanced model
responsiveness to human intentions [75].
The transformation from LLMs to LLM agents represents a
crucial AI research advancement, fundamentally shifting from
passive language processing tools to proactive intelligent en-
tities [13]. Researchers developed LLM-based agent by incor-
porating planning capabilities [76], [77], tool utilization [78],
memory management [79], and self-reﬂection [80], creating
5
Fig. 3. Evolution of Social Simulation Paradigms, including three major phases: mathematical modeling phase, traditional agent-based modeling (ABM)
phase, and LLM agent-based phase.
systems capable of autonomously completing complex tasks.
Projects like AutoGPT [81] and BabyAGI [68] demonstrated
LLM agents’ ability to decompose tasks, formulate plans, and
execute multi-step operations while prompting techniques like
ReAct [76] and Chain-of-Thought [77] enhanced reasoning
capabilities.
The progression from LLM agents to social intelligence
agents represents the frontier of AI research. These agents
not only execute tasks but engage meaningfully with humans
and other agents in social environments [22], [67]. Park et
al. [22] proposed a generative agent framework demonstrating
how LLM-based agents can develop persistent “personalities”
through memory, planning, and reﬂection, exhibiting human-
like social behaviors in simulated environments. Zhou et al.
[67] further explored multi-agent social interaction possibil-
ities with their Sotopia framework, enabling researchers to
study complex social dynamics in controlled environments.
These social intelligence agents serve as minimal units in
social simulations, generating decisions and behaviors that
help social researchers analyze complex human mechanisms
across dimensions of needs, emotions, cognition, and behav-
iors, ultimately facilitating the study of emergent phenomena
in social groups.
C. Comparison Between Traditional Social Simulation And
LLM Agent-Based Methods
We compare mathematical modeling, agent-based modeling,
and LLM-based modeling across four key dimensions: scal-
ability, complexity, assumption, and replicability. A detailed
analysis is provided below.
•Scalability. Mathematical modeling uses macroscopic equa-
tions with low computational cost, making it easy to scale.
Agent-based modeling simulates individual behaviors, but
faces limitations in large-scale scenarios due to computa-
tional and design complexity. LLM-based modeling involves
massive models and high costs in training, inference, and
interaction, making scalability even more challenging.
•Complexity. Agent modeling evolves from non-existent (in
mathematical models) to rule-based individuals (in ABMs),
and further to intelligent, language-driven agents (in LLM-
based models). Agent behavior shifts from homogeneous
to personalized, and interactions from rigid to ﬂexible. As
agent intelligence and interaction richness increase, system
complexity grows accordingly.
•Assumption. Mathematical models rely on strong macro-
level assumptions and overlook individual heterogeneity.
ABMs assume rule-based self-organization with limited
learning, though data-driven variants improve ﬂexibility.
LLM-based models make minimal assumptions, using lan-
guage to approximate human behavior, but may suffer from
hallucinations that reduce simulation reliability.
•Replicability. Mathematical models are highly reproducible
due to their simplicity and transparency. ABMs have mod-
erate reproducibility, with results sensitive to rule design
and parameter tuning. LLM-based models are the least
reproducible due to stochastic outputs, opaque architectures,
and prompt sensitivity.
D. Related Surveys
To clarify the unique contributions of this survey, we
conducted a comprehensive analysis of existing literature in
related ﬁelds, categorizing them into three types: LLM sur-
veys, LLM agent surveys, and surveys addressing both LLM
agents and social simulation.
6
1) LLM Surveys: Multiple comprehensive surveys on
LLMs currently exist, focusing primarily on model architec-
ture, training methods, capability assessment, and application
scenarios. Zhao et al. [82] provided a comprehensive LLM
survey, systematically analyzing LLMs across ﬁve dimensions:
historical development, architectural design, training methods,
evaluation benchmarks, and application domains. This survey
presents a clear structure, ﬁrst reviewing the evolution from
early neural language models to modern Transformer architec-
tures, followed by detailed discussions of pre-training and ﬁne-
tuning strategies, and exploration of LLM applications across
various domains. Min et al. [83] focused on LLMs’ language
understanding and generation capabilities, with particular at-
tention to emerging learning paradigms such as few-shot, zero-
shot learning, and in-context learning. Their paper is structured
around LLM capabilities, including reasoning ability, instruc-
tion following, dialogue capability, and knowledge application.
Qiao et al. [84] concentrated more speciﬁcally on LLMs’
reasoning abilities, systematically analyzing how different
reasoning strategies (e.g., chain-of-thought, self-consistency)
affect LLM performance. These LLM surveys provide a com-
prehensive overview of the technical foundations but primarily
focus on the capabilities and technical characteristics of the
models themselves, rather than applications of LLMs as social
intelligence agents in simulated environments.
2) LLM Agent Surveys: As LLMs evolve, researchers
are increasingly studying how to turn them into autonomous
agents. Xi et al. [13] provided a comprehensive overview,
deﬁning LLM agents as systems that grant LLMs autonomy
and initiative. Their survey covers three aspects: architec-
ture, core functions (e.g., planning, tool use, memory, self-
reﬂection), and applications. Wang et al. [41] focused on
decision-making in LLM agents, outlining a process involving
perception, planning, execution, and reﬂection. While these
works offer valuable insights into LLM agent design and
capabilities, they pay less attention to their use in social
simulation.
3) LLM Agent Based Social Simulation Surveys: Re-
cently, we noticed that works such as [2], [10], [11], [61]
also cover the pics of LLM agents and social simulation.
Compared with these works, our survey provides a more in-
depth analysis of social simulation’s development and a clearer
explanation of the logic of introducing LLM. Speciﬁcally, we
traced back the development history of social simulation and
divided it into three stages: the mathematical modeling phase,
the agent-based-modeling phase, and the LLM-agent modeling
phase. We conducted a detailed analysis of the characteristics,
advantages, and limitations of the three stages. Based on
this analysis, we identiﬁed ﬁve key capabilities essential for
effective social simulation and proposed ﬁve corresponding
core modules for the LLM agent. Guided by modular thinking,
we reviewed and analyzed recent related work, proposed
a framework for LLM-agent-based social simulation, and
outlined evaluation mechanisms from both macroscopic and
microscopic, as well as objective and subjective perspectives.
We also summarized its applications in sociological research
and real-world contexts.
E. Survey Scope
Our survey focuses on social simulation systems that involve
LLM-based agents. More speciﬁcally, we adopt the following
inclusion and exclusion criteria for paper collection.
1) Inclusion Criteria: A paper is included if it meets
at least one of the following criteria: (i) it constructs a
social simulation system involving LLM-based agents; (ii)
it emphasizes the general design of LLM-based agents with
applications in social simulation, including the explanation or
prediction of social phenomena.
2) Exclusion Criteria: Papers are excluded if: (i) an LLM
is used solely as a passive question-answering tool, without
incorporating agent-speciﬁc functionalities such as tool usage
or active interaction; (ii) the work focuses only on the design
of LLM-based agents, with no discussion of social simulation.
III. LLM AGENT CONSTRUCTION FOR SOCIAL
SIMULATION SYSTEM
LLM agent plays a pivotal role in contemporary social
simulation research and it aims to replicate believable human
behaviors. In order for agents to exhibit the behaviors and char-
acteristics of humans in the real society, each agent is required
to have complex and multifaceted personality settings, observe
the evolving social environment, draw on past experiences, for-
mulate plans and decisions, and transform these decisions into
tangible actions. To achieve this, we have summarized ﬁve key
capabilities that agents need, namely, adaptive role-playing,
social context perception, continuous learning, scenario-based
planning and dynamic decision-making.
Based on these key capabilities, we introduce a uniﬁed
framework for constructing LLM agents in social simulation
systems which leverages ﬁve structured modules: proﬁle mod-
ule, perception module, memory module, planning module and
action module as shown in Fig.1. These modules enable LLMs
to have the aforementioned ﬁve capabilities respectively and
collectively enable the transformation of LLMs from mere
language models into social simulation agents endowed with
complex social behaviors. In the following sections, we will
detail the methodologies for constructing the ﬁve modules of
LLM-based social simulation agents in sequence.
A. Proﬁle Module for Adaptive Role-playing
Proﬁle module is designed to endow LLM agents with
adaptive role-playing capability, which enables LLM agents
to ﬂexibly assume and adapt roles based on dynamic social
scenarios. LLM agents in social simulation must be able to
play diverse social roles, because individuals often assume
multiple roles in human society and sociological experiments
require representativeness and diversity within the sample
population. For example, a man might be an active content
creator on social media, work as a pediatrician, and also fulﬁll
the role of a father within a family. This versatility requires
LLM agents to adapt their behavior patterns and language
styles based on contextual cues, aligning with the speciﬁc
traits and requi
...[truncated]"
"Title: A Bidirectional Lens on Context and Emotional Expressions
Abstract: We almost never encounter facial expressions in isolation—but rather embedded in rich, dynamic contexts. Recent research on human interaction has shifted from the traditional view of expressions as stand-alone signals to the claim that context is the primary driver of emotional meaning. From this perspective facial expressions are inherently ambiguous cues whose interpretation hinges entirely on the surrounding situation.
But this one-way view misses a critical point: both context and expression provide information. The question is how this information is integrated. I propose a bidirectional perspective: just as context influences the interpretation of facial expressions, these expressions have sufficient intrinsic meaning to conversely influence the interpretation of the situation that elicited them. The real question is therefore not whether context or expression drives emotion understanding, but when and how each source of information becomes more informative.
Bio: Ursula Hess is Professor of Psychology at Humboldt-University of Berlin. Her research focuses on human emotion communication. Her main interests are processes related to nonverbal synchronisation (mimicry and contagion) and the role of emotion expressions for impression formation. She has over 200 scholarly publications, including six edited books. She is a former president of the Society for Research on Emotion and the Society for Psychophysiological Research.
Title: Recognise, Interpret, Simulate… Now What? Translating AI to Make Clinical Impact
Abstract: Advances in AI are rapidly transforming how we interact with emotional and behavioural data—but their impact in frontline mental health care remains limited. This keynote explores how research in affective AI and related fields can translate into real-world value, using youth mental health services as a test case. Drawing from over 12-years of a mission to translate AI to the clinic, I describe the road towards implementation in three countries. I will also share our team’s recent work building decision support systems that leverage natural language, speech, and clinical history to support shared decision-making in general practice and early intervention settings. The importance of translational infrastructure to bridge the translational chasm will be outlined in the context of a new $3M Medical Research Future Fund (MRFF) initiative to provide researchers with a National Critical Research Infrastructure to translate their AI models into medical devices. Within this scope, I’ll discuss key challenges—including bridging the gap between software development and production, user experience and design (UX), data governance, intellectual property, and regulatory uncertainty. To end the talk, I will discuss strategies for ensuring socially responsible deployment: from participatory design with young people to hybrid funding models that avoid exploitation. For the affective computing community, this talk offers both an invitation and a provocation: how do we move from detecting emotion to embedding emotional intelligence into the messy, high-stakes reality of care?
Bio: My vision is of a world where serious mental illness is preventable, care is proactive, and everyone has access to life-changing support. For over 10-years, I’ve worked to transform mental healthcare by harnessing AI—not as an end in itself, but as a way to make care more personal and create lasting change. I pioneered AI research in London and Munich for 7-years before returning to Orygen to accelerate the mission within our globally leading ecosystem. I now lead the MRFF National Critical Research Infrastructure for AI in Mental Health, which is a $3M project aiming to provide consultancy services and software for researchers to responsibly translate AI algorithms into clinical care. I also lead initiatives to create the next generation of AI algorithms as an NHMRC Principal Research Fellow (EL2) and Chief Investigator on over $30M of associated projects. My vision is supported by a resilient organisational structure where I am pioneering for-purpose social enterprise strategies.
Title: Modelling and Simulating Cyber-Physical-Social Behaviours with Multimodal Data
Abstract: Understanding and anticipating complex dynamic behaviour is fundamental to both computational social science and the scientific modelling of socio-technical systems. Behaviours of human and systems in the wild could unfold dynamically —often shaped by diverse contexts and evolving intentions.
Yet data capturing real-world behaviours are inherently noisy, context-dependent, and often only partially observed. This talk synthesises recent progress in understanding behaviour at scale through data-driven modelling and simulation, highlighting the convergence of data-efficient learning, generative models, and agentic AI for complex systems analysis. Recent advances reveal how latent routines, dynamics, and behavioural patterns can be learned without explicit ground-truth supervision. We will also demonstrate the use of LLMs for synthetic data generation. These approaches reflect a shift toward data-efficient, transferable, and context-sensitive models that are aimed at generalisation beyond limited user data and narrow domains. We also discuss the rise of agentic AI for enabling automated tooling and simulation. We will present our new cyber-physical-social simulation generation framework, enabling automated scenario generation, behaviour testing, and what-if analysis. This framework opens new possibilities for integrating empirical data with simulated environments.
Bio: Flora Salim a full Professor in the School of Computer Science and Engineering at the University of New South Wales (UNSW) Sydney, where she also serves as the Deputy Director (Engagement) of the UNSW AI Institute. Her work focuses on multimodal machine learning and foundation models for time-series and spatio-temporal data, behavioural modelling with multimodal sensors and wearables, robust and trustworthy machine learning, and on applications of AI and LLMs for smart and sustainable cities, and for mobility, transport, energy, and grid systems. She has received multiple nationally and internationally competitive fellowships, such as Humboldt Fellowship, Bayer Fellowship, Victoria Fellowship, ARC Australian Postdoctoral Industry (APDI) Fellowship, and many accolades and awards such as the Women in AI Award Australia and New Zealand (2022) and IBM Smarter Planet Industry Innovation Award. She is a member of the Australian Academy of Sciences’ National Committee for Information and Computing Sciences and an elect member of the Australian Research Council (ARC) College of Experts. She is a Vice Chair of the IEEE Task Force on AI for Time-Series and Spatio-Temporal Data. She serves in the editorial board of ACM TIST, ACM TSAS, PACM IMWUT, IEEE Pervasive Computing, and Nature Scientific Data, and has served as a senior reviewer or area chair for NeurIPS, ICLR, WWW, and many other top-tier conferences in AI and ubiquitous computing. Prof Salim is a Chief Investigator on the Australian Research Council (ARC) Centre of Excellence for Automated Decision Making and Society (ADM+S), co-leading the Mobilities Focus Area. She is also a Key Chief Investigator in the ARC Training Centre for Whole Life Design for Carbon Neutral Infrastructure, leading the Program on Machine Learning for Carbon Performance. She has worked with many industry and government partners, and managed large-scale research and innovation projects, leading to several patents and deployed systems locally and globally.
Title: Photorealistic Telepresence
Abstract: Telepresence has the potential to bring billions of people into artificial reality (AR/MR/VR). It is the next step in the evolution of telecommunication, from telegraphy to telephony to videoconferencing. In this talk, I will describe early steps taken at Meta Reality Pittsburgh towards achieving photorealistic telepresence: realtime social interactions in AR/VR with avatars that look like you, move like you, and sound like you. If successful, photorealistic telepresence will introduce pressure for the concurrent development of the next generation of algorithms and computing platforms for computer vision and computer graphics. In particular, I will introduce codec avatars: the use of neural networks to unify the computer vision (inference) and computer graphics (rendering) problems in signal transmission and reception. The creation of codec avatars require capture systems of unprecedented 3D sensing resolution, which I will also describe.
Bio: Yaser Sheikh is the Vice President and founding director of the Meta Reality Lab in Pittsburgh, devoted to achieving photorealistic social interactions in augmented and virtual reality. He is a consulting professor at the Robotics Institute, Carnegie Mellon University, where he directed the Perceptual Computing Lab producing OpenPose and the Panoptic Studio. His research broadly focuses on machine perception and rendering of social behavior, spanning subdisciplines in computer vision, computer graphics, and machine learning. He has served as an associate editor for the IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI) and has regularly served as a senior program committee member for SIGGRAPH, CVPR, and ICCV. His research has been featured by various news and media outlets including The New York Times, BBC, CBS, WIRED, and The Verge. With colleagues and students, he has won the Hillman Fellowship (2004), Honda Initiation Award (2010), Popular Science’s “Best of What’s New” Award (2014), as well as several conference best paper and demo awards (CVPR, ECCV, WACV, ICML)."
"Abstract
Agent-based modeling and simulation have evolved as a powerful tool for modeling complex systems, offering insights into emergent behaviors and interactions among diverse agents. Recently, integrating large language models into agent-based modeling and simulation presents a promising avenue for enhancing simulation capabilities. This paper surveys the landscape of utilizing large language models in agent-based modeling and simulation, discussing their challenges and promising future directions. In this survey, since this is an interdisciplinary field, we first introduce the background of agent-based modeling and simulation and large language model-empowered agents. We then discuss the motivation for applying large language models to agent-based simulation and systematically analyze the challenges in environment perception, human alignment, action generation, and evaluation. Most importantly, we provide a comprehensive overview of the recent works of large language model-empowered agent-based modeling and simulation in multiple scenarios, which can be divided into four domains: cyber, physical, social, and hybrid, covering simulation of both real-world and virtual environments, and how these works address the above challenges. Finally, since this area is new and quickly evolving, we discuss the open problems and promising future directions. We summarize the representative papers along with their code repositories in https://github.com/tsinghua-fib-lab/LLM-Agent-Based-Modeling-and-Simulation.
Similar content being viewed by others
Introduction
Simulation, as a computational tool, encompasses the emulation of real-world processes or systems by employing mathematical formulas, algorithms, or computer-generated representations to imitate their behaviors or characteristics. Agent-based modeling and simulation focuses on modeling complex systems by simulating individual agents and their interactions within an environment (Macal and North, 2005). It operates by assigning specific behaviors, attributes, and decision-making capabilities to these agents, enabling the examination of emergent phenomena resulting from agents’ interactions and environment dynamics. The significance of simulation spans various domains, serving as a valuable tool for understanding, analyzing, and predicting intricate phenomena that might be impractical or impossible to observe directly in real life. It facilitates experimentation, hypothesis testing, and scenario analysis, offering insights into systems’ behaviors under diverse conditions and aiding in decision-making processes across fields like economics, biology, sociology, and ecology. The capacity to acquire and use language is a key aspect that distinguishes humans from other beings (Hauser et al., 2002). The advent of large language models (LLMs) represents a recent milestone in machine learning, showcasing immense capabilities in natural language processing tasks and textual generation (Zhao et al., 2023). Leveraging their formidable abilities, LLMs have shown promise in enhancing agent-based simulations by enabling more nuanced and realistic representations of agents’ decision-making processes, communication, and adaptation within simulated environments. Integrating LLMs into agent-based modeling and simulation holds the potential to enrich the fidelity and complexity of simulations, potentially yielding deeper insights into system-level behaviors and emergent phenomena for the following reasons: First, LLM agents can take actions even if there are no explicit instructions (Team, 2022; Yoheinakajima, 2023). Second, LLM agents can respond like a real human with adaptive planning (Schick et al., 2024; Wang et al., 2024b; Xi et al., 2023). Lastly, LLM agents can interact with other agents (or even real humans) (Park et al., 2023). Thus, LLM agents have achieved success in a lot of areas (Boiko et al., 2023; Bran et al., 2023; Gao et al., 2023; Jinxin et al., 2023; Kovač et al., 2023; Li et al., 2023c, 2023e; Lin et al., 2023; Park et al., 2023, 2022). From this perspective, it is clear that LLM agents can serve as a new paradigm for simulation with human-level intelligence.
As a result of the massive potential of LLM agents, there has recently been a boom in research efforts in this area. However, as yet, there is no survey that systematically summarizes the relevant works, discusses the unresolved issues, and provides a glimpse into important research directions. In this survey, we analyze why large language models are essential in the fundamental problem of simulation, especially for agent-based simulation. After discussing how to design agents in this new paradigm, we carefully and extensively discuss and introduce the existing works in various areas, most of which have been published recently. The contribution of this survey can be summarized as follows.
-
We take the first step to review the existing works of large language model-based agent modeling and simulation. We systematically analyze why large language models can serve as an advanced solution for agent-based modeling and simulation compared with existing approaches. Specifically, we first extensively explain the requirements of the agent capability for agent-based modeling and simulation from four aspects: autonomy, social ability, reactivity, and pro-activeness. Then, we analyze how large language models address these challenges, including perception, reasoning and decision-making, adaptivity, and heterogeneity.
-
We divide the agent-based modeling and simulation into four domains, physical, cyber, social, and hybrid, which can cover the mainstream simulation scenarios and tasks, after which we present the relevant works, providing a detailed discussion about how to design the simulation environment and how to build simulation agents driven by large language models.
-
In addition to the existing works in this new area, we discuss four important research directions, including improving the simulation of scaling up, open simulation platform, robustness, ethical risks, etc., which we believe will inspire future research.
Discussions on PRISMA
Following Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA), we provide more details of how we collect and organize the related works. (1) Eligibility criteria, Information sources, and Search strategy. For the eligibility criteria, we delineate the scope of the review, including (1) the usage of LLM agents and (2) the studied problem of agent-based modeling and simulation. Information sources for our paper are diverse, encompassing peer-reviewed journals, conference proceedings, preprint archives, and reputable databases like IEEE Xplore, ACM Digital Library, Elsevier, Clarivate Web of Science, arXiv preprint, SSRN preprint, etc. The search strategy we used incorporates a combination of keyword searches and controlled vocabulary terms related to LLMs, ABMS, and their intersection, of which the keywords include “large language models,” “agent-based simulation,” “intelligent agents,” “AI-driven simulation,” etc. We also use the citation tracking function of Google Scholar to identify cited/citing papers for those seminal works, ensuring a thorough and relevant literature review. We believe this structured approach will facilitate a comprehensive understanding of the current landscape and emerging trends using LLM agents for ABMS. (2) Selection process, Data collection process, and Data items. After deploying the search strategy on the various information sources, we select the proper papers presented in this review. The filtering process mainly focuses on two specific problems: (1) double-checking whether the paper belongs to agent-based modeling simulation and uses LLM agents and (2) what kind of sub-category this paper belongs to. For the first problem, we found some papers that use an LLM agent as an assistant or decision-making helper, which is close to agent-based modeling and simulation but, indeed, not the same. We filter out these papers (20+) and reserve the remaining ones. For the second problem, we categorize the papers based on two-dimension criteria: the domain and the environment.
Background
In this section, we will first introduce the background of agent-based modeling and simulation, and large language models-empowered agents.
Agent-based simulation
Basic concepts of agent-based simulation
Agent-based simulation captures the intricate dynamics inherent in complex systems by concentrating on individual entities referred to as agents (Macal and North, 2005). These agents are heterogeneous, with specific characteristics and states, and adaptively behave according to context and environment, making decisions and taking actions (Elsenbroich et al., 2014). The environment, whether static or evolving, introduces conditions, instigates competition, defines boundaries, and occasionally supplies resources influencing agent behaviors (Cipi and Cico, 2011). The interaction includes interactions with both the environment and other agents, and the goal is to mirror the behaviors in reality based on predefined or adaptive rules (Elliott and Kiel, 2002; Macal and North, 2005). To summarize, the basic components of agent-based simulation include:
Agents
Agents are the fundamental entities in an agent-based simulation. They represent individuals, entities, or elements in the system being modeled. Each agent has its own set of attributes, behaviors, and decision-making processes.
Environment
The environment is the space in which agents operate and interact. It includes the physical space, as well as any external factors, e.g., weather conditions, economic changes, political shifts, and natural disasters, that influence agent behavior. Agents may be constrained or influenced by the environment, and their interactions can have effects on the environment itself.
Interaction
Agents interact with each other and their environment through predefined mechanisms. Interactions can be direct (agent-to-agent) or indirect (agent-to-environment or environment-to-agent).
With the above components, agent-based modeling and simulation provide a bottom-up perspective to study the macro-level phenomenons and dynamics from the individual interactions.
Agent capability
To achieve realistic simulation in a wide range of application domains, agents should have the following capabilities in terms of perception, decision, and action (Wooldridge and Jennings, 1995).
Autonomy
Agents should be able to operate without the direct intervention of humans or others, which is important in real-world applications such as microscopic traffic flow simulation (Lopez et al., 2018) and pedestrian movement simulation (Batty, 2001).
Social ability
Agents should be able to interact with other agents (and possibly humans) to complete the assigned goals. When studying social phenomena, group behavior, or social structures, the sociability of agents is key. This includes simulating the formation of social networks, the dynamics of opinions, the spread of culture, and more. The social interactions between agents can be either cooperative or competitive, which are critical when simulating economic activities such as market behavior, consumer decisions, etc.
Reactivity
Agents should be able to perceive their environment and respond quickly to changes in the environment. This capability is especially important in systems that need to simulate real-time responses, such as traffic control systems and automated production lines, and in disaster response scenarios where agents need to be able to respond to environmental changes immediately to effectively conduct early warning and evacuation. More importantly, agents should be able to learn from previous experience and adaptively improve their responses, similar to the idea of reinforcement learning (Lin, 1992).
Pro-activeness
Agents should be able to exhibit goal-directed behavior by taking the initiative instead of just responding to their environment. For example, agents need to proactively provide help, advice, and information in applications such as intelligent assistants and actively explore their environment, plan paths, and perform tasks in fields such as autonomous robots and self-driving cars.
It is worth mentioning that, like humans, agents cannot make perfectly rational choices due to limitations of knowledge and computational capacity (Simon, 1997). Instead, they can make suboptimal yet acceptable decisions based on imperfect information. This capability is particularly critical in achieving human-like simulations in the economic market (Arthur, 1991) and management organizations (Puranam et al., 2015). For example, considering agents’ bounded rationality when simulating consumer behavior, market transactions, and business decisions can more accurately reflect real economic activities. In addition, in simulating decision-making, teamwork, and leadership within organizations, bounded rationality helps reveal behavioral dynamics in real work settings.
Applications of agent-based modeling and simulation
The flexibility of agent-based modeling and simulation allows for the exploration of diverse scenarios and the study of emergent phenomena in a controlled simulation environment. Therefore, it offers researchers and practitioners a versatile tool for understanding and predicting the behavior of complex systems across various domains.
Based on the four categories of the target systems, current works of agent-based simulation can be divided into four domains.
Physical domain
This category refers to the natural system in the physical environment (An, 2012). Typical applications include ecology and biology (Pereira et al., 2004; Zhang and DeAngelis, 2020), such as modeling ecological systems (Heckbert et al., 2010; Lippe et al., 2019), species interactions (McLane et al. (2011), and the impact of environmental changes (Beltran et al., 2017; Pertoldi and Topping, 2004). Many simulation problems in urban environments also belong to the physical domain (An, 2012), such as transportation, human mobility, etc. Specifically, for urban planning (Gaube and Remesch, 2013), agent-based modeling and simulation can aid in simulating urban growth (Arsanjani et al., 2013; Barros, 2004), traffic patterns (de Souza et al., 2019; Mastio et al., 2018), and the impact of urban policies (Ma et al., 2013; Maggi and Vallino, 2016; Widener et al., 2013). Another application is engineering and manufacturing (Barbosa and Leitão, 2011; Rolón and Martínez, 2012), in which agent-based molding and simulation can be applied to model supply chain dynamics (Schieritz and Grobler, 2003), production processes (Parv et al., 2019), and the interactions of entities within manufacturing systems.
Social domain
The social domain mainly covers the social behavior simulation, which can be further divided into (1) social interaction that focuses on social networks, community interactions, or organizational behavior (Macy and Willer, 2002; Wall, 2016) and (2) economic system that simulates economic systems, market dynamics, or financial interactions (Samanidou et al., 2007). Specifically, for social sciences (Conte and Paolucci, 2014; Gilbert, 2007b; Gilbert and Terna, 2000; Ternaet al., 1998), agent-based modeling and simulation are widely used to model social phenomena such as crowd behavior (Kountouriotis et al., 2014; Luo et al., 2008), opinion dynamics (Banisch et al., 2012; Li et al., 2020), and social network interactions (El-Sayed et al., 2012; Gilbert, 2004a; Madey et al., 2003). The agent-based modeling can simulate the emergence of societal patterns and trends (Helbing, 2012). As for the research of economics (Hamill and Gilbert, 2015; Leombruni and Richiardi, 2005; Van Dinther, 2008), agent-based models are employed to study economic systems (Deguchi, 2011), market dynamics (Rouchier, 2017; Wang et al., 2018), and the behavior of individual economic agents (Mueller and Pyka, 2016).
Cyber domain
Besides the physical world and human society, our daily life has been further extended into cyberspace. Therefore, agent-based simulation has also been applied in wide areas like web-based behaviors (Guyot and Honiden, 2006) and cyber-security applications (Alluhaybi et al., 2019).
Hybrid domain
This category includes hybrid systems combining components covering the physical world, social life, and cyberspace. For example, an urban environment is a socio-physical environment that integrates social behavior with physical infrastructure. Moreover, it is also multi-layered after taking online social networks into account. That is, these applications involve more than one domain of physical, social, or cyber domains. Therefore, agent-based simulations within an urban environment, such as urban planning (Chen, 2012) and epidemic control (Silva et al., 2020), are far more complex and challenging than those in unitary environments. Moreover, for healthcare (Barnes et al., 2013; Cabrera et al., 2011), agent-based modeling and simulation can be used to model the spread of infectious diseases (Perez and Dragicevic, 2009), healthcare systems (Silverman et al., 2015), and the effectiveness of interventions (Beheshti et al., 2017), which help in understanding and planning for public health scenarios.
Methodologies of agent-based modeling and simulation
The development of modeling technologies utilized in agent-based simulation has also gone through the early stage of knowledge-driven approaches and the recent stage of data-driven approaches. Specifically, the former includes various approaches based on predefined rules or symbolic equations, and the latter includes stochastic models and machine learning models.
-
Predefined rules: This approach involves defining explicit rules that govern agent behaviors. These rules are typically based on logical or conditional statements that dictate how agents react to specific situations or inputs. The most well-known example is the cellular automata (Wolfram, 1984) that leverages simple, local rules to simulate complex global phenomena that exist not only in the natural world but also in complex urban systems.
-
Symbolic equations: Compared with predefined rules, symbolic equations are used to represent relationships or behaviors in a more formal, mathematical manner. These can include algebraic equations, differential equations, or other mathematical formulations. A typical example is the social force model widely used in pedestrian movement simulation (Helbing and Molnar, 1995). It assumes that pedestrian movements are driven by a Newton-like law decided by an attractive force driven by the destination and a repulsive force from neighboring pedestrians or obstacles.
-
Stochastic modeling: This approach introduces randomness and probability into agent decision-making, which is useful for capturing the uncertainty and variability inherent in many real-world systems (Feng et al., 2012). For example, to account for the impact of randomness originating from human decision-making, we can leverage discrete choice models for simulating pedestrian walking behaviors (Antonini et al., 2006).
-
Machine learning models: Machine learning models allow agents to learn from data or through interaction with their environment. Supervised learning approaches are generally used for estimating parameters of agent-based models, while reinforcement learning approaches are widely used in the simulation period, enhancing the adaptation capability of agents within dynamic environments (Kavak et al., 2018; Kim et al., 2021; Platas-López et al., 2023).
Limitations
Early works on agent-based simulation are keen to design “deliberative architectures” that rely on explicit, often complex, internal models to make decisions, emphasizing the importance of planning, reasoning, and decision-making processes (Wooldridge and Jennings, 1995). However, optimizing the internal world model and planning-reasoning module based on symbolical AI approaches are generally intractable in practice. This leads to the prevalence of “reactive architectures” in agent-based simulations, which instead rely primarily on direct sense-action loops rather than complex internal models of the world or deep reasoning processes to make decisions. The subsequent development of AI, especially deep learning technology, does not fundamentally change this paradigm of agent-based simulation due to the poor interpretability and generalization capability. However, facing the need for realistic simulation of real-world processes or systems, current approaches still have several limitations, as described below.
Simple agent architecture is not enough to cope with complex tasks
Although “reactive architectures” are able to adapt to different environmental conditions, they may be limited in handling complex tasks or situations that require long-term planning. To achieve human-like simulation in real-world complex problems, current agent architecture requires redesigns that solve challenges in processing speed, resource efficiency, and task complexity. Specifically, agents should be capable of complex planning and reasoning processes, like using internal models to predict the consequences of different courses of action and choose the best one, and able to develop and execute complex strategies to achieve long-term goals.
It is difficult to develop a general agent that can support simulations across environments
Different environments vary in dimensions like complexity, dynamics, and uncertainty. Due to this diversity, a specific agent that is effective in one environment (like a financial market simulation) might be completely ineffective in another (like a social campaign simulation). In real-world applications where the target environment is often hybrid with significant dynamics and uncertainty, developing specific agents case by case is highly inefficient and costly.
Existing methods cannot support integrative simulation in real-world problems
A versatile agent-based simulation model should be able to describe how systems operate under known conditions, explain why certain patterns emerge, predict future states based on existing observations, and explore the outcomes of hypothetical scenarios. However, existing methods cannot support the above tasks simultaneously: rule-based methods are useful in descriptive problems, while symbolic or stochastic methods can provide explanations regarding underlying mechanisms that drive the system. Comparatively, machine learning models are better at predictive problems by learning hidden patterns from data but with less interpretability. Therefore, there remain challenges in developing methods that simultaneously capture the accuracy of behavioral modeling, interpretability of mechanisms, adaptability, and reliability under environmental changes.
Large language models and LLM-empowered agents
Large language models (LLMs), such as ChatGPT (OpenAI, 2022), Gemini (DeepMind, 2023), LLaMA (Touvron et al., 2023), Alpaca (Taori et al., 2023), and GLM (Zeng et al., 2023), are the latest paradigm of language models, which evolve from early statistical language models (Bellegarda, 2004) to neural language models (Melis et al., 2017), then to pre-trained language models (Brown et al., 2020), and finally to large language models (Zhao et al., 2023c). With billions of parameters and extensive pre-training corpus, LLMs have shown astonishing abilities not only in natural language processing tasks (Li et al., 2023a; Zhang et al., 2024c) such as text generation, summarization, translation, etc., but also in complex reasoning and planning tasks, such as solving mathematical problems (Arora et al., 2023), etc. Pre-training on large-scale corpora lays the foundation ability for zero-shot generalization. Moreover, pre-trained models can be further fine-tuned for specific tasks, adapting to particular application scenarios (Jiang et al., 2023). In addition, the advances of large language models in the past year, such as ChatGPT and GPT-4 have achieved human-like reasoning ability, a milestone that is now being considered to be the seed of artificial general intelligence (AGI). Specifically, the capacity to acquire and use language is a key aspect of how we humans distinguish ourselves from other beings (Tomasello, 2010). Language is one of the most important mechanisms we have to interact with the environment, and language provides the basis for high-level abilities (Hauser et al., 2002).
Thus, it is promising to construct large language model-empowered agents (Wang et al., 2024b; Xi et al., 2023) due to their human-like intelligence in perceiving the environment and making decisions. In the following, we have a short summary of the motivations to apply large language models to agent-based modeling and simulation.
First, the LLM agent is able to adaptively react and perform tasks based on the environment without predefined explicit instructions (Team, 2022; Yoheinakajima, 2023). In addition, during the simulation process, the LLM agent can even form new ideas, solutions, goals, etc. (Franceschelli and Musolesi, 2023). For example, AutoGPT Team (2022) can automatically schedule plans when given a set of available tools and the final task goal, exemplifying the significant potential of LLMs in constructing autonomous agents. Meanwhile, BabyAGI (Yoheinakajima, 2023) created an LLM-driven script running an infinite loop, which continuously maintains a task list, in which each task is completed the task by ChatGPT API (OpenAI, 2022) based on the task context. Second, the LLM agent has enough intelligence that it can respond like a human and even actively take actions with self-oriented planning and scheduling (Wang et al., 2024b; Xi et al., 2023). Actually, the input of the environment is not limited to text; rather, recent multi-modal fusion models can be fed other types of information, such as image or audio (Zhu et al., 2024). The action space of the LLM agent is neither limited to text, for which the tool-usage ability allows the agent to take more actions (Schick et al., 2024). Third, the LLM agent has the ability to interact and communicate with humans or other AI agents (Park et al., 2023). In the simulation, especially agent-based simulation, the agent’s communication ability elevates individual simulation to the community level (Gilbert and Troitzsch, 2005). An LLM-driven agent can generate text, which can be received and understood by another agent, in turn providing the basis for interpretable communication among agents or between humans and agents (Park et al., 2023). Fourth, the simulation at the community level requires heterogeneity of agents, and the LLM agents can meet these requirements for playing different roles in society (Qian et al., 2024). An artificial society constructed by LLM agents can further reveal the emergence of swarm intelligence with collective agent behaviors (Gao et al., 2023; Park et al., 2023), similar to wisdom-of-crowds in human society (Surowiecki, 2005).
As mentioned above, the simulation system has widely utilized the paradigm of agent-based modeling, which requires agents with high-level abilities. This well motivates the use of large language model-empowered agents in simulation scenarios. In the following, we will discuss the critical abilities of a large language model for agent-based modeling and simulation in detail in the section “Critical abilities of LLM for agent-based modeling and simulation”. Then, in the section “hallenges and approaches of LLM agent-based modeling and simulation”, we will elaborate on the recent advances of large language model agent-based modeling and simulation to further answer the question of how large language model agents meet the requirements (what kind of challenges and how to address them).
Critical abilities of LLM for agent-based modeling and simulation
As mentioned above, agent-based modeling and simulation serve as a basic approach for simulation in many areas (Elsenbroich et al., 2014; Macal and North, 2005), but it still suffers from several key challenges. Large language model-empowered agents not only meet the requirements for agent-based simulation but also address the limitations relying on their strong abilities in perception, reasoning, decision-making, and self-evolution, illustrated in Figs. 1, 2.
Perception
The core of agent-based modeling and simulation is to model how an individual agent interacts with an environment (Macal and North, 2005), which requires the agent to accurately sense various types of information from said environment. As for the large language model-empowered agents, the ability of language enables agents to comprehend and respond to diverse environments directly or indirectly. On the one hand, the basic ability to understand and generate text enables agents to engage in complex dialogs, negotiate, and exchange information, and support direct interaction. On the other hand, the interface between the agent and environment can be operated via texts (Team, 2024), which leads to indirect interaction. Of course, such ability also supports the communication between different agents, besides the agent-environment perspective.
It is worth mentioning that the ability to interact with the environment and other agents is not adequate to achieve human-like simulations. To be more specific, it is also required that large language model-based agents “put themselves in real humans’ shoes”, thereby allowing the agent to imagine that it is indeed in the environment. That is, LLM agents should be able to comprehend, perceive, and respond to diverse needs, emotions, and attitudes within different contexts, from the “first-view sight” (Shanahan et al., 2023). This capability enables models to better understand the information from the environment or other agents and generate more real responses.
Reasoning and decision making
One critical ch
...[truncated]"
"About Me
- Hello, I'm Wei Liu (åç»´). Welcome to my blog (0 views). You can search me on google with keyword ""thinkwee"", which means ""The Thinking Wei"".
- Past experience:
- 2014-2021: Bachelor of Communication Engineering in BUPT, and Master of Computer Engineering in CIST Lab@BUPT.
- 2021-2023: Application Research in the NLP&LLM Department in Tencent.
- 2023-2025: Working at THUNLP@Tsinghua University with Prof. Zhiyuan Liu and Prof. Chen Qian on LLM Multi-Agent System.
- 2025-now: Proud to be a PhD advised by Prof. Yulan He and Prof. Yali Du, and a member of KCLNLP!
- I'm interested in:
- Inference Time Scaling and Agentic AI.
- Compression Intelligence in NLP.
Recent News
- 2025.05.16 Checkout KCLNLP's amazing works here, with 15 papers accepted by ACL 2025 and 3 papers accepted by ICML 2025!.
- 2025.06.09 Check out AgentsMeetRL, an awesome list of Reinforcement Learning-based Large Language Agent!
- 2025.08.20 Two papers aceepted at EMNLP! Check NOVER, a novel verifier-free reinforcement learning framework for training General Large Reasoning Model. The model, paper, code and datasets are all open-sourced! Train your own R1-Zero-like reasoning model on ANY DATA!
Services
- Program Committee Member
- ACL(2021,2022,2024)
- EMNLP(2020,2023,2024,2025)
- NeurIPS(2024,2025)
- ICLR(2024)
- CVPR(2025)
- AAAI(2025)
Industrial Experience
- At Tencent, I aim to bridge the gap between technology in NLP and
scenario in Recommendation & Advertisement.
- Improving the NLU ability for News Feed Recommendation.
- Resolving the mismatch between commercial inclinations and content interests for Wechat Ads.
- Stability, Warm-Up, Efficiency-Quality Tradeoff, Interpretability & Explainability on Large Recommendation System.
- Diverse user interest modeling.
Publications
- * denotes first/co-first author.
- Personal Agentic AI:
- Inference Time Scaling:
- Multi-Agents System with LLMs:
- ACL 2024 paper code Communicative Agents for Software Development
- ACL 2024 paper code Experiential Co-Learning of Software-Developing Agents
- ACL 2025 paper code Multi-Agent Software Development through Cross-Team Collaboration
- EMNLP 2025 paper code EcoLANG: Efficient and Effective Agent Communication Language Induction for Social Simulation
- arXiv 2024 paper code Iterative Experience Refinement of Software-Developing Agents
- Compression Intelligence in NLP:
- ACL 2021 paper code UniKeyphrase: A Unified Extraction and Generation Framework for Keyphrase Prediction*
- AAAI 2022 paper code Fast and Constrained Absent Keyphrase Generation by Prompt-Based Learning
- CoNLL 2021 paper code In Conclusion Not Repetition: Comprehensive Abstractive Summarization with Diversified Attention Based on Determinantal Point Processes*
- SIGIR 2019 paper CIST@CLSciSumm-19: Automatic Scientific Paper Summarization with Citances and Facets
- EMNLP 2020 paper CIST@CL-SciSumm 2020, LongSumm 2020: Automatic Scientific Document Summarization*
- RANLP 2019 paper code Multi-lingual Wikipedia Summarization and Title Generation On Low Resource Corpus*
- CCIS 2018 paper A Multi-View Abstractive Summarization Model Jointly Considering Semantics and Sentiment
- arXiv 2021 paper code CO2Sum: Contrastive Learning for Factual-Consistent Abstractive Summarization*
- arXiv 2021 paper code Subjective Bias in Abstractive Summarization*
Social Media
- I sometimes sync some articles from this blog on Wechat and Rednote, and I share other interesting things on these platforms. You can find me via above links."
"LLM-Based Social Simulations Require a Boundary
Abstract
This position paper argues that large language model (LLM)-based social simulations should establish clear boundaries to meaningfully contribute to social science research. While LLMs offer promising capabilities for modeling human-like agents compared to traditional agent-based modeling, they face fundamental limitations that constrain their reliability for social pattern discovery. The core issue lies in LLMs’ tendency towards an “average persona” that lacks sufficient behavioral heterogeneity, a critical requirement for simulating complex social dynamics. We examine three key boundary problems: alignment (simulated behaviors matching real-world patterns), consistency (maintaining coherent agent behavior over time), and robustness (reproducibility under varying conditions). We propose heuristic boundaries for determining when LLM-based simulations can reliably advance social science understanding. We believe that these simulations are more valuable when focusing on (1) collective patterns rather than individual trajectories, (2) agent behaviors aligning with real population averages despite limited variance, and (3) proper validation methods available for testing simulation robustness. We provide a practical checklist to guide researchers in determining the appropriate scope and claims for LLM-based social simulations.
1 Introduction
Social simulation is a modeling tool that employs computational methods to understand social phenomena. Computational methods, particularly those modeling interactions between individuals, demonstrate advantages in capturing the complex and nonlinear behaviors typically inherent in social phenomena [35, 103, 106]. Among these, Agent-Based Modeling (ABM) is a widely used technique in this area, simulating how individual behaviors and local rules give rise to macro-level patterns [14, 36, 108]. ABM offers a bottom-up modeling approach, supports heterogeneity among agents, allows for the exploration of emergent phenomena, and provides researchers with interpretable mechanisms linking micro- and macro-level behaviors [62, 93, 102]. Meanwhile, it is controversial due to its reliance on simplification [33], limited adaptability [135], sensitive to initial conditions [83], and challenges in representing subjective or human-like behaviors [78, 100], diminishing the contribution of social simulation methods to social science [102].
Recently, LLM agents and social simulations have attracted growing attention. Existing studies have applied LLM agents to domains such as economics [52, 69], education [148], game theory [117], and social networks [126, 140, 145], with claimed advantages like handling natural language, enabling flexible behaviors, and showing human-like reasoning. However, concerns have also been raised: LLMs may carry social and cognitive biases [86, 90], lack behavioral diversity [79], and are hard to validate or explain [66, 78]. Whether or not using LLMs is a good protocol for social simulations remains a question—or may not even be the central question to ask. Many existing studies focus primarily on the simulation itself, while we argue that this narrow focus limits the method’s contribution to advancing social science. Before moving forward with more LLM-based social simulations, two critical questions remain:
-
1.
How can LLM-based social simulations benefit studies of social science?
-
2.
Can we draw a line to identify what types of problems are suitable for LLMs to solve?
In this paper, we take the position that social simulation benefits social science primarily through uncovering social patterns and generating hypotheses. Achieving this requires simulations with high fidelity and robustness. We emphasize fidelity in particular, examining how alignment, homogeneity, and especially heterogeneity shape social dynamics, and why individual-level heterogeneity is essential for meaningful social simulation. This perspective helps explain why the limited behavioral diversity of current LLM agents constrains their effectiveness in representing complex, multi-agent societies [79, 111]. We further investigate common issues in LLM-based simulations—such as behavioral variance, social bias, and outcome inconsistency [66, 86] (Figure 1)—and propose to regulate the applicable boundaries of LLM agent-based social simulation to enhance its contribution to social science, as our central position. We argue this as a general checklist for evaluating the use of LLMs in social simulation, rather than a how-to guide for conducting such studies.
Our Contributions. This position paper makes three key contributions. First, we systematically analyze the boundary problems of LLM-based social simulations—the inherent limitations that fundamentally determine their reliability for social pattern discovery, moving beyond implementation issues to examine what current LLM capabilities can and cannot achieve. Second, we discuss simulation fidelity through the concept of agent heterogeneity, indicating why LLMs’ tendency to converge towards common patterns fundamentally limits their capacity to simulate complex social dynamics requiring behavioral diversity. Third, we provide heuristic boundaries and a general checklist for when LLM-based simulations can make real contributions to social science research. Rather than asking whether LLMs can replace traditional agent-based models, we reframe the question to focus on precisely defining the scope of problems where LLMs can meaningfully advance social science, which is a perspective essential for responsible deployment and guiding future research towards socially beneficial applications. We expect that the boundaries of LLM-based social simulations, as outlined in this paper, would help bridge the gap between AI and social science sectors and contribute to findings in social science research.
2 LLM-Based Social Simulations
2.1 Objectives of Social Simulations
The primary objective of social simulations is not to replicate reality in fine detail, but to serve as a research tool for scientific endeavors, specifically for explaining social patterns, constructing theories, and providing interpretable foundations for hypothesis generation [9, 112, 113]. A clear modeling objective is essential for guiding methodological choices, which can vary significantly depending on the simulation’s intended purpose. When objectives are poorly defined, effective validation becomes difficult, particularly when testing alignment with reality and ensuring reproducibility, both of which are critical for establishing credible simulations [7, 9, 32, 34]. To clarify social simulation’s boundaries from the perspective of modeling objective, we examine two modeling objectives frequently declared in LLM-based simulations: replication and prediction. We argue these should not constitute primary goals and may even obstruct effective social science discovery.
Replication-oriented work is common in LLM-based simulation literature, yet studies achieving novel, valuable social science discoveries remain extremely limited. This approach suffers fundamental flaws. Critics note that replication merely repeats known behaviors without revealing new social dynamics or mechanisms [23], contradicting social simulation’s core purpose. Schelling’s model, a classical model in this field, exemplifies the alternative [108]: through simple, verifiable interaction rules, it demonstrates universal mechanisms of community segregation without replicating any specific community, revealing broadly applicable social patterns. This suggests that reproducing real-world social patterns through simple rules requires no precise replication of the world to provide explanatory insights and causal understanding. Also, pursuing exact replication increases parameters and artificial assumptions, risking data overfitting, and reducing model verifiability [66]. This creates complex “artificial societies” increasingly detached from reality [113]. Additional computational constraints and complexity of sensitivity analysis further obstruct precise replication and reproduction [15, 118]. Hence, social simulation neither needs nor can completely replicate reality; focus should center on reproducing and validating key behavioral patterns consistent with real social phenomena [19, 34, 113]. Only findings from testable reproduction apply meaningfully to social science problems.
Another misconception involves emphasizing predictive capabilities through detailed replication performance. Despite numerous attempts at LLM-based social prediction, evidence shows limited performance in predicting social dynamics without oracle information, neither discoveries on effective methods for prediction improvement [50, 138, 151]. Critics argue that social simulation predictions often constitute mere retrodictions of existing patterns, lacking effective future scenario generalization [31, 98]. Predicitive capabilities are further undermined by the flaws and biases in LLM-based simulations. For example, using retrodictive tests to claim predictive capabilities [130] may introduce data leakage, as the retrospective scenarios could already be contained within the LLM’s training data; such bias is hard to eliminate because the LLM could infer the scenario, despite the removal of time, location, and persons involved in the incident from the prompt, and implicitly use its knowledge to make “predictions”. Many simulation works’ predictive claims thus exceed actual model capabilities, which requires enhanced validation [10, 17, 26, 92, 123, 127, 138, 145]. Nevertheless, few studies establish reliable validation methods and sensitivity analyses [20]. Moreover, current works claim that simulations reflect real social dynamics [140, 145], based on simple validation efforts such as LLMs’ explanation of their own decision-making process, which might raise endogeneity issues. Whereas it is crucial to create comprehensive frameworks for simulating social phenomena using LLM agents at unprecedented scales, as achieved in these works, researchers need to be cautious with their objectives and findings, because misguided objectives and overstated findings will prevent social simulation from fulfilling its promise in social science research.
In sum, social simulation’s limitations stem from both LLMs’ inherent capabilities and simulation framework design issues [128]. We thus emphasize caution regarding simulation modeling with replication and prediction as core objectives, advocating instead for greater focus on simulation alignment with key social patterns and its validation.
2.2 Current Challenges that LLM-Based Simulations Face
Now that we have distinguished the fundamental purpose of social simulation from replication and prediction purpose, we turn our attention to the specific issues confronting LLM-based social simulations. We categorize these challenges into two primary areas: (1) usage problems, which pertain to how researchers apply LLMs in simulations and whether these applications align with effective simulation practices; and (2) boundary problems, which relate to the inherent subjective capabilities and limitations of LLMs themselves, discussing what LLM-based simulations can and cannot reliably achieve. This paper will primarily focus on the latter, the boundary problems, as they fundamentally determine the reliability and applicability of LLM-driven social pattern discovery.
Usage Problems: Misuse of LLM-Based Simulations
Usage problems arise from the way LLMs are employed in social simulation designs. A common issue, as mentioned in Section 2.1, is the tendency for simulations to aim for perfect replication of reality. Such an objective is not only inherently difficult but can also undermine the capacity for meaningful social pattern discovery [31, 53]. Overly precise replication sometimes introduces researchers’ subjective judgments or requires extensive data to calibrate simulations at the micro level [12, 97, 142].
Beyond the fundamental purpose, other common usage problems specific to LLM-based simulations include, but are not limited to: (1) imprecise or self-evident prompt engineering that can lead to simulation distortion [82, 105]; (2) overly large or ill-defined action spaces for LLM agents, which often result in the generation of invalid behaviors, complicate rigorous sensitivity analysis, and amplify errors across multiple iterations [51, 72, 73, 143]; and (3) simulation frameworks that introduce excessive researcher assumptions or constraints, inadvertently causing models to lose their emergent capabilities [112]. While these usage issues significantly impact the effectiveness of social simulation, this paper will NOT primarily focus on problems caused by researchers’ subjective choices or design flaws that could, in principle, be mitigated by better practices. Our scope specifically targets the intrinsic limitations of LLM technology itself.
Boundary Problems: Inherent Limitations of LLM Agents
Boundary problems constitute the core focus of this paper, as these boundaries fundamentally determine the reliability of social pattern discovery derived from LLM-based simulations. These problems represent the inherent, subjective limitations of current LLM technology when applied to social simulation. Clarifying these boundaries is essential for understanding where LLM-based social simulations can reliably contribute. We specifically examine three critical aspects that collectively define the scope of LLM-based social simulations:
-
1.
Alignment (Sections 3 and 4): This concerns whether the simulated agents’ behaviors and collective dynamics are aligned with real societal patterns. This aspect primarily affects whether such simulations can genuinely be used to understand real social phenomena, as discussed previously. Alignment is our main focus in this paper, where we delve deep into the types of alignment, what is currently lacking in LLM agents, and what we can reliably claim and simulate.
-
2.
Consistency (Section 5): This refers to whether the simulated agents can maintain fidelity to their assigned roles and behavioral patterns over a long temporal horizon. Social simulations often span extended periods, but LLMs inherently face challenges with long-context understanding and coherent behavior over time. Ensuring a consistent simulation throughout an entire episode is crucial for deriving reliable conclusions.
-
3.
Robustness (Section 6): This addresses whether the simulated society is reproducible and stable under different prompt settings, initial conditions, or minor perturbations. Robustness directly impacts the reliability of the simulation’s findings, which is paramount for any subsequent analysis and valid pattern discovery.
We will proceed by discussing these three aspects of LLM-based social simulations in the aforementioned order, prioritizing the intricate challenges related to alignment. By analyzing these three aspects, we aim to precisely delineate the boundaries of the scope of social problems and the validity of related claims that can be researched under current LLMs’ capabilities.
3 Alignment and Heterogeneity
The degree of alignment between LLM-based simulations and real-world behavior is a key factor in determining the reliability of insights drawn from social pattern discovery. This alignment can be further divided into two aspects: individual-level alignment, which concerns whether each agent behaves in a human-like manner, and collective-level alignment, which concerns whether the interactions among agents reproduce realistic social dynamics and emergent phenomena. These two aspects are interrelated, and understanding their respective roles is essential before applying LLMs to simulating social phenomena.
Relative Importance of Individual Alignment
While individual-level alignment is often desirable, perfectly capturing individual behavioral patterns is not always essential for obtaining conclusions with practical utility in social simulations. This is because social phenomena emerge primarily from interactions between individuals rather than from individual behaviors alone. As Durkheim argued in his foundational work on social facts, collective phenomena possess properties that cannot be reduced to individual psychological states [30]. Building on this insight, while reductionist approaches focus on individual-level fidelity, the emergent properties of social systems cannot be fully predicted from the knowledge of individual components alone [56, 76, 116]. Studies in computational social science have demonstrated that weak individual alignment can still lead to the emergence of complex collective behaviors: Granovetter’s threshold models show how individual decisions with simple thresholds can produce unpredictable collective outcomes [48], while Reynolds’ boids model demonstrates how complex flocking behaviors emerge from just three simple rules governing individual agents’ separation, alignment, and cohesion [104]. These findings suggest that individual-level fidelity is neither the sole nor the primary factor in generating realistic social dynamics. On the other hand, approximate individual-level modeling can still capture the essential social interaction dynamics. For instance, an LLM-based simulation that reproduced the aforementioned Schelling’s model demonstrated that highly segregated societies still emerge even when LLMs exhibit relatively low bias, with simple behavior settings, decision methods, and a degree of individual homogeneity [24]. In this setup, the final social structure is largely independent of specific individual intentions or detailed behavioral trajectories. This illustrates that the emergence of collective patterns can be relatively insensitive to individual-level modeling imperfections, suggesting that strict individual alignment, while beneficial, is not uniformly the most critical factor for valid social simulations focused on macroscopic phenomena.
Homogeneity and Collective Alignment
To further explore collective alignment, it is necessary to understand the interplay between individual homogeneity and heterogeneity within a system, as these properties of agents become apparent through their interactions. Homogeneity, characterized by agents sharing similar traits or behaviors, can, in certain cases, lead to emergent social patterns. As previously noted in the Schelling’s model example, even simple, uniform preferences can result in collective phenomena such as segregation.
However, when agents are highly homogeneous in their decision-making rules and responses, the resulting collective behaviors tend to converge to predictable equilibrium states that can be analytically characterized. For example, in voter models where all agents follow identical imitation rules, the system predictably converges to consensus with mathematically derivable convergence rates and final outcome probabilities [18, 57]. Similarly, in simple social contagion models where individuals adopt behaviors through independent exposures with uniform transmission probabilities, the spread patterns follow predictable epidemic trajectories characterized by standard parameters such as peak timing and final adoption rates [55, 115]. The scope and complexity of patterns that can emerge solely from homogeneous interactions are often limited.
Critical Role of Heterogeneity
Conversely, heterogeneity is widely recognized as a fundamental driver of complex social dynamics and intricate emergent phenomena. Existing work across various contexts has consistently reported that certain emergent phenomena only occur with sufficient diversity among agents, a domain where traditional rule-based simulation methods like ABM often face limitations [28, 42]. The importance of agent heterogeneity has been emphasized in numerous studies, spanning computational simulation directions (e.g., social network modeling [91], epidemic intervention [75, 102], climate change policy [84], and wealth formation [125]) and problem-solving applications (e.g., multi-agent cooperation analogous to human dynamic collaboration [22] and multi-agent software development [58, 101]).
Heterogeneity v.s. Homogeneity
From a complex systems perspective, collective behavior fundamentally differs from simple aggregations of individual behavior. When individual differences exist, interactions create feedback mechanisms that may amplify these differences, producing emergent collective phenomena that cannot be predicted from average individual characteristics [85]. While heterogeneity enables rich individual interactions that generate intricate patterns and structural biases [4], homogeneity tends to average out these behavior, limiting emergent complexity [81].
Consider two illustrative cases. In social choice theory, the Condorcet Paradox demonstrates how diverse individual preferences can produce collective voting cycles—collective behavior outcomes that cannot be understood by simply averaging individual preferences [44]. From another side, when we assume perfect homogeneity of agents in economic models (i.e. identical rationality and complete knowledge in “Homo economicus”), the ideal simulated market will reach immediate equilibrium with zero average profits, precluding the market dynamics that define real economic systems [49]. These examples show that while homogeneity can yield certain patterns, it fundamentally limits the emergence of rich, complex dynamics characteristic of real-world social systems.
Implications for LLM-Based Simulations
In summary, these considerations illustrate that neither perfect individual alignment nor homogeneous interactions alone are sufficient for capturing complex social dynamics. The ability of social simulations to discover and accurately reflect novel, complex social patterns largely depends on the degree of heterogeneity among agents. Consequently, whether the behavior of collectives composed of LLM agents can reflect “sufficient” heterogeneity becomes a critical indicator of simulation validity. If the phenomena under investigation fundamentally require sufficient heterogeneity for their emergence, yet LLMs inherently represent insufficient diversity among individuals, then the conclusions drawn from such simulations may not reliably apply to real-world situations. The subsequent discussion will systematically examine how heterogeneity may be lacking in existing works on LLM-based simulations and the potential distortions this absence may introduce.
4 LLM-Based Simulations Lack Heterogeneity
4.1 The “Average Persona”: Origin and Dimensions of Limited Heterogeneity
As established in the previous discussion, the capacity of agents to exhibit sufficient heterogeneity is important for social simulations aiming to reveal novel and complex social dynamics. Current LLM agents basically fall short in generating such necessary diversity. This problem is often reflected in their tendency to act as an “average persona”. This average persona reflects LLMs’ built-in bias to converge towards common patterns. The argument advanced in this paper is that the impact of this average persona on heterogeneity can be analyzed through two key behavioral dimensions: variance (representing the diversity and spread of behaviors) and the mean (representing the central tendency or average behavior, and its alignment with human behaviors). We propose this variance-mean decomposition as a useful framework for diagnosing different types of alignment problems, where variance captures whether LLMs can generate the behavioral diversity necessary for complex social dynamics, and mean alignment determines whether the central tendency of LLM behavior corresponds to real human populations. This analytical approach enables us to categorize the specific limitations of LLM-based simulations and establish appropriate boundaries for their application.
This “average persona” phenomenon can be understood through the lens of the models’ training processes. A key contributing factor appears to be that language model training maximizes the conditional probability of predicting text through likelihood-driven loss functions over vast human expression data. This training objective inherently rewards high-frequency, mainstream expressions and suppresses marginal ones, thereby fostering an “average persona” that aggregates group thinking and limits distributional representativeness [29, 121, 124]. The inherent heterogeneity of subgroups is consequently erased, causing behavior to concentrate on a few dominant patterns that often reflect social biases and demographic stereotypes, even when instructions attempt to elicit alternative perspectives [71, 119]. This results in the difficulty in capturing long-tail patterns, even with advanced simulation frameworks or model improvements [119, 124]. We delineate two primary cases based on how this average persona manifests through variance and mean, each with distinct consequences for social simulations.
4.2 Applicability and Claim Boundaries in LLM-Based Social Simulations
Case 1: Low Behavioral Variance, Mean Aligned
In the first case, LLM agents exhibit a low behavioral variance, meaning their strategies and actions are concentrated, lacking the broad diversity observed in human populations. However, the mean (average behavior) of these agents may still align reasonably well with the average behavior observed in real-world human experiments.
Existing work consistently notes that LLMs generate insufficient diversity and exhibit overly homogeneous behavior, often missing human randomness and error patterns [3, 5, 23, 67]. For instance, in economic market simulations, while LLM agents can replicate macroscopic patterns observed in human experiments, individual LLM agents demonstrate significantly less behavioral variance, employing more concentrated strategies compared to diverse human participants [27, 52]. Similarly, in the Keynesian Beauty Contest (KBC, guessing 2/3 of the average), LLM-based simulations successfully reproduced several peak guess values consistent with human experiments, but the frequency of guesses on non-high-frequency values was markedly lower than that of real human participants (Figure 2) [136]. Even in emergency evacuation simulations, despite group-level differences based on personas, individual agent trajectories could be surprisingly similar [135]. These examples illustrate that models may converge towards unified, high-frequency answers that align with human values but diverge from the full behavioral distribution of real humans [3].
The phenomenon here is that even with limited intra-group behavioral differences among LLM agents, if their collective behavioral patterns are meaningful and consistent with real-world aggregated outcomes, insufficient variance does not always undermine the purpose of the simulation at the macroscopic level. However, this scenario mandates strict examination of the boundaries of claimed findings. Researchers should focus on the collective behavior and qualitative patterns, as these may be well-reflected despite low individual variance. Conversely, attempting to interpret the significance of individual agent “behavioral trajectories”—such as specific decisions in an economic market or particular paths during an evacuation—can lead to “interpretive overfitting”. This is because individual decisions may not align with real-world situations (e.g., for personal mobility simulation, real-world activities are oberserved to be less frequent than simulated ones in the COVID-19 pandemic scenario [63]), and it is difficult to verify their underlying reasoning or distinguish them from potential hallucination [114]. Thus, while exploring specific agent decision logic might enhance understanding from an AI/ML perspective (e.g., k-level reasoning in KBC [41, 146]), its significance for social science is relatively low when individual variance is constrained.
Case 2: Low Behavioral Variance, Mean Deviated
The second and more critical case arises when LLM agents exhibit not only low behavioral variance but also a mean (average behavior) that deviates significantly from human values or real-world distributions. This deviation means the aggregated behavior of LLM agents does not accurately reflect the central tendency or typical actions of the human population or subgroup they are intended to represent.
The consequences of this mean deviation are profound for social simulations. Unlike Case 1, where some insights into average collective patterns might still be gleaned, this scenario can render LLM-based social simulations problematic or even inapplicable for deriving meaningful insights into real human societies. For example, research has found that LLMs perform significantly differently when simulating various population subgroups, often exhibiting biases not present in the intended human population [79]. In public opinion surveys, models trained with human feedback tend towards liberal views and exhibit more polarized attitudes, proving difficult to debias through role-play [11, 13, 107]. Such deviations are widespread across models and contexts; generated dialogues often differ from real human conversations in linguistic features and exhibit lower diversity [70]. Moreover, training processes that aim to debias or rationalize certain LLM behaviors, while highly valuable for general applications, can paradoxically compromise the simulation’s utility for studying certain social phenomena. When the research objective specifically requires understanding how biases and irrational behaviors contribute to social patterns, their elimination becomes problematic rather than beneficial. For instance, humans exhibit response biases to specific survey wording, whereas models can be less sensitive to such perturbations, failing to capture behavioral mechanism
...[truncated]"
"|
Selected Publications
Discover more on Publications page or Google Scholar. (* indicates equal contribution)
Yuzhe Yang*, Yifei Zhang*, Minghao Wu*, et al.
TwinMarket: A Scalable Behavioral and Social Simulation for Financial Markets
Best Paper Award, ICLR 2025 Workshop on Advances in Financial AI
A multi-agent framework that leverages LLMs to simulate socio-economic systems
Paper /
Code /
Project Page
Yuzhe Yang*, Yifei Zhang*, Yan Hu*, et al.
UCFE: A User-Centric Financial Expertise Benchmark for Large Language Models
Findings of NAACL 2025
A User-Centric framework designed to evaluate LLMs' ability to handle complex financial tasks
Paper /
Code /
Dataset
Zihao Li*, Xu Wang*, Yuzhe Yang, et al.
Feature Extraction and Steering for Enhanced Chain-of-Thought Reasoning in Language Models
Proceedings of EMNLP 2025
Enhance LLM reasoning by steering activations via a novel SAE-free method using Chain-of-Thought features, without external data
Paper
Jiaqi Wu, Simin Chen, Jing Tang, Yuzhe Yang, et al.
FDPT: Federated Discrete Prompt Tuning for Black-Box Visual-Language Models
Proceedings of ICCV 2025
Federated prompt tuning approach for black-box visual-language models
Paper
|"
"Machine Learning
Authors and titles for February 2025
- [301] arXiv:2502.02270 (cross-list from cs.LG) [pdf, html, other]
- [302] arXiv:2502.02379 (cross-list from cs.LG) [pdf, html, other]
- [303] arXiv:2502.02407 (cross-list from cs.LG) [pdf, html, other]
- [304] arXiv:2502.02410 (cross-list from cs.LG) [pdf, html, other]
- [305] arXiv:2502.02450 (cross-list from stat.CO) [pdf, html, other]
- [306] arXiv:2502.02483 (cross-list from cs.LG) [pdf, html, other]
- [307] arXiv:2502.02496 (cross-list from cs.LG) [pdf, html, other]
- [308] arXiv:2502.02516 (cross-list from cs.LG) [pdf, html, other]
- [309] arXiv:2502.02529 (cross-list from math.PR) [pdf, other]
- [310] arXiv:2502.02531 (cross-list from cs.LG) [pdf, html, other]
- [311] arXiv:2502.02561 (cross-list from cs.LG) [pdf, html, other]
- [312] arXiv:2502.02562 (cross-list from cs.LG) [pdf, html, other]
- [313] arXiv:2502.02580 (cross-list from math.ST) [pdf, other]
- [314] arXiv:2502.02671 (cross-list from cs.LG) [pdf, html, other]
- [315] arXiv:2502.02793 (cross-list from math.ST) [pdf, html, other]
- [316] arXiv:2502.02797 (cross-list from cs.LG) [pdf, html, other]
- [317] arXiv:2502.02812 (cross-list from stat.AP) [pdf, html, other]
- [318] arXiv:2502.03048 (cross-list from cs.LG) [pdf, html, other]
- [319] arXiv:2502.03163 (cross-list from math.CA) [pdf, html, other]
- [320] arXiv:2502.03174 (cross-list from math.ST) [pdf, html, other]
- [321] arXiv:2502.03210 (cross-list from cond-mat.dis-nn) [pdf, html, other]
- [322] arXiv:2502.03279 (cross-list from stat.ME) [pdf, html, other]
- [323] arXiv:2502.03366 (cross-list from cs.LG) [pdf, html, other]
- [324] arXiv:2502.03587 (cross-list from cs.LG) [pdf, html, other]
- [325] arXiv:2502.03600 (cross-list from econ.EM) [pdf, html, other]"
"About
I’m a Ph.D. candidate in Information Science at Cornell University under the guidance of René F. Kizilcec in the Future of Learning Lab. My research sits at the intersection of education, data science, and computational social science. I am interested in AI evaluation in education systems especially in high-stakes decision-making processes. At Cornell, I have explored various topics related to college admissions, including: (1) analyzing how policy changes impact the allocation of educational opportunities; (2) investigating distinctive patterns in LLM-generated content such as homogenization, compared to human-authored college application essays; and (3) examining the uncertainty and arbitrariness of algorithmic predictions.
I’m also fortunate to collaborate with Thorsten Joachims, Nikhil Garg and AJ Alvero.
Prior to Cornell, I spent several years as a data scientist at Korea University to develop course and major recommender systems to support college students’ decision making process.
I have a love-hate relationship with tennis — You’ll often find me attempting to upgrade my skills from the ‘absolute beginner’ category. I also love listening to music and curating songs!
News
Jul, 2025 Our work Poor Alignment and Steerability of Large Language Models: Evidence from College Admission Essays has been accepted to COLM Social Simulation with LLMs Workshop!
May, 2025 I became a PhD candidate!
Apr, 2025 Our work has been presented at ICLR-HAIC 2025 workshop and Georgetown University
Apr, 2025 Relocating to NYC this summer - Excited to be a PiTech Fellow!
Mar, 2025 New paper is out! Poor Alignment and Steerability of Large Language Models: Evidence from College Admission Essays
Aug, 2024 “Large Language Models, Social Demography, and Hegemony: Comparing Authorship in Human and Synthetic Text” has been accepted for publication in Journal of Big Data
Jul, 2024 “Ending Affirmative Action Harms Diversity Without Improving Academic Merit” has been accepted to EAAMO’24 See you in San Luis Potosí, Mexico!
Jun, 2024 “The Life Cycle of Large Language Models in Education: A Framework for Understanding Sources of Bias” has been accepted to the British Journal of Educational Technology.
Apr, 2024 Our project “Evaluating the Impact of Different Application Ranking Policies on College Admission Outcomes” has been awarded a grant from the Cornell Center for Social Sciences! ($12,000)
Jan, 2024 Our work “Comparing Authorship in Human and Synthetic Text” has been accepted to Generative AI and Sociology workshop at Yale University!
Dec, 2023 Our workshop paper “When Bias Meets Personalization: Challenges and Perspectives in LLM-Based Educational Technology” has been accepted to LAK24!
Dec, 2023 Our proposal “Application Essays and Characters in Higher Education Admissions” has been accepted to NCME 2024!
Oct, 2023 Gave a talk about our on-going literature review “Bias in Large Language Models in Education: Sources, Measures, and Mitigation Strategies” at NCME-AIMC(National Council on Measurement in Education-AI in Measurement and Education)
Jul, 2023 Our workshop paper “Augmenting Holistic Review in University Admission using Natural Language Processing for Essays and Recommendation Letters” has been accepted to AIED Tokyo 2023!
Selected Publications
Ending Affirmative Action Harms Diversity Without Improving Academic Merit
Jinsook Lee*, Emma Harvey*, Joyce Zhou, Nikhil Garg, Thorsten Joachims, René F. Kizilcec
The 4th ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization (EAAMO ‘24)
The Life Cycle of Large Language Models in Education: A Framework for Understanding Sources of Bias
Jinsook Lee, Yann Hicke, Renzhe Yu, Chris Brooks, René F. Kizilcec
British Journal of Educational Technology (BJET)
Large Language Models, Social Demography, and Hegemony: Comparing Authorship in Human and Synthetic Text
AJ Alvero, Jinsook Lee, Alejandra Regla-Vargas, René F. Kizilcec, Thorsten Joachims, anthony lising antonio
Journal of Big Data
Augmenting Holistic Review in University Admission using Natural Language Processing for Essays and Recommendation Letters
Jinsook Lee, Bradon Thymes, Joyce Zhou, Thorsten Joachims, and René F. Kizilcec
EDI in EdTech R&D workshop at Artificial Intelligence in Education: 24th International Conference (AIED ‘23)
Development and Application of an AI-Powered Adaptive Course Recommender System in Higher Education: An Example From K University
Jinsook Lee, Kibum Moon, Suyeon Han, Sukang Lee, Hyejung Kwon, Jaeho Han and Gyutae Kim
Journal of Educational Technology, 37(2), 267-307
Academic Service
Reviewer
Frontiers in Psychiatry
ACM Conference on Fairness, Accountability, and Transparency (FAccT’25)
International Journal of Artificial Intelligence in Education (IJAIED)\
Teaching Experience
TA
SP25;SP24;FA22 INFO4100 Learning Analytics (Instructor: Prof. René F. Kizilcec)
FA24 INFO4240 Designing Technologies for Social Impact (Instructor: Prof. Chris Csíkszentmihályi)"
